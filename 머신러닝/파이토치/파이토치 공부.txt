# 파이토치에서의 학습 순서
1. 데이터를 불러오고, 순서대로 전처리를 해줄 pipeline를 구축.
ex)	데이터 세트를 텐서의 형태로 바꾸고, 평균 0.5, 표준편차 0.5로 전처리를 하는 순서.
	transform = torchvision.transforms.Compose(
		torchvision.transforms.ToTensor(),
		torchvision.Normalize((0.5,), (0.5,))

2. 데이터 샘플(MNIST, FashionMNIST, CIFAR-10)을 훈련 세트 / 테스트 세트 들을 가져온다.(다운로드)
ex)	FashionMNIST의 경우
	train_dataset = torchvision.datasets.FashionMNIST(
		root='./fashionMNIST',
		train=True,
		download=True,
		transform=transform

3. 학습 데이터 세트를 검증 데이터 세트로 나눔.
ex)	검증 세트를 20%만 떼준다면
	train_size = int(0.8 * len(train_dataset))
	val_size = len(train_dataset) - train_size
	train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])

4. 각 데이터 세트를 배치단위로 나누어줌.
ex)	각 데이터 세트를 64개의 배치 사이즈로 나눈다.
	train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)
	test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)
	val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)

5. 파이토치에서의 모델 정의.
	: 아레는 nn.Module을 상속받는 내 모델을 정의한 클래스를 만든다.

ex) 	torch.nn.Module을 상속받는 클래스를 만들어서, Layer를 정의하고, 순전파, forward()를 정의한다.
class MyModel(nn.Module):
	def __init__(self):
		super(MyModel, self).__init__() # 이거로 부모 클래스의 생성자를 호출하여 부모 클래스의 기능을 사용할 수 있게 함. (model.train()이라던가, 등등)
		self.flatten = nn.Flatten()
		self. fc1 = nn.Linear(28*28, 100)
		self.relu = nn.LeRU()
		self.fc2 = nn.Linear(100, 10)
		self.softmax = nn.Softmax(dim=1) # 여기서 dim=1은 axis=1처럼 뒷 차원 것을 기준으로 사용할 것임을 의미.
	def forward(self, x): # model(inputs)를 하면 호출되는 순전파. 아레의 과정을 거쳐 학습이 진행됨.
		x = self.flatten(x) # x에는 예제에선 
		x = self.fc1(x)
		x = self.relu(x)
		x = self.fc2(x)
		x = self.softmax(x)
		return x
	- model = MyModel() # 을 통해서 모델을 객체화함.

6. optimizer와 criterion(손실함수)를 정의한다.
ex)	optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 모델의 학습 가능한 파라미터들을 미리 알려주어 학습 과정에서 업데이트할 대상으로 지정한다.
		- 미리 옵티마이저가 파라미터를 알지 못하면 어떤 값을 업데이트해야 할지 모름.
	criterion = nn.CrossEntropyLoss() # 모델의 예측과 실제 정답 간의 차이를 계산하는 손실 함수를 정의하는 역할을 함.

7. 실제 학습을 수행하는 함수 정의.
	: 학습을 어떻게 수행할 것인지를 함수로 만들어서 이 함수에 따라 학습이 진행되도록 함.
ex)	
    def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs = 50):
        for epoch in range(num_epochs): # 각 에포크마다 아레를 반복한다.
            model.train()  # 모델을 train()모드로 변환
            train_loss = 0 # 이번 에포크에서 계산할 손실값을 0으로 초기화함.
            correct_train = 0 # 학습 데이터 중 맞춘 것의 개수 초기화
            total_train = 0 # 학습 데이터의 총 개수 초기화
            correct_val = 0 # 검증 데이터 중 맞춘 개수 초기화
            total_val = 0 # 검증 데이터의 총 개수 초기화
            for inputs, targets in train_loader: # train_loader에 배치사이즈만큼 나누어져 들어가있음.
                optimizer.zero_grad() # 이전 배치에서 계산된 기울기가 현재에 영향을 미치지 않도록 함.
                outputs = model(inputs) # 순전파(Forward Pass)로 모델의 예측값을 계산함.
                loss = criterion(outputs, targets) # 예측값과 정답 간의 손실을 계산.
                loss.backward() # 역전파(Backward) 계산한 손실을 기준으로 각 파라미터의 기울기를 계산한다.
                optimizer.step() # 계산한 파라미터의 기울기를 기반으로 모델을 파라미터를 업데이트함.
                train_loss += loss.item() # 이번 batch에서의 손실값을 누적해서 더해줌.
                _, predicted = torch.max(outputs, dim=1) # 최종적으로 softmax를 통해 나온 데이터 하나(1, 10)일텐데, 중 가장 큰 (값, index)를 받아서 predicted에 index를 받음.
                correct_train += (predicted == targets).sum().item() # 예측한 index와 정답을 비교해 맞으면 .item()으로 자료형으로 바꿔서 더해줌.
                total_train += targets.size(dim=0) # targets는 (배치 사이즈, 1)의 크기일텐데 그러니까 이걸 데이터 개수만큼 더해준것임.
            
            model.eval() # 검증을 하는 과정은 위와 동일함.
            val_loss =  0
            with torch.no_grad():
                for inputs, targets in val_loader:
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                    val_loss += loss.item()
                    # 정확도
                    _, predicted = torch.max(outputs, dim=1)
                    correct_val = correct_val + (predicted == targets).sum().item()
                    total_val += targets.size(dim=0)
                    
            train_loss /= len(train_loader) # 검증 세트와 훈련 세트의 개수가 다르니, 개수로 나눠줘서 평균 손실값을 계산하는것임.
            val_loss /= len(val_loader)
            
            
            print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {(correct_train/total_train):.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {(correct_val/total_val):.4f}') # 출력을 위한 
        torch.save(model.state_dict(), 'final_model.pth')



