### Optimizer
1. Adam
	작동 방식 : SGD와 RMSprop의 개념을 결합한 옵티마이저임.
	장점 :
		- 빠른 수렴 속도
		- 적응적 학습률(각 파라미터에 학습률이 다르게 설정됨) → backward()에서 계산된 기울기 정보를 사		용하여 파라미터별로 학습률을 자동으로 조절한다.
		하지만, lr = 0.0001로 설정한 그 lr에 크게 벗어나게 다르진 않음.
	단점 : 
		- 빠르게 수렴하지만, 수렴 이후 진동을 일으킬 수 있다. # 조기종료를 적용해야함.
	- 하이퍼 파라미터
		1. betas=(0.9, 0.999) # 모멘텀과 2차 모멘트 추정에 사용되는 감쇠 계수.
		2. weight_decay = 1e-5 (정규화 항으로 파라미터 값이 너무 커지는 것을 방지함.)
		3. eps = 1e-8 (분모가 0으로 나눠지는 것을 방지)
		4. lr

2. Adagrad
	작동 방식 
		: 각 파라미터마다 학습률을 개별적으로 조정하여 학습 속도를 최적화함.
		- 이전 기울기들의 제곱합을 기반으로 학습률을 점진적으로 줄여가면서 업데이트함.
		- 기울기가 큰 경우에는 학습률이 줄어들고, 작은 경우에는 학습률이 상대적으로 커진다. (이전까지의 epoch들에서 학습되었던 각 파라미터마다의 기울기의 제곱합(G_t)을 추적하고, 이게 크다면, 자주 업데이트된 파라미터라는 의미기 때문에 이 파라미터의 학습률을 줄이고, G_t가 작다면, 드물게 업데이트되는 파라미터이므로 더 많이 업데이트 될 수 있도록 학습률을 올려준다. 
		- 위의 과정을 통해 각 파라미터가 골고루 잘 학습되어 모델에 영향을 미치도록 함.
	단점:
		- 학습률이 계속해서 감소하게 되어 학습 속도가 느려지게 됨.
	- 하이퍼 파라미터
		1. lr
		2. lr_decay : 학습률 감소 값. 학습이 진행될수록 학습률을 줄이기 위한 매개변수
		3. weight_decay : 정규화를 통해 과적합을 방지함.
		4. eps


3. RMSprop
	작동 방식 
		: 기울기의 제곱에 대한 지수 이동 평균(EMA)을 사용하여 기울기의 2차 모멘트를 추적하고 이를 사용하여 학습률을 조정함.
		- 파라미터가 E[g^2]_t에 반비례 하여 이 값이 커지면 적게 업데이트되고, 작으면 크게 업데이트된다.
		- 뭔소린지 모르겠다.
		///
		- Adagrad의 단점인 학습률이 점점 줄어드는 것을 보완하기 위해 나옴.
		- 수렴 속도가 Adagrad보다 개선되었다.
		- 손실 곡률에 맞는 적응적 학습률을 제공해준다.
	- 하이퍼 파라미터
		1. alpha (감쇠율, 지수가중이동 평균을 계산할 때 사용) # 1에 가까울수록 기울기의 최근 변화에 더 많은 가중치를 둔다.
		2. eps (작은 상수 입실론인데, 분모가 0이되는걸 방지한다?)
		3. weight_decay (가중치 감소, default:0)
		4. momentum (모멘텀을 추가할 수 있다. default:0) # 이전의 기울기를 일정 비율 추가하여 최적화 과정을 가속화한다.

4. SGD(+momentum)
	: 미니 배치(batch) 또는 데이터 샘플을 사용하여 기울기(gradient)를 계산하고, 해당 기울기를 사용하여 파라미터를 업데이트한다.
	- Momentum은 기울기 업데이트를 할 때 이전 업데이트 값을 일정 비율 가중치를 부여하여 더해준다. 이렇게 해서 느린 수렴속도를 더 빠르게 해준다.
	- 특징 : 느리지만 안정적으로 수렴함. 진동도 적다.
	- 하이퍼 파라미터
		1. lr
		2. momentum : SGD에서는 기본값은 0.0이지만, 0.9로 설정하는것이 일반적이다. 
		3. weight_decay : 가중치 감소, 정규화를 통해 과적합을 방지함.
		4. nesterov : Nesterov라는 모멘텀을 사용할지 여부를 결정함. # 표준 모멘텀보다 더 빠르게 수렴하도록 도와준다.
		

# 바꿀만한 Optimizer의 하이퍼파라미터
1. learn rate(lr) = 몇으로 할 지 0.0001 0.001 0.01
2. 


### 활성화 함수
	1. ReLU : 
	2. Leaky ReLU : 
	3. ELU : 
	4. Sigmoid : 
	5. Tanh : 

### 드롭아웃 바꿔보기

### 손실함수 바꿔보기
	1. CrossEntropyLoss
		: 다중 클래스 분류 문제에서 기본적으로 많이 사용되는 손실 함수.
		- 모델이 예측한 클래스 확률과 실제 정답 간의 차이를 측정한다.
		- 정답에 해당하는 클래스의 확률이 낮을수록 손실값이 커진다.
		// 얘가 softmax로 각 클래스에 대한 확률을 뱉는데, 실제 정답인 클래스에 대한 확률이 낮을수록 loss가 커진다는 것 같다.
	2. Negative Log Likelihood Loss(NLLLoss)
		: CrossEntropyLoss와 유사하지만, 출력에 softmax를 적용하지 않고 로그 확률을 사용하는 경우 적합.
	3. Mean Squared Error Loss (MSELoss) 
		: 일반적으로 회귀 문제에 사용되지만, 분류 문제에서 가끔 실험적으로 사용함? 왜.
	4. Focal Loss 
		: 클래스 불균형 문제를 다룰 때 유용한 손실함수, 어려운 샘플에 더 많은 가중치를 부여한다.