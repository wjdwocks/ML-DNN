0. 대략적인 학습의 과정
	- 머신러닝
		1. 데이터를 불러옴 (pandas로 read_csv → to_numpy())
		1-1. 데이터의 특성이 n개라면 data_input.reshape(-1, n)으로 각 샘플 당 특성들을 가지도록 형태를 바꾸어줌.
		2. 데이터를 학습 데이터 / 테스트 데이터 / 검증 데이터 로 나누어줌 (train_test_split())
		3. 사용하고자 하는 머신러닝 클래스의 객체를 선언함.
		4. 그 객체가 전처리가 필요하다면, 정규화 등의 전처리를 수행해줌.
		4-1. StandScaler의 경우, StandardScaler의 객체를 선언한 다음, ss에 학습 데이터 세트를 fit()해주고, 그 학습 데이터 세트의 표준 평균과 표준편차로 테스트 세트를 transform 해주어야 함.
		5. 학습 데이터를 머신러닝 객체에 fit() 해주어 학습을 시킴.
		6. predict() 메소드를 통해서 특정 샘플이나, 세트의 값을 예상해볼 수 있다.
		7. cross_validate나 cross_val_score를 사용하여 교차 검증 수행 
		7-1. 이 때 cross_validate는 내부적으로 fit()을 수행하기 때문에 fit()을 생략하는 경우도 있다.




### K- 최근접 이웃 알고리즘을 이용한 분류 알고리즘.
	: 새롭게 예측할 데이터와 가장 가까운 n개의 점을 확인하여 더 많은 것을 따라가는 알고리즘이다.
	- python의 사이킷런의 neighbors 라이브러리에 있다.
		(from sklearn.neighbors import KNeighborsClassifier)
	- KNeighborsClassifier()클래스의 객체를 선언할 때 n_neighbors를 지정해줄 수 있는데, 이 값은 주변 몇 개의 이웃을 결과 예측에 참고할지를 지정해주는 것이다.

2. 정렬된 데이터를 랜덤으로 수집하여 학습하도록 도와주는 train_test_split함수.
	: 데이터가 50개가 있는데, 이 중 앞의 30개는 A클래스의 데이터이고, 뒤의 20개는 B클래스의 데이터일 때 데이터를 랜덤으로 섞어서 학습하지 않으면 30개의 A클래스만의 데이터를 학습했다면, 당연히 모든 예측값이 A가 될 것이기 때문에 중요하다.
	- 이 함수를 사용하지 않고, numpy의 arange와 shuffle함수를 통해서 구현할 수 있다.
	- index = np.arange(50)으로 50개의 인덱스를 의미하는 numpy배열을 만들고,
	- np.random.shuffle(index)를 통해 index의 원소들을 랜덤으로 섞는다.
	- 그러면 그 섞인 인덱스를 데이터에서 나누어서 사용하면
	- train_input = input_arr[index[:35]] 로 하면 index 배열의 0~34번 은 섞였기 때문에 train_input도 섞인 데이터가 들어가게 된다.

	: train_test_split함수
	- from sklearn.model_selection import train_test_split에 있다.
	- train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2)
	- 와 같이 사용하고, data와 target을 각각 (학습 : 테스트)를 (8:2)로 랜덤으로 섞어서 만들어주는 역할을 한다.	


3. 데이터의 정규화의 필요성
	: 두개 이상의 데이터를 이용해서 분류 문제를 해결해야할 때 발생하는 문제를 해결하기 위한 방법.
	- 만약 컴퓨터가 물고기의 길이와 무게를 보고 이것이 농어인지, 도미인지를 구분해야 한다면, 컴퓨터는 이 무게 데이터와 길이 데이터를 똑같은 수로만 판단하여 계산을 할 것이다.
	- 하지만, 길이와 무게는 단위도 다르고, 값도 크게 다를 것이다.
	- 예를 들어) 100cm~200cm까지 있는 길이 데이터에 반해 무게는 0.3kg ~ 2kg까지 있다고 한다면, 분류 문제를 해결할 때 무게 데이터는 거의 없는 것과 다름이 없을 것이다.
	- 그렇기 때문에 데이터를 정규화 해야 한다.

4. 데이터를 정규화 하는 방법 (numpy를 이용하여 직접 변환)
	: 생선의 길이, 무게의 데이터를 각각 표준점수로 바꾸어서 분류에 반영하면 된다.
	- 모든 데이터의 길이 데이터끼리 평균과 표준편차를 이용하여 표준점수를 내고, 무게 데이터끼리 평균과 표준편차를 구해서 표준 점수를 낸다.
	- 이는 numpy의 mean()함수와 std()함수를 통해서 직접 계산할 수 있다.
	- 각 길이와 무게 데이터를 train_input = np.column_stack(length, weight)를 통해서 하나의 배열로 만들었다면, 각 행에는 	(길이1, 무게1) 이와 같이 데이터들이 쌓여있을 것이다.
	(길이2, 무게2)
	-> 그렇다면 세로축을 기준으로 평균과 표준편차를 구하면 각 특성에 따라서 구분할 수 있게 됨.
	- mean = np.mean(train_input, axis=0)이라고 한다면 위의 의미가 됨.
	- std = np.std(train_input, axis=0)도 마찬가지.
	- 이제 각 데이터를 평균에서 빼서, 표준편차로 나누어주면 각 데이터들의 표준점수가 된다.
	-> train_scaled = (train_input - mean) / std 를 하면 각각이 모두 크기가 같은 2차원 리스트이기 때문에 알아서 각 항목끼리 뺄셈을 수행하고, std를 수행하게 된다. 0번 열에 있는 것들은 0번 열에 있던 얘들끼리 빼고, 나누고 수행.
	- 이렇게 나온 값들을 다시 matplotlib의 pyplot으로 그려보면, 그림은 똑같이 그려진다. x, y축의 단위만 달라짐.

5. 데이터를 정규화 하는 방법 (StandardScaler)
	: from sklearn.preprocessing import StandardScaler로 선언함.
	- 데이터들을 몇개의 특성을 가지고 있는지는 관계없이 각 행이 데이터, 각 열이 특성을 의미하고 있다면, ss = StandardScaler객체를 선언하고, ss.fit(train_input)로 학습 데이터를 학습한 뒤
	- 그 학습 데이터의 평균과 표준편차로 학습 데이터를 변환하고,
	ex) train_scaled = ss.transform(train_input)
	- 그 학습 데이터의 평균과 표준편차로 테스트 데이터도 변환해야함.**
	ex) test_scaled = ss.transform(test_input)
	- 왜냐하면 훈련 세트와 같은 평균과 표준편차로 학습을 해야 매변 예측마다 같은 일반화 성능을 기대할 수 있다.
	- 이게 무슨소리냐면 테스트 케이스라는건 사실상 아에 처음 보는 데이터여야 하기 때문에, 이를 미리 학습한다면 의미가 없어짐.



6. K_neighbors에서 예측을 위해 선택된 점을 확인하는 방법
	: 만약 우리가 학습되지 않은 새로운 데이터 길이, 무게가 (25, 150)을 예측할 때 선택된 최근접점이 무엇인지 확인하기 위해서는 kn.kneighbors([data])를 이용할 수 있다.
	- 위의 함수는 2차원 numpy배열을 매개변수로 받고, 그 데이터와 최근접 점이 무엇인지 가까운 점부터 그 점까지의 거리, 그 점의 index를 반환한다.
	- 그래서 new = ([25, 150]-mean)/std 를 통해 예측할 점을 정규화 하고
	- distances,  indexes = kn.kneighbors([new])를 통해 2차원 배열로 바꾸어서 넘겨줌.
	- 그러면 distances에는 5개의 가장 가까운 점까지의 거리가, indexes에는 그 점들의 index번호가 들어간다.
	- 이것들을 plt.scatter로 그려보면, 
	- plt.scatter(train_scaled[:, 0], train_scaled[:, 1])로 학습된 전체 데이터를 그리고
		// train_scaled가 2차원 배열이기 때문에, 모든 배열의 0번쨰, 1번째 index값을 의미함.
	- plt.scatter(new[0], new[1], marker='^')로 예측할 데이터를 그리고
	- plt.scatter(train_scaled[indexes, 0], train_scaled[indexes, 1], marker='D')로 선택된 점들을 덧칠해주면 됨.



7. K최근접 이웃 알고리즘을 이용한 회귀 문제.
	: 위와 같은 K-최근접 이웃 알고리즘을 통해 선정된 n개의 점의 평균을 내가 예측하고자 하는 샘플의 값으로 예측한다.
	- python의 scikitlearn에서는 sklearn.neighbors의 KNeighborsRegressor이라는 클래스로 구현되어있다.
	- KNeighborsRegressor 객체를 선언하고, fit()메소드에 (학습 input데이터, 학습 target 데이터)를 넘겨주어 데이터를 학습시킨다. 
	- 이 때 학습 input데이터는 2차원 배열의 형태(샘플 수, n개의 특성)의 형태로 넘겨주어야 함.
	- 첫 번째 차원은 그 샘플의 순서를 의미하고, n개의 특성을 포함하고 있다. 즉,
ex) 	[[특성1, 특성2, 특성3] 이는 (5, 3)의 배열임
	[특성1, 특성2, 특성3]
	[특성1, 특성2, 특성3]
	[특성1, 특성2, 특성3]
	[특성1, 특성2, 특성3]]
	- predict()메소드를 사용하여 2차원 배열을 넘겨주어 새로운 샘플이나 세트의 예상값도 출력해볼 수 있다.


### 선형 회귀 알고리즘. (뒤의 릿지와 라쏘모델은 라쏘(L1), 릿지(L2)규제를 적용한 선형 회귀 모델이다.)
	: 이름에서 나와있듯이 회귀 학습을 하는 문제를 해결하기 위해 학습을 수행한다.
	- (특성1, 특성2, 특성3, 특성4) (값) 들을 가진 세트들을 학습하여 선형 방정식의 형태로 만들어서 새로운 특성들을 가지는 세트의 값을 유추하도록 한다.
	- 선형 회귀를 이용한 학습을 위해서 from sklearn.linear_model import LinearRegression클래스의 객체를 생성하고 처리된 데이터를 fit()메소드에 넘겨주어 학습을 한다.
	- 똑같이 학습 데이터는 위와 같이 ((샘플 수, n개의 특성), 앞의 배열 index에 해당하는 결과값)으로 이루어진다.
	- 선형 회귀 클래스의 내부 변수로 lr.coef_과 lr.intercept_가 있어서 내가 학습한 직선의 기울기와, y절편을 알아서 학습해서 넣어져 있다.
	- 이를 통해서 z = aw1 + bw2 + cw3 + dw4 + e 의 직선을 이용해서 각 특성을 아는 샘플 하나를 예측할 수 있다.
	- 이를 matplotlib.pyplot으로 그려볼 수도 있는데, 특성이 1개인 경우 특성의 최대 ~ 최소치를 임의로 정해서 plt.plot([min, max], [min*lr.coef_ + lr.intercept_, max*lr.coef_ + lr.intercept_])로 그릴 수 있다.
	----
	- 2차원 선형 회귀로 학습을 시킬 수도 있다.(다항 회귀)
	- 2차원 선형 회귀를 하기 위해서는 학습 데이터의 형태를 [특성1^2, 특성1]과 같이 2차원으로 결합된 형태를 띄어야 한다.
	- 특성이 하나였다면 기존의 [특성1], [정답1]과 같은 훈련 세트가 np.column_stack((특성1^2, 특성1))로 바뀌어야 한다.
	- np.column_stack()은 멍청하게 직접 하는 방식이고, from sklearn.preprocessing import PolynomialFeatures를 통해서 2차항 이상의 항목들을 자동으로 생성해 줄 수 있다. (이는 아레에서 더 자세하게 다뤄보도록 한다)
	- 	[[1. 특성],			[(특성^2, 특성), 	왼쪽과 같았던 input_data가 오른쪽과 같이 변경됨.
		[2. 특성],			(특성^2, 특성),
		[3. 특성],			(특성^2, 특성),
		[4. 특성]] 였던 것이	(특성^2, 특성)]
		
	- 이렇게 변환한 훈련 세트를 fit()메소드에 넘겨주면, 자연스럽게 2차원임을 인지하고, 차원에 맞는 각 가중치와 절편을 생성하게 된다.


9. 특성 공학
	- 지금까지의 학습은 샘플의 '길이' 를 가지고 '무게'를 유추하는 방식을 사용했다.
	- 하지만, 당연히 무게를 유추하기 위한 데이터(특성)이 많을 수록 당연히 도출할 결과값이 더욱 정확해질 것이다.
	- 예를 들어 길이라는 특성 말고도 너비, 물고기종 등의 다른 특성들도 알고 있다면 무게를 유추하는 데에 더 큰 도움이 될 것이다.
	- 하지만 이들을 조합하여 (길이, 너비, 종, (길이, 너비), (너비, 종), (길이, 종), (길이, 너비, 종))이라는 여러 가지의 조합된 특성들로 사용할 수도 있다.

9-1. 변환기
	: 변환기는 우리가 가진 샘플에 대한 특성을 여러 형태로 변환해줄 수 있는 역할을 해준다.
	- 위의 예시와 같이 (길이, 너비, 종)의 세 개의 특성을 가진 샘플을 각 특성을 조합해 특성의 개수를 늘려주거나 할 수 있다.
	- Polynomial Features : 다항식 회귀를 이용하여 유추하기 위해 특성을 변환해주는 변환기.
	- 이러한 변환기는 전에 설명한 StandardScaler와 같이 학습 데이터를 fit()해주기 전에 데이터를 Polynomial Features객체에 넘겨주어 transform하여 특성을 변환한 뒤, 학습을 수행하게 한다.
	- 마찬가지로, 학습 데이터를 Polynomial Features객체에 fit() 해주어서 학습 데이터를 기준으로 변환기를 학습시켜준다.
	- 그 이후 학습 데이터와 테스트 데이터를 transform해준다.

9-2. Polynomial Features를 사용할 때 주의할 점.
	- PolynomialFeatures의 객체를 선언할 때 degree를 정해줄 수 있는데 이것은 변환할 다항식의 차수라고 볼 수 있다.
	- 만약 특성이 3개가 있고, degree=5라면 x, y, z를 결합하여 5차 다항식을 이루어서 특성으로 사용한다는 의미임.
	- 즉, 위와 같은 상황이라면 3개의 특성이었던 것이 엄청 많이 늘어나게 되어, 과대적합이 발생할 확률이 높다.
	- 과대 적합이란, 너무 학습 데이터에 맞춤으로 학습을 한 나머지 학습 데이터에 대한 점수는 매우 높게 나오지만, 새로운 데이터셋(테스트 데이터, 검증 데이터)에 대해서는 일반화된 성능을 이끌어내지 못하는 모델을 의미한다.
	- 그렇기 때문에, 위와 같이 특성이 늘어남에 따라서 과대 적합을 제어할 수 있는 규제(regularization)이 필요하다.

10. 규제. (lasso, ridge) (L1, L2규제)
	- 규제의 필요성 : 특성이 폭발적으로 늘어남에 따라서 학습에 큰 영향을 미치는 특성과 적은 영향을 미치는 특성이 생기고, 여러 특성을 모두 만족하게 하는 결과값을 도출하기 때문에 과대 적합이 발생하는 것이다.
	- 위의 PolynomialFeatures를 사용한 예시가 대표적이다. 샘플의 특성들을 서로 조합하여 더 다양한 특성들을 만들어냄으로써, 과대적합이 발생할 확률이 높아지고, 과대적합을 막기 위해 규제를 적용하는 것이다.

	- 이러한 규제를 적용하는 방식은 L1, L2규제가 있는데 이러한 규제를 사용하기 전에 각 특성이 미치는 영향을 동일하게 해주기 위해서 정규화 해주어야한다. StandardScaler를 통해 가능함.
	- lasso(L1규제) : 계수의 절댓값을 기준으로 규제를 적용함.
	- ridge(L2규제) : 계수를 제곱한 값을 기준으로 규제를 적용함. (다항차식으로 나타나는 특성들에 대해서)
	- 규제를 적용하는 방법 : sklearn.linear_model에 있는 Ridge나 Lasso의 객체를 선언하고, PolynomialFeatures로 변형된 학습 데이터를 ridge와 lasso에 fit()하여 맞추어준 뒤 학습 데이터와 테스트 데이터를 transform하여 규제가 적용된 특성들의 집합으로 데이터 샘플들을 변형해줄 수 있다.
ex)	
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(train_scaled, train_target) # 이 train_scaled는 PolynomialFeatures를 통해 특성을 늘리고, StandardScaler를 통해 정규화 된 학습 데이터를 의미함.
# 그 다음 ridge를 통해 학습을 하고 아레와 같이 점수를 출력해볼 수 있다.
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

10-1. 릿지와 라쏘회귀를 사용할 때 규제의 양을 조절해줄 수 있는 alpha라는 매개변수
	- alpha값이란, 각 특성의 계수를 얼마나 줄일 지를 조절하는 매개변수이다. (규제의 강도를 조절함)
	- alpha값이 작다면 필요 없는 특성의 계수를 조금 줄이고, 크다면 필요 없는 특성의 계수를 많이 줄이게 된다.
	- 그런데 내가 alpha가 어떤 값일 때가 적절한지 어떻게 아는가?
	: alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]을 두고, 각 alpha에 해당하는 Ridge나 Lasso객체를 생성한 뒤 각 모델의 점수를 확인해 봐서 가장 좋은 점수를 가진 모델을 채택하면 된다. 하지만, 가장 좋은 점수를 가진 다는 것은 가장 좋은 일반화 성능을 가진 다는 것으로, 학습 데이터 샘플의 점수와 테스트 데이터 샘플의 점수의 차가 가장 적은 때를 의미한다.

ex)	for alpha in alpha_list: # 각 alpha를 적용한 모델을 따로 만들어 점수를 계산하여 비교한다.
		ridge = Ridge(alpha=alpha)
		ridge.fit(train_scaled, train_target)
		print(ridge.score(train_scaled, train_target))
		print(ridge.score(test_scaled, test_target))


### 로지스틱 회귀
	: 이름은 회귀이지만, 분류 문제를 해결하기 위한 알고리즘이다.
	- 로지스틱 알고리즘은 선형회귀와 동일하게 선형 방정식을 학습한다.
	- z = a*(Weight) + b*(Length) + c*(Diagonal) + d*(Height) + e*(Width) + f
	- 위의 공식에서 a ~ e는 가중치, f는 절편을 의미한다.
	- 이 z값은 이진 분류인지, 다중 분류인지에 따라 다르게 처리된다.
	- 이진 분류에서는 z값을 시그모이드 함수를 통해서 1 / 1+e^(-z) 를 통해 양성 클래스에 대한 확률로 변환하고, 나머지 음성 클래스에 대한 확률은 이 확률을 1에서 제외하여 알 수 있다.
	- 다중 분류에서는 각 클래스에 해당하는 z값들을 모두 계산한 뒤 소프트맥스함수를 통해서 모든 z값을 e의 지수로 하여 양수로 변환해준 다음 모두 더한 뒤 분모에 두고, 각 클래스의 z값을 분자에 두어서 각 클래스의 확률을 계산할 수 있다.
	- z값은 Gradient Descent 알고리즘을 통해 손실함수가 최소가 되는 지점에서의 가중치와 절편을 구하고, 그 때의 z값을 각 클래스마다 계산한다.


12. 점진적 학습
	: 점진적 학습이란, 우리가 모델을 학습할 때 한 번에 모든 데이터를 학습할 수 없는 경우가 있다. (예를 들어 아직 데이터가 모두 모이지 않은 경우)
	- 이럴 때에는 지금까지 모인 데이터를 우선 학습시켜 놓고, 점진적으로 뒤에 이어지는 데이터를 학습해야 한다.
	- 이런 점진적 학습을 하는 대표적인 예시가 경사 하강법이다.


13. 경사 하강법
	: 점진적 학습의 대표적인 예시로, 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 내려가는 방식이다.
	- 확률적 경사 하강법 : 하나의 케이스 마다 손실 함수를 계산하고, 이를 줄이는 방향으로 가중치와 절편을 조정한 다음 다음 모든 케이스에 대해 이를 반복하는 것.
	- 미니배치 경사 하강법 : 특정 개수의 케이스마다 손실 함수를 계산하고, 이를 줄이는 방향으로 가중치와 절편을 조정한 다음 반복. (즉, 지정된 개수의 케이스의 손실값을 평균내서 가중치와 절편을 정함)
	- 배치 경사 하강법 : 각 epoch당 전체 케이스에 대해 이를 반복한다. 전체 데이터를 사용하기 때문에 경사가 완만하다는 장점이 있다. (데이터의 전체 개수가 달라지거나 하는 경우에는 추천되지 않는다.)
	- 위의 경사 하강법들은 공통적으로 반복하여 가중치와 절편을 수정하고, 손실 함수가 최소가 되는(다시 올라가게 되면) 중지하고, 최소인 지점에서의 가중치와 절편을 선택하게 된다.


14. 손실 함수
	: 어떤 문제에서 이 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준이다.
	- 그렇기 때문에 손실 함수의 값이 작을 수록 좋다.
	- 분류 문제에서의 손실 함수의 값은 (맞춘 개수) / (전체 개수)로 표현할 수 있는데, 이는 비연속적인 값이므로, 분류 문제에서는 연속적인 손실 값을 제공해주는 '크로스 엔트로피 손실 함수'를 주로 이용한다.
	(크로스 엔트로피 함수는 연속적이고, 미분가능하기 때문에.)
	- 회귀 문제에서의 손실 값은 '평균 제곱 오차(MSE)'를 통해서 예측값과 정답값을 제곱한 차이가 얼마나 되는지를 주로 이용한다.

15. epoch
	: epoch란 한 학습 세트를 전부 이용한 학습을 몇 번 반복할 것인지를 의미한다.
	- 위의 예시로 확률적 경사 하강법에서는 하나의 샘플을 기준으로 손실함수를 조정해 가는데, 모든 샘플을 한 번씩 다 사용한 경우 1 epoch를 수행했다고 의미하며, 점진적 학습에서는 여러 epoch를 반복하여 일반화된 성능을 가져올 수 있다.
	- 이전에 배웠던 K-neighbors나 선형 회귀 등과 같은 모델에는 epoch를 사용하지 않는다. 왜냐하면 이들은 한 번의 계산으로 최적의 가중치와 절편을 직접 계산하여 사용하기 때문이다. (하지만, 경사 하강법을 사용하는 선형 회귀도 있다.)
	- 즉, epoch는 점진적 학습을 위해 한 샘플을 여러 번 사용하는 경우에만 사용하는 개념이다.
	- epoch가 너무 적다면, 과소적합의 위험이 있고, epoch가 너무 많다면, 과대적합의 위험이 있다.

16. 확률적 경사 하강법을 이용한 분류 문제

# SGD Classifier의 학습 과정
1. 초기화: 모델 파라미터를 초기화한다. sc = SGDClassifier(loss='log_loss', max_iter=10) # 10의 epoch, 손실함수는 로지스틱 손실함수로 지정
# 아레의 과정은 sc.fit(train_scaled, train_target)을 호출하면 자동으로 수행되는 부분이다.
2. 반복 과정:
   데이터 셔플링: 각 epoch마다 데이터를 랜덤하게 섞는다.
   각 샘플에 대해:
      예측 수행
      손실 함수 계산
      모델 파라미터 업데이트 (경사 하강법)
3. 반복 종료: max_iter에 도달할 때까지 반복한다.

	- 하지만, 우리가 만약 sc = SGDClassifier(loss = 'log_loss', max_iter=100, tol='None')에서 tol을 지정해주지 않으면, 알아서 손실함수 값이 적어지는 값이 적어지면 컴퓨터가 알아서 epoch를 멈춰버린다.
	- 그렇기 때문에 tol='None'을 꼭 넣어주어서 내가 원하는 epoch까지 반복하도록 해주어야 한다.
	- 또한 tol의 기본값은 1e-3으로, 손실함수의 변화가 0.001 이하가 되면 종료된다는 의미이고, 이를 우리가 더 적게 하여 원하는 값을 얻을 수도 있다.

### 트리 알고리즘.
	: 트리 알고리즘은 각 클래스의 예측이 어떤 방식을 통해서 선택이 되었는지를 직접 눈으로 확인하기가 쉽다. (로지스틱 회귀와 다른 장점 - 로지스틱 회귀는 어떤 방식을 통해 분류가 되었는지 직관적으로 알기 어려움.)
	- 결정 트리를 통해 학습하는 경우 이전의 알고리즘과 달리 정규화 작업을 미리 하지 않아도 된다.
(각각의 특성들이 혼합되어서 영향을 주는 것이 아닌, 각 특성끼리의 비교만으로 나누어지기 때문.)
	- 각 트리의 노드에는 어떤 기준을 통해 left child와 right child로 구분되어지는지 나와있다.
(이것을 아레의 plot_tree를 통해서 직접 눈으로 확인할 수 있다.)
	- 또한 matplotlib.pyplot과 sklearn.tree의 plot_tree를 통해 그 트리를 직접 그려볼 수도 있다.

1. 트리를 이용한 분류 알고리즘.
	- from sklearn.tree import DecisionTreeClassifier 로 결정트리 분류 클래스를 import함.
	- dt = DecisionTreeClassifier(max_depth=3)과 같이 객체를 만든다.
(만약 max_depth를 정해주지 않으면 트리가 과대적합 될 수 있으니 조심하자.)
(이 또한 전에 공부했던 릿지와 라쏘의 경우처럼 max_depths = [3, 5, 7, 9]와 같이 리스트에 넣고 각 max_depth 별로 트리를 학습시켜서 결과를 확인해보는 것도 괜찮은 선택이다.)
	- 위에서 max_depth는 트리의 최대 깊이를 의미한다.
	- 그런 후에 dt.fit() 메서드를 통해서 모델을 학습시키게 되는데 여기에서 학습 데이터 세트는 따로 전처리가 필요 없다.
	- PreProcessing이 필요 없다는 의미. (물론 해도 결과는 달라지지 않는다.)

2. dt.score(train_input, train_target) 를 통해서 학습 세트의 점수를 알 수 있다.

3. from sklearn.tree import plot_tree 로 plot_tree를 import하여 트리를 그릴 준비를 함.
	- plot_tree는 학습된 결정 트리의 모습을 보여주는 함수이다.
	- plot_tree(dt, filled=True, features_names=['alcohol', 'sugar', 'pH'])과 같이 사용.
	- 여기서 filled는 이 결정사항에 대해 어디로 더 많은 샘플들이 몰렸냐를 시각적으로 보여주고
	- features_name은 우리의 데이터 샘플에 특성을 알려주는 것으로, 데이터셋의 헤더를 그대로 리스트로 옮겨주면 된다.

4. 마지막으로 dt.feature_importances_ 라는 DecisionTreeClassifier의 변수가 있는데 이 변수에는 이 모델을 학습할 때 어떤 특성이 얼마나 영향을 미쳤는지를 수치화해서 보여준다.
	- 예를 들어 위의 alcohol, sugar, pH를 토대로 이 와인이 화이트 와인인지, 레드 와인인지를 구분한다고 한다면,
	- print(dt.features_importances_) # [0.12345626 0.86862934 0.0079144 ]
	- 위와 같은 결과가 나타나고, 이 수치가 클 수록 더 큰 영향을 미친 것이라고 볼 수 있다.
	- 위의 순서는 우리가 데이터셋에서 있었던 특성의 순서와 같다.

5. 트리의 노드 분할 기준은 gini 불순도라는 기준을 사용한다.
	- gini 불순도란 위 노드에서 내려온 데이터 샘플들이 얼마나 좌우로 잘 나누어지느냐를 의미하는 척도이다.
	- 완전히 한 쪽으로만 가진다면 gini 불순도는 0이 되고, 정확히 반씩 나누어진다면 0.5로 가장 높다.


### 교차 검증과 그리드 서치
1. 모델 검증의 중요성
	: 우리가 지금까지 학습 데이터와 테스트 데이터로 나누어서 학습 데이터로 학습을 하고, 테스트 데이터로 점수를 확인했다.
	- 하지만, 이렇게 얻은 점수가 과연 큰 의미를 가질 수 있는가?
	- 위와 같이 학습을 한다면, 정해져 있는 테스트 데이터에 대한 최적의 값을 찾는 것일 뿐이다.
	- 그렇기 때문에, 변하는 검증 세트(일반화된 성능을 얻기 위해서는 검증을 하는 세트 자체가 계속 일반화되도록 변해야 한다.) 를 이용하여 일반화된 성능을 확인할 수 있다.

2. 교차 검증
	: 교차 검증이란 처음 정해진 훈련 세트 중에서 검증 세트를 일부 랜덤으로 떼어낸 후 점수를 확인하는 것을 본복하는 과정이다.
	- 3-fold 교차 검증이라면 이러한 행위를 3번 해서 평균을 내기 때문에 더 일반화된 성능을 확인할 수 있다.
	- from sklearn.model_selection import cross_validate를 통해서 교차 검증 함수를 import 하여 교차 검증을 수행할 수 있다.
	- scores = cross_validate(dt, train_input, train_target, cv=5) 와 같이 사용하며, 기본적으로 4개의 매개 변수를 갖는다.
	- dt는 이전에 생성해놓은 결정 트리 모델의 객체를 의미함.
	- train_input, train_target으로 학습 시킬 데이터 세트를 그대로 넘겨준다. (이렇게 넘겨주면 알아서 검증 세트를 랜덤으로 분리하여 학습과, 검증과정을 수행한다.)
	- cv=5 는 이 검증 과정을 KFold 방식으로 수행하는데, 몇 번 fold할 것인지를 의미한다.

3. cross_validate 함수의 return 값.
	- cross_validate 함수를 수행한 뒤 나오는 결과값은 교차검증을 수행한 뒤 나오는 평균값이 *아니다.*
	- cross_validate의 return값으로는 dictionary가 나오게 되고, 그 안에는 대표적으로 'test_score'가 있는데, 이 검증 테스트 점수들을 평균내면 그것이 교차 검증 값이 된다.
	- 즉, 위와 같이 cross_validate를 수행한 결과를 scores로 받는다면, print(np.mean(scores['test_score']))를 통해서 교차 검증을 수행한 평균 점수를 확인할 수 있게 된다.

4. cross_validate 함수의 cv 매개변수에 대해서
	- cv 매개변수를 우리가 지정해주지 않으면 n_splits = 5를 기본값으로 훈련 세트를 섞지 않고, 폴드를 나누어 교차 검증을 수행한다.
	- 여기서의 섞는다의 의미는 훈련 세트 중 일부 검증 세트를 생성할 때 랜덤으로 선택하지 않는다는 것을 의미한다.
	- 그렇기 때문에 cv에 splitter = stratifiedKFold(shuffle=True, n_splits=5) 와 같이 클래스의 객체를 넘겨주거나
	- cv = stratifiedKFold(shuffle=True, n_splits=5)와 같이 생성자를 넘겨주어야 한다.

### 그리드 서치 CV와 랜덤 서치 CV
1. 그리드 서치란?
	: 우리가 DecisionTreeClassifier와 같은 모델을 생성할 때 주요 매개변수(max_depth, min_impurity_decrease, min_samples_split)들이 나의 훈련 세트에 비교해서 어떤 값에서 최적의 값을 갖는지를 알려주는 역할을 한다.
	- 아레와 같이 매개변수 목록을 범위의 형태로 dictionary로 만들어 놓는다.
		params = {'min_impurity_decrease' : np.arange(0.0001, 0.001, 0.0001),
				'max_depth' : range(5, 20, 1),
				'min_samples_split' : range(2, 100, 10)}
	- 그 다음, 아레와 같이 학습시킬 모델, 위의 params를 넘겨주어 GridSearchCV의 객체를 생성한다. n_jobs=-1(얘는 cpu를 몇개의 코어를 사용하여 search를 할 것이냐를 의미함.)
		gs = GridSearchCV(DecisionTreeClassifier(), params, n_jobs=-1)
--------------아레와 같음---------------
gs = GridSearchCV(DecisionTreeClassifier(), 
			param_grid={ 
                      'min_impurity_decrease': np.arange(0.0001, 0.001, 0.0001),
                      'max_depth': range(5, 20, 1),
                      'min_samples_split': range(2, 100, 10)
                  }, 
                  n_jobs=-1)
------------------------------------------
	- 그 다음, gs.fit(train_input, train_target)을 통해서 우리의 훈련 세트에 비교하여 위의 범위 내의 매개변수의 최적값을 찾아주게 된다.
	- gs.fit() 부분에서 학습 데이터의 일부를 검증 데이터로 나누어서 훈련 점수와 검증 점수를 비교하여 최적의 parameter 조합을 찾아낸다.
	- 그렇게 찾아진 최적의 값은 gs.best_params_에 dictionary의 형태로 들어가게 됨.
	- 즉, 그리드 서치를 위와 같이 여러 가지 매개변수에 대해 수행하면 각 매개변수 범위마다 브루트 포스의 방식으로 계산을 하기 때문에 많은 계산을 필요로 하게 될 수 도 있다. 그렇기에 n_jobs가 필요한 것임.

2. 그리드 서치의 결과
	- gs.fit()을 수행한 결과로 최적의 매개변수들을 모두 적용한 학습 모델이 gs.best_estimator_에 들어가게 되고, 이 gs.best_estimator_.score(train_input, train_target)을 통해서 점수를 확인할 수 있게 됨.
	- 물론 dt = gs.best_estimator_로 새로 객체에 넣어주어도 됨.



3. 랜덤 서치란?
	: 우리가 위의 그리드 서치에서는 특정 parameter의 최적의 값을 찾기 위해 어느정도는 범위를 정해주고, 모든 경우의 수를 실행하여 결과를 얻었다.
	- 하지만, 랜덤 서치는 위의 parameter들에 대해 일정 범위 내의 랜덤한 값을 갖게하여 여러 번 조합하여 그 중 최적의 값을 리턴해주는 방식이다.
	- 랜덤 서치에서의 매개변수 범위를 지정해주는 방법은 scipy.stats의 uniform과 randint를 사용한다.
	- scipy.stats.randint(start, end)는 랜덤으로 정수를 하나 생성하는데 [start ~ end-1] 사이로 지정한다.
	- scipy.stats.uniform(start, end)는 랜덤으로 소수를 하나 생성하는데 [start ~ end) 사이로 지정함.
	- 아레와 같이 쓰임. (여러 개가 아닌 이유는 아레서 나옴.)
params = {'min_impurity_decrease' : uniform(0.0001, 0.001),
          'max_depth' : randint(20, 50),
          'min_samples_split' : randint(2, 25),
          'min_samples_leaf' : randint(1, 25)}
	- 위와 같이 params를 랜덤 함수를 사용하여 만들어서 아레의 n_iter 만큼의 반복 중 계속 랜덤으로 파라미터 조합을 생성하여 모델의 검증을 수행한다.
	- rs = RandomizedSearchCV(DecisionTreeClassifier(), params, n_iter=100, n_jobs=-1, cv=5)


### 트리의 앙상블
# 앙상블 학습
	: 정형 데이터를 다룰 때 가장 뛰어난 성과를 내는 알고리즘이다.
	- 대개 결정 트리를 기반으로 만들어져 있다.
1. 랜덤 포레스트
	: 앙상블 학습의 대표 중 하나로 안정적인 성능 덕분에 널리 사용된다.
	- 앙상블 학습을 적용할 때 가장 먼저 랜덤 포레스트를 시도해보아도 될 정도.
	- 랜덤 포레스트는 이름처럼 결정 트리들을 이용하여 숲을 만드는 알고리즘이다.

2. 랜덤 포레스트의 과정
	- 1. 부트 스트랩 방식으로 데이터를 샘플링 한다.
# 부트 스트랩 방식이란 중복을 허용하여 샘플을 선택하는 것을 의미한다.
# 즉, 100개의 데이터가 있을 때 부트 스트랩 방식으로 샘플링을 한다면 1번 샘플이 2번 ~ 3번정도 중복되어 나타날 수 있고, 아에 선택되지 않은 샘플이 있을 수도 있지만, 전체 크기는 100개로 같다는 것이다.

	- 2. 특성의 랜덤 선택
		: 각 결정 트리가 학습을 할 때 전체 특성들 중 제곱근 개의 특성개수를 랜덤으로 선택하여 학습을 한다.
		- 이렇게 하여 각 트리마다 다른 특성 조합을 통해 서로 다르게 학습되어 예측 성능이 더 향상된다.
		- 하지만, 회귀 문제에서는 기본적으로 모든 특성을 고려한다.
	
	- 3. 여러 트리 학습
		: 기본적으로 랜덤 포레스트는 100개의 트리를 학습시킨다. 각 트리는 위의 부트 스트랩 샘플과 특성의 랜덤 선택을 기반으로 독립적으로 학습된다.
		- 트리의 개수는 사용자가 설정할 수 있고, 트리가 많을수록 예측 성능이 좋아지지만, 계산의 비용도 증가하낟.
		- 트리가 많아지더라도 각 트리는 조금씩 다른 데이터를 학습할 것이고, 조금씩 다른 예측을 하기 때문에, 과적합이 발생하지 않고, 예측의 안정성이 높아진다.

	- 4. 예측 단계
		: 각 트리가 학습을 완료한 뒤 결과를 통해 값을 예측한다.
		- 회귀 문제 : 각 트리들이 예측 한 결과를 평균내여 최종 결과로 내놓는다.
		- 분류 문제 : 각 트리들이 예측 한 결과를 다수결 내어 최종 결과로 내놓는다.

3. 랜덤 포레스트를 이용한 모델의 검증
	- rf = RandomForestClassifier() # 랜덤 포레스트 모델의 객체를 생성함.
	- scores = cross_validate(rf, train_input, train_target, cv=StratifiedKFold(n_splits=10, shuffle=True), return_train_score=True) # 미리 준비한 데이터 샘플을 cross_validate()함수에 모델과 같이 넣어서 검증을 수행한다.
	- print('학습 점수 vs 검증 점수 : ',np.mean(scores['train_score']), np.mean(scores['test_score'])) # 학습 점수와 검증 점수의 평균을 내어 확인함.
	- 여기서는 결과로 0.997 vs 0.894 이렇게 나왔는데, 과대적합 되어 있는 것을 볼 수 있다. 과대적합을 막기 위해서 내가 rf = RandomForestClassifier(max_depth=5)로 바꾸어 보았는데 그 결과 0.868 vs 0.859 이렇게 나왔다.
	- 위의 결과로 보면 사실 max_depth를 설정해주지 않는 것이 훨씬 더 좋은 모델이라고 볼 수 있다. 왜냐하면 모델의 일반화 성능을 의미하는 검증 점수의 결과가 max_depth를 설정해 주지 않았을 때 더 높기 때문이다.

4. 특성 중요도.
	: 우리가 Wine의 특성 3개의 정보를 토대로 Wine의 종류를 맞추는 모델을 학습하고 있는데, 어떤 특성이 얼마나 더 큰 영향을 미치는지를 의미하는 특성 중요도 라는 것이 있다.
	- rf.feature_importances_를 통해서 우리가 지정한 순서대로 리스트에 저장되어 있다.
	- 숫자가 크면 더 높은 중요도를 가진다고 생각하면 된다.

5. OOB 점수
	: 랜덤 포레스트에는 자체적으로 모델을 평가할 수 있는 oob_score_라는게 있다.
	- 모델의 객체를 생성할 때 oob_score = True를 넣어주면 rf.oob_score_를 사용할 수 있게 된다.
	- 이 oob점수란 우리가 처음 각 트리의 학습 데이터를 부트 스트랩 샘플링을 한다고 했는데, 이 전체 데이터 중 63%정도만 부트 스트랩 샘플링에 이용하고 37%정도를 oob 데이터로 따로 두어서 결정 트리가 학습을 한 뒤에 oob 데이터를 이용해 oob_score를 계산하게 된다.
	- 이런 방식으로 진행된다고 보면 됨.

# 엑스트라 트리
	: 랜덤 포레스트와 비슷하게 동작하지만, 트리를 학습 시킬 때 부트 스트랩 방식으로 데이터를 샘플링 하지 않는다.
	- 무작위로 특성을 선택한 후(랜덤 포레스트와 같음), 분할 지점 또한 랜덤으로 선택(랜덤 포레스트는 최적의 지점을 찾아서 분할함)
	- 즉, 좌우 노드로 나뉠 때 각 노드끼리는 다른 클래스인 것이 많도록, 같은 노드 안에는 같은 클래스인 것이 많도록 하는 최적의 분할 지점을 찾는 것이 아닌, 랜덤으로 분할 지점을 찾는다는 것이다.

# 그래디언트 부스팅
1. 동작 방식
	-1. 우선 초기 간단한 트리를 만들어 값을 예측한다.
	-2. 방금 전에 트리가 예측한 값의 오차를 줄이는 방향으로 트리를 조금 수정한다.
	-3. 이 과정을 우리가 설정한 n_estimators의 횟수만큼 반복한다(default : 100)
	-4. 회귀 문제에서는 예측값과 실제 값의 차이(잔차, Residual), (평균제곱 오차)를 직접적으로 줄이는 방식으로 트리가 수정됨.
	-5. 분류 문제에서는 이 트리의 예측 확률과 실제 클래스 간의 차이를 로지스틱 함수를 이용하여 계산한 뒤 이 로지스틱 손실을 줄이는 방향으로 트리를 수정한다.

2. 그래디언트 부스팅 모델의 주요 하이퍼파라미터
	-1. n_estimators : 앙상블에 추가될 트리의 개수.
		- GradientBoosting 모델은 다음 트리로 이동하면서 오차를 줄이는 방식으로 학습을 이어가기 때문에, 추가되는 트리가 많아질수록 
	-2. max_depth : 앙상블에 추가되는 트리의 최대 깊이.
		- 각 단계의 트리의 깊이를 의미하는 것으로 깊이가 깊어질 수록 더 세밀하게 학습이 될 수 있지만, 과대적합의 위험성이 있다.
	-3. learn_rate : 학습률을 의미하는 것으로 다음 트리로의 수정에서 얼마나 급진적으로 변화를 할 것인가를 의미하는 척도이다.

3. 그래디언트 부스팅 모델의 단점
	- 확률적 경사 하강법과 같이 트리를 하나씩 추가하기 때문에 훈련 속도가 많이 느리다.
	- learn_rate가 작으면 작을수록 시간이 더 늘어나는 것도 맞음.
	- max_depth가 크면 트리를 하나씩 학습하는 시간도 늘어나는것도 맞다.
	- 또한 그래디언트 부스팅 모델은 n_jobs라는 매개변수가 없는게, 병렬로 처리할 필요가 없이 트리가 하나씩 학습되면서 추가되기 때문임.

4. 그래디언트 부스팅 모델을 cross_validate()로 여러 번에 걸쳐 학습할 때
	- 일반화된 모델의 성능을 알기 위해서 우리는 scores = cross_validate()를 자주 사용함.
	- 이 때 return_train_score=True로 해서 학습 데이터에 대한 점수도 반환하게 함.
	- 그러고 나서 scores['train_score']와 scores['test_score']를 확인하는데
	- 여기서 train_score는 학습 데이터 점수들이고, test_score가 검증 세트에 대한 점수들이므로
	- 결국은 test_score가 높은 모델이 더 나은 성능의 모델이다.

# 히스토그램 기반 그레디언트 부스팅
1. 정형 데이터를 다루는 머신러닝 알고리즘 중 가장 인기있다.
	- 입력 특성을 256개의 구간으로 나누기 때문에 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.
	- 즉, 트리를 더욱 빠르게 구성하기 때문에, 학습의 속도가 빠르다.
	- 또한 하이퍼파라미터를 직접 지정하지 않아도, 기본 매개변수값으로도 안정적인 성능을 얻을 수 있다.
	- 여기서 학습할 트리의 개수를 지정할 땐 n_estimators가 아닌, max_iter로 사용한다.

2. 히스토그램 기반 그레디언트 부스팅에서의 특성 중요도를 파악하는 방법
	- permutation_importance 함수를 이용해서 각 특성들의 중요도를 파악할 수 있다.
	- 큰 값일 수록 더 중요한 특성이라는 의미(클래스를 나누는 기준으로)
	- sklearn.inspection import permutation_importance로 불러서 쓸 수 있다.
	- 이거를 사용하려면 모델을 fit()으로 학습을 시킨 다음에 사용해야 한다.
틀)	- 내가 cross_validate()가 내부적으로 학습을 시키니까, 이거를 사용하고 그 객체(hgb)에 넣어도 될거라고 생각했는데 cross_validate()는 모델을 학습시키고, 모델 객체에 반영하지 않고, 결과만 반환하기 때문에 안된다고 한다.
맞)	ex)
hgb = HistGradientBoostingClassifier()
cross_validate(hgb, train_input, train_target, n_jobs=-1, return_train_score=True)
# permutation_importance(hgb, train_input, train_target, n_repeats=10, n_jobs=-1) # 이거는 안됨.
hgb.fit(train_input, train_target) # 이렇게 fit() 메서드를 직접 수행해야 객체의 변경이 일어나서 permutaiton_importances함수를 사용할 수 있다.
results = permutation_importance(hgb, train_input, train_target, n_repeats=10, n_jobs=-1)

3. permutation_importance()함수의 결과(반환값)
	- 위에서 results로 받았는데 results.importances_mean에 리스트의 형태로 각 특성들의 중요도가 들어있다.

# XGBoost(그레디언트 부스팅의 확장된 구현체)
	- 성능과 효율성을 극대화한 eXtreme Gradient Boosting의 약자임.
1. XGBoost의 장점
	- 병렬 학습 : 병렬 처리를 통해 큰 데이터셋에서도 빠르게 학습이 가능하다.
	- 정규화 : L1, L2 정규화를 지원하여 모델이 복잡해지지 않도록 과적합을 방지하는 기능이 강화됨.
	- 조기 종료 : 특정 epoch에서 성능 향상이 없을 경우 학습을 중단하여 시간을 절약한다.
	- 유연한 손실 함수 : 다양한 손실 함수를 지원하여, 회귀, 분류, 순위 모델링 등 다양한 문제를 해결할 수 있다.

2. 주요 하이퍼파라미터
	- n_estimators : 기본 100, 생성할 트리의 개수임.
	- learn_rate : 기본 0.1, 각 트리가 생성될 때 전체 모델에 끼칠 영향의 정도.
	- max_depth : 기본 6, 개별 트리의 최대 깊이를 의미.
	- n_jobs : 이 모델의 장점인 병렬 처리를 위해 CPU 코어의 개수를 지정해줄 수 있다.

# 신경망 알고리즘
	: 비정형 데이터를 다룰 때 주로 사용되는 알고리즘이다.
	- 비정형 데이터는 규칙성을 찾기 어렵기 때문에 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭다.


### 비지도 학습.
# 이미지 파일을 numpy배열로 다루는 방식.
2. 이미지의 픽셀값들이 저장되어있는 데이터를 다루는 방법
	: 흑백 이미지의 데이터(100 x 100 크기라고 가정)는 당연히 각 픽셀마다의 밝기를 나타내는 값들이 들어있기 때문에, (100, 100)의 크기를 갖는다.
	- 이러한 사진이 300장이 있다면 이 데이터는 (300, 100, 100)의 크기를 가질 것이다.
	- fruits.shape는 (300, 100, 100)이라고 나옴.

3. 픽셀로 이루어진 데이터를 실제 사진으로 보여주는 방법
	- matplotlib.pyplot의 imshow()함수를 사용하면 된다.
	- plt.imshow(fruits[0], cmap='gray')를 하는데 앞의 fruits[0]는 (100, 100)의 2차원 배열이 들어간다는 의미이고, cmap='gray'는 흑백사진이라는 것을 의미함.
	- 컬러 사진이라면 앞에 (100, 100, 3)의 크기가 들어와야 함.
ex)	color_img = np.random.rand(100, 100, 3) # 이렇게 랜덤으로 (100, 100, 3)의 랜덤한 배열을 생성함.
각 픽셀의 값은 0~1사이의 실수 or 0~255사이의 정수가 와야 한다.
	plt.imshow(color_img)
	plt.title('Random Color Image')
	plt.show()

4. 여러 사진을 하나의 화면으로 보여주는 subplot함수
	- fig, axs = plt.subplots(2, 2) # 1행 2열의 그림을 그린다.
# 여기서 fig는 전체 도화지를 의미하고, axs는 그 도화지의 특정 위치를 의미하게 됨.
	- axs[0][0].imshow()로 # 1행 1열에 그림 하나
	- axs[1][1].imshow()로 # 2행 2열에 그림 하나

# 군집 알고리즘 시작.
1. 군집 알고리즘.
	- 정답이 없는 비지도 학습에서의 대표적인 문제 해결 방식. 
	- 비슷한 것 끼리 묶는 학습을 의미한다.
	- 정답이 없기 때문에 비슷한 것을 최대한 같은 집단에 모으는 것의 목표로 함.

2. 간단한 알고리즘.
	- 300, 100, 100크기의 이미지 데이터들이 있다.
	- 300개의 사진이 100, 100크기의 픽셀들로 이루어졌다는 의미.
	- 이 사진들은 0~99번까지는 사과, 100~199까지는 파인애플, 200~299까지는 바나나로 이루어짐.
apple = fruits[0:100]
pineapple = fruits[100:200]
banana = fruits[200:300]

	- 대충 각 사진의 픽셀들이 있는데 위치에 관계없이 사진 내의 모든 픽셀의 평균들을 내보자.
fruits가 있음 (300, 100, 100)크기의.

	-1. 각 사진마다 (100, 100)의 배열이 있는데 이를 1차원으로 내려버린다.
apple.reshape(-1, 10000)
pineapple.reshape(-1, 10000)
banana.reshape(-1, 10000)

	-2. 그러면 각 사진마다 10000개의 값이 있을 텐데, 이를 모두 더해서 10000으로 평균을 낸다.
np.mean(apple, axis=1) 과 같이 수행
여기서 axis=1은 두 번째 차원을 의미하며, apple은 (100, 10000)의 크기를 갖는데, 두 번째 차원인 10000개의 값을 뜻하여 이 10000개의 값들을 평균내게 됨.

	-3. 그렇게 100개의 사진이 하나의 값을 가진다.
	-4. 이렇게 나온 값들을 히스토그램에 그려본다.
plt.hist(np.mean(apple, axis=1), alpha=0.8)

3. 다른 방식으로의 분류.
	- 각 그림마다의 전체 픽셀의 평균이 아닌 각 위치마다의 평균을 토대로 분류를 해보자.
	- 이번에는 각 그림마다 특정 픽셀 위치에서의 값들을 평균낼 것이기 때문에
	- np.mean(apple, axis=0)이 옳게 됨. 이것은 그림이 위로 100장이 쌓여있는데, 위에서 바닥으로 쭉 누르면서 평균을 계산한다고 생각하면 편하다. 축이 높이 : 0, 가로 : 1, 세로 : 2라고 하면 axis=0끼리 모두 더해서 결과적으로 (axis 1) x (axis 2)가 나온다고 생각하면 편하다. 각 픽셀의 값은 높이들의 평균이라고 보면 쉽다.
	- 그렇게 나온 10000개의 픽셀들의 평균값들을 바그래프로 나타내보자.
	- 바 그래프로 10000개의 각 픽셀들의 평균 값들을 표현하기 위해서 apple.reshape(-1, 10000)을 무조건 해야함.
ex)
	fig, axs = plt.subplots(1, 3, figsize=(12, 4))
	axs[0].bar(range(10000), np.mean(apple, axis=0))
	axs[1].bar(range(10000), np.mean(pineapple, axis=0))
	axs[2].bar(range(10000), np.mean(banana, axis=0))
	plt.show()

4. 평균값들을 이어붙여서 그림으로 만들어보자. (이렇게도 그려볼 수 있다 라는것을 알아라는것.)
	- 위의 np.mean(apple, axis=0)의 값을 이용해서 이로 나온 10000개의 값을 다시 (100, 100)으로 재구성 해준 뒤 imshow()함수를 통해 다시 그려보자.
ex)
	apple_mean = np.mean(apple, axis=0).reshape(100, 100)
	pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
	banana_mean = np.mean(banana, axis=0).reshape(100, 100)

	fig, axs = plt.subplots(1, 3, figsize=(20, 5))
	axs[0].imshow(apple_mean, cmap='gray_r')
	axs[1].imshow(pineapple_mean, cmap='gray_r')
	axs[2].imshow(banana_mean, cmap='gray_r')
	plt.show()

5. 위의 평균과일(?)을 이용해서 간단한 분류 모델을 만들어보자.
	: 주요 취지는 어떤 과일 하나가 주어졌을 때, 이 과일의 각 픽셀(10000개)의 값을 평균과일들(사과, 파인애플, 바나나)와 절댓값 오차를 내서, 가장 작은 것이 이 과일의 클래스라고 예측하는 것이다.
	- 이것을 직접 코드로 작성해보자.
#	- 중요한 것 : np.argsort(array) 함수는 반환값이 정렬된 배열이 아닌, 인덱스를 넘겨준다. 반환된 값은 정렬된 배열이 될 index가 된다. 
ex)
	apple_mean = apple_mean.reshape(100, 100)
	pine_mean = pine_mean.reshape(100, 100)
	banana_mean = banana_mean.reshape(100, 100)
	
	abs_diff = np.abs(fruits-apple_mean) # 각 픽셀끼리 모든 과일들과 사과의 평균을 뺄셈함.
	abs_mean = np.mean(abs_diff, axis=(1,2))
	
	apple_index = np.argsort(abs_mean)[:100] # 이 np.argsort()는 정렬된 인덱스를 출력해줌.
	
	fig, axs = plt.subplots(10, 10, figsize=(10, 10))
	for i in range(10):
	    for j in range(10):
	        axs[i, j].imshow(fruits[apple_index[10*i + j]], cmap='gray')
	plt.show()

### 비지도 학습 K_Means Clustering 
1. K-평균 알고리즘
	: 비지도 학습에서 레이블이 없는 데이터들을 유사성에 따라 k개의 그룹으로 나누고, 각 그룹의 중심을 찾아가면서 반복적으로 군집을 형성하는 알고리즘이다.

2. K-Means 알고리즘의 작동 방식
	-1. 무작위로 k개의 클러스터 중심을 정한다.
	-2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다.
	-3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경한다.
	-4. 클러스터 중심에 변화가 없을 때까지 위 2번으로 돌아가서 반복한다.
	- 즉, 무작위 클러스터 k개(분류 문제라면 종류에 맞게 있으면 좋음 근데 모르는 경우가 많다.)를 만들고, 새로운 과일을 가장 가까운 클러스터에 포함시킨 후 클러스터에 포함된 과일들의 평균을 새로 구해서 클러스터의 중심을 옮기는 것을 반복한다.

3. K-Means와 K-neighbors의 차이.
	- K-neighbors는 지도 학습에서(정답이 정해져있는) 새로운 데이터에 대해 이 데이터와 가장 유사한 n_neighbors개의 데이터를 확인한 후 그 추세를 따르는 알고리즘이다.
	- K-means는 비지도 학습에서 데이터를 군집화하는 목표만 있다는 점에서 다르다.

4. 실제코드
	- 
ex)
	import numpy as np
	fruits = np.load('fruits_300.npy')

	fruits_2d = fruits.reshape(-1, 10000) # 비지도학습이므로 300개의 데이터(과일)과 각각 10000개의 특성을 갖도록 재구성
	from sklearn.cluster import KMeans
	km = KMeans(n_clusters=3) # 중심이 될 클러스터의 개수를 정해줌.
	km.fit(fruits_2d) # 비지도 학습이기 때문에 (샘플, 특성들)의 2차원 numpy배열만 받는듯.
	print('각 과일에대한 예측 : ',km.labels_) # 이 km.labels는 넘겨진 데이터(300개)가 각각 어떤 클래스로 예측이 되었는지 들어있다.
	print('전체 과일 수 : ', km.labels_.shape)
	print('각 클러스터(과일종류)에 속한 개수 : ', np.unique(km.labels_, return_counts=True)) # np.unique()는 매개변수로 넘겨주는 것에 어떤 종류가 있는지만 알려주는데, return_counts=True를 하면 각 종류에 몇개씩 포함되는지까지 알려주게 된다.

5. 이제 각 클러스터에 포함된 그림들을 그려봐서 얼마나 정확도가 있는지 살펴보자.
	- 아레와 같이 함수화 해서 그리기 쉽게 해보자.
	- 
ex)
import matplotlib.pyplot as plt
def draw_fruits(arr, ratio=1):
    n = len(arr) # n은 넘어온 그림의 개수
    rows = int(np.ceil(n/10)) # float로 계산되면 약간의 오차가 발생할 수 있으므로 int()로 감싸줌.
    cols = n if rows < 2 else 10 # 행이 0 or 1이라는것은 그림이 9개 이하라는 의미. 그러면 1차원으로 그림.
    fig, axs = plt.subplots(rows, cols, figsize=(cols, rows)) # 여기부터는 쉽다.
    for i in range(rows):
        for j in range(cols):
            if i*10 + j < n: # 만약 그림이 97개라면 마지막 98, 99, 100은 그릴 수가 없기때문에 넘겨야함.
                axs[i, j].imshow(arr[i*10+j], cmap='gray')
            axs[i, j].axis('off')
    plt.show()


6. 클러스터의 중심
	: 클러스터의 중심은 이 모델이 예측한 과일의 평균값에 가까울 것이다.
	- 즉, 클러스터의 중심 또한 10000개의 특성을 갖고 포함된 데이터들의 각 특성 평균이 될 것인데, 이를 (100, 100)으로 바꾸면 그냥 클러스터에 포함된 데이터의 평균값이 되므로, 위에서 그린 평균의 그림처럼 될 것이다.
	- km.transform()함수는 학습한 데이터의 크기처럼 2차원 numpy배열을 받는다.
	- 즉, (?, 10000)의 배열을 받으면 해당 데이터가 각 클러스터 중심과 얼마나 거리가 되는지를 리스트의 형태로 반환해준다.
	- 이걸 보고 가장 가까운 곳으로 예측이 되었다고 생각할 수 있다.
	- km.n_iter_은 몇번 반복으로 수렴에 이르렀는지를 알려주는데, 그냥 작을수록 빨랐다고 생각하자.

7. 최적의 k값을 찾기.
	- 우리가 이번 예시에서는 과일의 종류가 3개라는 것을 알고 있었기 k_clusters = 3이라고 지정을 해주었는데 이를 모르는 경우가 더 많을 것이다.
	- 이럴 때는 k값을 범위로 지정을 하여 여러 개의 k_means모델을 학습을 하고, 각 학습된 모델의 km.inertia_를 확인해서 값이 확 꺾이는 부분을 찾으면 된다.
	- 여기서 inertia는 데이터가 얼마나 잘 뭉쳐있는지를 나타내는 지표로, 당연히 1일 때 가장 높고 점점 줄어들 것이다. 하지만, 급격하게 inertia값이 떨어지다가 갑자기 완만해지는 부분이 있다면, 그 전단계가 최적의 k라고 볼 수 있을 것이다.

8. KMeans의 랜덤성을 줄여주는 n_init 매개변수
	- KMeans에서 클러스터의 중심이 무엇으로 설정되느냐에 따라 학습의 결과가 크게 달라질 수 있다.
	- 이 때 n_init매개변수를 통해서 일반화된 성능을 기대할 수 있다.

### 주성분 분석
# 차원 축소
1. 차원축소란?
	- 우리가 100 x 100 픽셀의 이미지를 학습할 때 각 픽셀을 특성으로 보아 10000개의 특성을 가진 데이터 샘플이라고 생각했다.
	- 이를 50개의 주요한 차원으로 축소할 수 있다는 것을 의미한다.
	- 각 50개의 주성분들은 원본벡터 10000개의 조합으로 이루어져 있을 것이다.
ex) 	a(1번 주성분) = w1*(1번픽셀) + w2*(2번픽셀) + ... + w9999*(9999번픽셀) + w10000*(10000번픽셀) , w는 각 가중치.

2. pca를 이용하여 특성의 개수를 줄인 뒤 그 특성을 표현해보기
	- fruits_300은 (100, 100)픽셀의 사진이 300장 들어있는 데이터 샘플이다.
	- 이를 (300, 10000)의 형태로 각 이미지가 10000개의 특성을 가지도록 변환하고, 이를 pca를 이용한 주성분분석으로 50개의 주성분으로 줄일 것이다.
	- 그렇다면 pca.components_ 라는 변수에는 (50, 10000)의 배열이 들어가는데, 이는 각 50개의 주성분 벡터가 원본 벡터의 특정 조합으로 이루어져있다는 것을 의미하게 된다.
	- 이를 통해서 다시 (50, 100, 100)의 형태로 변환한 뒤 plt.imshow()로 그려볼 수 있다.

3. 설명된 분산이란? (explained_variance_ratio_) 
	- 주성분 분석(pca)를 이용하여 특성을 줄인 경우, 이를 다시 pca.inverse_transform() 매서드를 통해서 원본의 형태로 데이터를 재변환할 수 있다. 이때 얼마나 원본을 정확히 반영하냐를 의미하는 것이 설명된 분산이다.
	- 이는 pca.transform(fruits_2d)와 같이 변환하고, pca.inverse_transform(fruits_pca)와 같이 재변환 할 수 있다.
	- 설명된 분산은 np.sum(pca.explained_variance_ratio_)로 우리가 pca = PCA(n_components=50)을 통해서 생긴 50개의 주성분이 원본 데이터의 얼마나를 재현할 수 있는지를 의미한다.
	- pca.explained_variance_ratio_ 의 각 원소들은 그 주성분이 원본 데이터의 얼마나를 구현하는 지를 의미함.

4. PCA의 n_components를 실수값으로 주는 경우
	- 이 경우 자동으로 explained_variance_ratio_가 처음으로 n_components를 넘는 정수의 n_components(주성분 개수)를 계산하여 넣어준다.
	- 예를 들어 n_components=0.5로 한다면 처음으로 0.5를 넘게 되는 주성분 개수가 자동으로 선택되는 것임.

# PCA와 다른 알고리즘을 결합하여 사용해보기
1. LogisticRegression과 결합하여 여러가지 비교해보자
	- 위의 fruits_300의 데이터를 그대로 사용하여 회귀 모델로 학습을 해보자.
	- 각 클래스(과일 종류)를 0, 1, 2의 값으로 두고, 회귀모델로 학습을 하면 이론상 1에 가까운 점수가 나올 것이다.
	- 아레와 같이 학습을 하는 데 걸리는 시간이 줄어들었지만, 점수의 차이는 유의미하게 발생하지 않음.
ex)
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
target = np.array([0] * 100 + [1] * 100 + [2] * 100)
print(target) # 1차원 배열로 300개의 원소가 생김.

from sklearn.model_selection import cross_validate
scores = cross_validate(lr, fruits_2d, target) # 모델과 전체 샘플 데이터, 정답 데이터를 넘겨줌.
print('각 폴드 당 점수의 평균 : ', np.mean(scores['test_score'])) # 각 검증에서의 점수의 평균을 계산 # 0.997
print('각 폴드 당 걸린 시간 평균 : ', np.mean(scores['fit_time'])) # 각 사이클에서 데이터 학습과 검증까지 걸리는 시간의 평균을 계산 # 2.776

### pca로 변환한 데이터로 로지스틱 회귀를 수행한 검증 결과
scores = cross_validate(lr, fruits_pca, target) # fruits_pca는 주성분 50개로 변환된 데이터임.
print('pca를 사용한 각 폴드 당 점수의 평균 : ', np.mean(scores['test_score'])) # 각 검증에서의 점수의 평균을 계산 # 1.0
print('pca를 사용한 각 폴드 당 걸린 시간 평균 : ', np.mean(scores['fit_time'])) # 각 사이클에서 데이터 학습과 검증까지 걸리는 시간의 평균을 계산 # 0.072

2. KMeans와 pca를 결합한 경우.
	- 원본 데이터를 reshape(-1, 10000)만 해서 KMeans에 돌려보고,
	- pca로 변경한 데이터를 KMeans에 돌려본 뒤 나온 결과를 비교해보자.
	- 아레의 km.labels_에 클러스터링을 한 결과가 들어가게 된다.
	- 아레에서는 np.unique(km.labels_, return_counts=True)를 통해서 각 클래스에 몇개의 과일이 들어갔는지를 한눈에 보기 쉽게 되어있다.
ex)
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

if __name__ == '__main__':
    fruits = np.load('fruits_300.npy')
    fruits_300 = fruits.reshape(-1, 100*100)
    pca = PCA(n_components=50)
    fruits_pca = pca.fit_transform(fruits_300)

    km = KMeans(n_clusters = 3, n_init = 10)
    km.fit(fruits_pca)
    print(np.unique(km.labels_, return_counts=True)) # (array([0, 1, 2]), array([ 98, 111,  91], dtype=int64))

    km = KMeans(n_clusters = 3, n_init = 20)
    km.fit(fruits_300)
    print(np.unique(km.labels_, return_counts=True)) # (array([0, 1, 2]), array([ 91,  98, 111], dtype=int64))


### DNN(Deep Neural Network)
# tensorflow의 keras에서 데이터 불러오기
	- 이번에 주로 사용할 Fashion Mnist 데이터를 tensorflow의 keras에서 불러올 수 있다.
	- keras.datasets.fashion_mnist.load_data()로 데이터를 불러올 수 있는데 이 함수는 2개의 데이터 쌍을 반환한다.
	- (학습 데이터 세트), (테스트 데이터 세트)의 두 세트를 반환함.	그래서 아레와 같이 데이터를 받아야 함.
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
	- 위의 학습 데이터와 테스트 데이터는 이미 나누어진 상태로 고정된 데이터들이다.
	- fashion_mnist 또한 mnist와 같이 10개의 클래스를 구분하는 문제이다.

# 로지스틱 회귀를 통한 Fashion_mnist 데이터 학습.
	1. 로지스틱 회귀는 선형 모델이므로 from sklearn.linear_model import LogisticRegression 으로 불러온다.
lr = LogisticRegression(max_iter=1000)
	2. 선형 모델의 경우 데이터의 전처리가 필요하므로 (-1, 28, 28)의 형태로 되어있는 데이터를 (-1, 28*28)의 형태로 바꾸어서 StandardScaler에서 전처리를 해준다.
train_input1d = train_input.reshape(-1. 28*28)
test_input1d = test_input.reshape(-1. 28*28)
ss.fit(train_input)
train_scaled = ss.transform(train_input1d)
test_scaled = ss.transform(test_input1d)
	3. 로지스틱 회귀 모델에 넣어 학습을 한 후 테스트 세트에 대한 점수를 확인한다.
lr.fit(train_scaled, train_target)
print(lr.score(test_scaled, test_target))

# SGDClassifier(확률적 경사 하강법)을 이용한 로지스틱 회귀
	1. sc = SGDClassifier(loss='log_loss', max_iter=1000) 로 객체 선언
	2. 위의 전처리 과정
	3. cross_validate()함수에 넣어서 교차 검증 및 일반화 점수를 확인한다.
from sklearn.model_selection import cross_validate
scores = cross_validate(sc, train_scaled, train_target, cv=10, n_jobs=-1)
print(scores['test_score'])로 점수 확인.

# keras의 DNN을 사용하여 학습
	1. 케라스의 모델 만들기
		- 학습 데이터 중 일부를 검증 데이터로 뺀다.
		- DNN은 교차 검증을 사용하지 않음. 충분히 데이터가 많기 때문에 교차검증을 하지 않아도 충분히 일반화된 성능이라고 유추할 수 있다.
	2. dense 층 구성
		- Dense층은 모든 입력 뉴런이 모든 출력 뉴런과 연결되는 층이다.
		- 첫 Dense층은 데이터의 특성(여기선 784개)의 개수가 입력으로 들어오고, 출력의 개수는 우리가 정할 수 있다.
		- 마지막 Dense층은 출력층이라고 해서 결과로 나눌 클래스(여기선 10개)의 개수로 한다.
		- 또한 각 층은 전 층의 출력 개수를 입력의 개수로 받고 자신이 출력할 개수를 정한다.
		- 마지막 Dense층(출력층)은 softmax를 사용한다.
	3. 학습
		- 위와 같이 모델의 층을 다 구성했다면 모델이 만들어지고, 이제 학습을 진행한다
		- model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])와 같이 모델에서 사용할 손실 함수를 지정해주고, 정확도를 출력하도록 한다.
		- model.fit(train_scaled, train_target, epochs=5)로 테스트 세트와, epochs(몇번 반복할지) 를 정해주며 모델이 학습을 시작한다.


### 심층 신경망 (layer가 여러개인 신경망)
# 은닉층을 추가하여 인공 신경망 구성하기
	: dense층을 여러 개 만들고, 모델을 Sequential()로 만들 때 리스트에 모두 넣어 생성함.
ex)
dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)) 
dense2 = keras.layers.Dense(10, activation='softmax')
model = keras.Sequential([dense1, dense2])
model.summary()
	- 첫 입력 층에서는 input_shape=(784,) parameter를 통해서 처음 입력의 크기를 지정해줘야함.
	- 마지막 출력 층에서는 최종으로 분류를 수행할 클래스의 개수(10개)로 뉴런의 크기를 지정해주어야 함.
	- 마지막 출력 층의 활성화 함수는 다중 분류 문제에서는 softmax로 한다.

# dense층
	: dense층은 대표적인 완전연결 층으로, 입력 뉴런과 출력 뉴런을 모두 곱하여 각 가중치와 절편을 학습한다.
	- 즉, 100개의 입력 뉴런, 10개의 출력 뉴런이 정해져 있다면 모든 뉴런들을 곱하고, 출력 뉴런만큼의 절편을 학습하여 1010개의 parameter를 학습하는 것이다.
	- 모델의 최종 출력을 이끄는 층의 역할을 한다.

# 층을 추가하는 방법
	1. 층을 미리 만들어 놓고 keras.Sequential()함수를 통해 리스트의 형태로 각 층들을 순서대로 넣어준다.
	2. keras.Sequential()함수의 parameter로 리스트 속에 각 층의 생성자를 넣어준다.
	3. model = keras.Sequential()함수로 빈 깡통 모델을 만들고, model.add()를 통해 층을 추가한다.

# 모델의 학습
	: 위에서는 모델을 생성하고, 층을 구성하는 방법을 배움. 여기서는 모델을 직접 compile하고, fit()하는 방식을 보자
	- 모델을 생성했으면, model.compile()을 통해서 손실 함수를 지정해주고, 각 step당 알고 싶은 정보를 metrics=['accuracy']와 같이 지정해준다.
	- model.fit()을 통해 학습 데이터 세트, epochs를 지정해주어 학습을 시작한다.
	- 모델의 검증은 model.evaluate(val_scaled, val_target)으로 모델이 만들어진 후 할 수도 있고,
	- model.compile()을 하면서 validation_data=(val_scaled, val_target)을 통해 학습 도중에도 계속 검증을 해볼 수 있다.

# Flatten 층
	: dense층은 완전연결층으로 1D의 데이터 샘플만을 받으므로, Flatten층을 통해 다차원 데이터를 1차원으로 펼쳐주는 역할을 한다.


### 신경망 모델 활용
# FashionMnist 데이터를 가지고 간단한 Dense층들만을 이용해서 간단한 모델을 만들어보자.
	- 간단한 모델을 생성하여 반환해주는 함수를 작성해보자. 매개변수로 모델들이 들어있는 리스트를 받고, 이를 기본 모델 (Flatten, 최초 Dense층, 출력층)만 있는 모델의 출력 층 전에 넣어주는 함수로 설계.
	- (train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data() 로 데이터를 불러온다.
	- 모델을 만들고, summary() 함수를 통해서 내가 생각한 구조가 맞는지 확인.
	- 모델을 compile()하면서, optimizer와, loss함수, 학습률(learning_rate)를 optimizer와 함께 설정해준다
	- 모델을 fit()하면서 학습 데이터 세트, 검증 데이터 세트, epochs를 지정해준다. 이 때 verbose도 지정해줄 수 있는데 기본값은 1로 학습을 하면서 그 과정을 보여주는 것이다.
	- 또한 fit()함수의 반환값으로 history를 받을 수 있는데, history.history에는 dictionary의 형태로 각 key에 대한 각 단계에서의 점수들이 저장된다.

# 검증 데이터 손실값과 적당한 epochs값 찾기
	- 모델을 fit()하고 나온 history객체에는 각 epochs마다 검증 세트에 대한 손실값과 정확도도 들어있다.
	- 이 값들을 plot으로 그려보고, loss가 점점 감소하다가 증가하게 되는 그 시점의(최소값)을 찾아서 그 때를 적정 epochs라고 생각하자.

# Dropout층.
	: 드롭 아웃은 은닉층(입력과 출력 사이에 있는 층)에 적용하는 방식으로 작용하는 층이다.
	model.add(keras.layers.Dense(100, activation='relu'))
	model.add(keras.layers.Dropout(0.3))
	model.add(keras.layers.Dense(10, activation='softmax'))
	- 예를 들어 위와 같이 Dropout 층이 들어간다면, 이전 은닉층의 100개의 뉴런이 Dropout층으로 들어올 텐데, 이 중 랜덤으로 30%를 비활성화 하고, 다음 층의 입력으로 내보내는 역할을 한다.
	- 드롭 아웃 층을 사용하는 이유는, 학습 도중에 뉴런들이 특정 패턴이나, 뉴런에 지나치게 의존하지 않도록 도와준다. 만약 특정 뉴런이 학습에 미치는 영향이 지배적이라면, 그 뉴런이 비활성화 되었을 때 다른 뉴런들도 학습에 영향을 더 줄 수 있기 때문이다.

# CallBack
	: 훈련 도중에 어떤 작업을 수행할 수 있게 하는 객체이다.
	- callback은 우리가 모델을 학습하면서, 각 epoch마다 callback의 조건을 만족하거나, 학습이 끝난 후 지정한 조건의 step에서의 상태(가중치)를 하나의 모델로 저장하거나, 다른 일을 수행하게 한다.
		1. val_loss가 가장 작았을 때의 가중치로 모델을 저장할 수도 있다.
		2. val_loss가 가장 작았을 때를 기억하고, 그 이후로 val_loss가 두번 연속 증가하면 조기종료를 하고, val_loss가 가장 작았을 때 or 조기종료를 할 때 기준의 모델을 저장한다.
		3. 학습 도중에 learning_rate를 조정할 수도 있음.
ex)	학습 종료 후 val_loss가 가장 작았을 때를 기억하고, 파일로 저장하는 콜백
	checkpoint_cb = keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')
	- 위 콜백은 val_loss를 기준으로 가장 낮았던 시점의 모델을 best_model.h5라는 파일로 저장하는 것.

ex)	val_loss가 두번 연속으로 다시 증가하는 시점에 학습을 조기종료하고, 가장 좋았던 시점의 모델을 저장하는 callback
	early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)


### 합성곱 신경망
# 완전 연결 신경망
	: 입력 뉴런과 출력 뉴런을 모두 완전히 연결하여 각각마다 가중치를 갖게 하는 완전연결층으로 이루어진 신경망을 의미.

# 합성곱 신경망 개념
	: 필터를 통해서 입력 데이터를 새로운 특성들(특성맵)으로 바꾸는 것을 의미함.
	- 하나의 특성마다 가중치를 계산해서 하는것이 아닌, 필터를 통해 걸러진 정보로 새로운 특성들(특성맵, out_channels)을 만드는 것이 목표임.
	- 각 필터가 한번 씩 Capture를 하면 아레의 필터와 해당하는 픽셀들의 행렬곱을 수행하여 새로운 값이 됨.
필터 예시
	1 0 -1
	1 0 -1
	1 0 -1
	- 이를 통해 더 깊은 차원의 정보를 학습하고, 특성의 크기를 늘리고, 줄일 수 있다.
	- 각각의 필터는 특정한 특징 (수직선, 수평선, 엣지 등)을 학습하고, 여러 필터가 각기 다른 특징을 감지한다.
	- 활성화 함수 또한 convolution층 다음에 적용을 해 주어야 한다.

# Padding
	- 만약 패딩이 없이 그냥 필터를 통해 데이터를 훑게 된다면, 가장자리에 있는 얘들은 비교적 적게 훑어지게 될 것이다.
	- 그러면 합성곱 연산(특성맵을 만드는)에 참여하는 비중이 적어지기 때문에, 고르지 못하다.
	- 그렇기에 패딩을 넣어주어 특성맵에 골고루 참여할 수 있게 해주는 것.

# Stride
	- 필터를 통해 입력 데이터(특성맵이든 원본이미지든)를 훑을 때 몇 칸을 띄어넘고 훑을 지를 의미함.
	- Stride=2로 한다면 padding의 유무에 따라 다르지만, 크기가 반 정도 줄어든다고 보면 된다.(특성 맵의 크기가 그렇고, 개수는 그대로임)

# Pooling
	: 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행한다.
	- 왜 줄이느냐? 모든 픽셀에 대한 정보를 유지할 필요는 없고, 중요한 정보에 대한 내용만 남기기 위해서.
	- 특성맵의 크기를 줄여 계산량을 줄이고, 모델의 복잡도와 과대적합을 방지하는데 사용함.
	- 대게 pooling의 사이즈는 2로 하는데, 이는 너무 많은 정보의 손실이 발생하면 안되기 때문.

# conv2D의 예시
	1. convolution 층을 만든다. (입력 channel 크기, out channel 크기(필터의 개수), kernel_size = 3, padding='same')과 같이
	2. forward()에서 학습이 수행될 때에는 convolution층 다음에 activation함수를 통과하도록 해줘야함.
	3. 이를 이용해서 pytorch에서는 nn.functional.relu(self.conv1(x))와 같이 한번에 연결하기도 함.



























