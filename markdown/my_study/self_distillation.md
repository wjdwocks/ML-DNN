# Self-Distillation을 사용하는 이유

Self-Distillation은 딥러닝 모델의 성능과 일반화 능력을 높이기 위해 고안한 기법임. 사용하는 주요 이유는 다음과 같다.

- **자원 효율성**
  - 대형 모델은 이미 수십억 개의 파라미터를 다루기 때문에 더 큰 Teacher 모델을 따로 학습하는 건 계산 비용과 메모리 측면에서 부담됨.
  - Self-Distillation은 동일한 모델 내에서 과거 학습 결과(logits)나 내부 표현(feature map)을 활용해 추가 모델 구성 없이 성능 개선 가능함.

- **반복적 자기 개선**
  - 모델이 자신의 예측 분포(soft label)나 내부 feature를 반복적으로 재학습해 초기의 불확실성이나 노이즈가 점진적으로 보정됨.
  - 이 과정이 최종 모델의 예측력을 높이고 과적합을 줄이는 데 도움 줌.

- **내부 표현 강화**
  - 네트워크 내부의 중간 계층(feature map 등)에서 얻은 정보를 활용해, 모델이 다양한 수준의 특징을 더 정교하게 학습함.
  - 단순히 최종 출력만 맞추는 게 아니라 전체 네트워크의 표현력과 추상화 능력을 높임.

- **학습 안정성 향상**
  - Soft label은 일반적인 one-hot 정답보다 클래스 간의 관계와 불확실성 정보를 포함함.
  - 이를 통해 모델은 더 부드러운 학습 신호를 받아들이고 학습 과정의 불안정성을 완화함.

---

# Self-Distillation과 전통적인 KD와 다른 점

전통적인 Knowledge Distillation(KD)과 Self-Distillation은 몇 가지 주요 측면에서 차이가 있다.

- **Teacher 모델의 존재 여부**
  - **전통적인 KD:**  
    - 큰 Teacher 모델이 있어, Student 모델에게 soft label이나 내부 feature 정보를 전달함.
    - Teacher 모델은 보통 Student보다 훨씬 복잡하고 파라미터 수가 많음.
  - **Self-Distillation:**  
    - 별도의 Teacher 모델 없이 동일한 네트워크 내에서 과거 학습 결과나 내부 표현을 Teacher 역할로 사용함.
    - 시스템 설계가 단순하고 추가적인 대형 모델 구축 없이도 증류 효과를 얻을 수 있음.

- **지식 전달 방식**
  - **전통적인 KD:**  
    - Teacher와 Student는 서로 다른 아키텍처일 수 있으며, Teacher의 soft label이나 중간 표현을 직접적으로 Student에 전달함.
  - **Self-Distillation:**  
    - 동일한 모델의 여러 시점 또는 내부 레이어 간의 정보 전달을 통해 진행됨.
    - 시간적 또는 계층적 자기 지도를 활용함.

- **학습 과정의 반복성**
  - **전통적인 KD:**  
    - 보통 한 번의 distillation 과정으로 진행되며, Teacher 모델은 고정된 상태에서 Student가 이를 모방함.
  - **Self-Distillation:**  
    - 모델이 반복적인 학습 단계를 거치며 매 단계마다 이전 학습 결과를 참고해 개선됨.

- **모델 경량화 및 단순성**
  - **전통적인 KD:**  
    - 대형 Teacher 모델과 별도의 Student 모델을 관리해야 해서 구현 및 관리 측면에서 복잡해짐.
  - **Self-Distillation:**  
    - 동일한 네트워크 구조 내에서 교사와 학생 역할을 동시에 수행하기 때문에 시스템 설계와 하이퍼파라미터 관리가 더 단순함.

---

# Self-Distillation의 작동 원리

Self-Distillation은 모델이 스스로를 개선하는 과정을 통해 성능을 높이도록 설계됨. 작동 원리는 다음 단계로 요약됨.

## 1. 초기 학습 단계

- **기본 지도학습 수행**
  - 모델은 주어진 데이터와 정답을 사용해 일반적인 지도학습 과정을 거침.
  - 이때 보통 Cross Entropy Loss(분류 문제)나 MSE Loss(회귀 문제) 같은 기본 loss를 사용함.

- **내부 표현 및 예측 결과 기록**
  - 초기 학습 단계에서 모델이 출력하는 soft label이나 중간 레이어의 feature map 등을 저장해 두어 이후에 교사 역할로 활용함.

## 2. 증류 단계

- **과거 정보와의 비교**
  - 저장된 과거 정보(soft label 또는 내부 표현)를 현재 모델의 출력과 비교함.
  - 이 과정에서 Kullback-Leibler Divergence(KL Divergence - logits 비교.)나 Mean Squared Error(MSE - Feature Map 혹은 Similarity Map 비교) 같은 loss 함수를 사용해 두 분포 간의 차이를 최소화함.

- **Combined Loss 계산**
  - 최종 손실 함수는 기본 loss와 증류 loss를 가중치를 두어 합산한 형태로 구성됨.
  - 예시:
    ```
    L_total = (1-λ) * CE_Loss + λ * KD_Loss
    ```
    - λ는 증류 loss의 기여도를 조절하는 하이퍼파라미터임.

## 3. 반복적 자기 개선

- **시간적 앙상블 또는 반복 학습**
  - 모델이 여러 에포크 또는 단계에 걸쳐 반복적으로 자기 자신의 예측 분포나 내부 표현을 개선함.
  - 예를 들어, 초기 몇 에포크 동안 저장된 모델 상태를 기준으로 이후 단계에서 새로운 모델을 학습하고, 성능이 개선되면 업데이트된 모델이 다음 단계의 교사 역할을 수행함.

- **내부 표현 정제**
  - 네트워크 내부의 여러 레이어에서 추출된 feature map들이 서로 비교되고 정렬되면서 전체 모델이 더 일관된 표현을 학습함.
  - 이를 통해 모델의 복잡한 패턴 인식 능력이 강화됨.

## 4. 최종 모델의 성능 향상

- **일반화 능력 증진**
  - 반복적 자기 개선과 soft label을 통한 학습은 모델이 데이터의 노이즈를 줄이고, 과적합 없이 일반화된 패턴을 학습하도록 도움.
  
- **안정성 강화**
  - 자기 자신의 과거 예측을 활용해 학습 과정에서 나타날 수 있는 불안정성을 완화하고, 더 안정적인 성능을 유지함.

---

# 아키텍처와 최종 성능의 관계

"결국 여러 번 Student를 학습해 보고, 가장 좋은 모델을 찾는 것과 비슷한것 아닌가?"<br>
"그 여러 번 Student를 학습하는 횟수를 줄이기 위한 방법이라고 보는게 맞는 않은가?"<br>

- “아키텍처가 같으니 최종 성능 한계가 같을 것”이라고 보기엔, **학습 과정에서 어느 지점의 파라미터로 수렴하느냐**가 매우 중요함.

- 딥러닝 모델은 최적화 과정에서 여러 국소 최적점(local optimum)이나 안장점(saddle point) 중 하나로 수렴함.
- 같은 아키텍처라도 학습 절차나 초기화 등에 따라 전혀 다른 파라미터 세트로 수렴할 수 있음.

---

## Self Distillation의 역할

- **과거 모델(teacher)의 soft label이나 내부 표현**을 통해 더 나은 방향으로 모델을 유도함.
- 단순히 동일 구조를 한 번 학습했을 때보다 **더 좋은 국소 최적점**에 도달할 가능성을 높임.
- 결국 **일반화 성능**이 더 우수한 지점을 찾도록 도움.

---

## 모델 용량과 최종 수렴 지점

- 모델 용량(파라미터 수)은 같아도, **학습 과정**과 **최종 수렴 지점**이 달라지면 성능 차이가 생길 수 있음.
- “마지막까지 가봤자 그 성능이 그 성능”이라기보다는,  
  **동일 아키텍처라도 학습 과정의 지식 재활용(증류)** 덕분에 **더 좋은 수렴 지점**을 얻어  
  성능이나 일반화가 향상될 수 있음.

---

# 결론

Self-Distillation은 별도의 대형 Teacher 모델 없이 동일 네트워크 내에서 반복적인 자기 개선을 통해 성능을 향상시킬 수 있는 강력한 기법임.

- **Self-Distillation을 사용하는 이유:**  
  자원 효율성, 반복적 자기 개선, 내부 표현 강화, 학습 안정성 향상을 통해 모델의 일반화 성능을 높임.

- **전통적인 KD와의 차이점:**  
  별도의 Teacher 없이 동일 아키텍처에서 시간적 또는 계층적 정보 전달을 활용하며, 반복적 학습 과정을 통해 지속적으로 성능을 개선함.

- **작동 원리:**  
  1. 초기 학습을 통해 모델이 기본적인 예측을 수행하고 내부 정보를 기록함.  
  2. 저장된 정보를 기반으로 증류 loss를 계산해 현재 모델의 출력과 비교하고 개선함.  
  3. 이 과정을 반복해 모델의 예측 분포와 내부 표현을 점진적으로 정제하며 최종 성능을 향상시킴.
