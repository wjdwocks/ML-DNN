# 논문 초안

## Abstract
시계열 데이터 분석은 의료, 산업, IoT 등 다양한 분야에서 중요한 연구 주제로 주목받고 있다. 특히 Edge 단에서 데이터를 직접 수집하고 실시간으로 분석하는 환경에서는 다음과 같은 두 가지 주요 문제가 발생한다. 

첫째, 센서 데이터에 포함된 심각한 노이즈로 인해 모델의 Robustness가 저하되는 문제,  
둘째, 제한된 리소스 환경으로 인해 모델 성능이 떨어진다는 문제이다.  

최근 Topological Data Analysis (TDA)는 데이터의 구조적, 위상적 특성을 효과적으로 포착할 수 있어 노이즈에 강건한 특성을 제공하며 머신러닝 분야에서 주목받고 있다. 또한 Gramian Angular Field (GAF)는 시계열 데이터의 시각 간 상관관계를 효과적으로 이미지로 변환하여, 원본 시계열 데이터가 담지 못하는 중요한 정보를 보완할 수 있다.  

본 연구에서는 TDA 기반 Persistence Image (PI), GAF, 원본 Signal을 각각 Teacher로 사용하는 3-Teacher Multi-Modality Knowledge Distillation 프레임워크를 제안한다. 이를 통해 Edge 환경에서 발생하는 노이즈 및 리소스 제한 문제를 효과적으로 극복하고, 최종 Student 모델의 경량화 및 성능 향상을 동시에 달성하고자 한다.

---

## Introduction
시계열 데이터 기반의 Human Activity Recognition (HAR)은 의료 모니터링, 스마트 홈, 웨어러블 디바이스 등 다양한 분야에서 널리 활용되고 있으며, 특히 Edge Device (Wearable Sensor Device)에서 실시간 처리를 목표로 많은 연구가 진행되고 있다. 그러나 Edge 환경에서는 다음과 같은 핵심적인 문제들이 존재한다.

- 센서 데이터에 포함된 노이즈로 인한 Robustness 저하
- 낮은 fps 및 제한된 연산 자원으로 인한 성능 저하

이러한 문제를 해결하기 위해 Topological Data Analysis (TDA)가 주목받고 있다.  
TDA는 데이터의 위상적 구조를 분석하여 노이즈에 영향을 덜 받는 Global Feature를 추출할 수 있다는 장점이 있다. 특히, Persistence Image (PI)는 복잡한 시계열 데이터에서 중요한 위상 정보를 요약하여 이미지로 표현할 수 있어, 시계열 데이터의 노이즈를 효과적으로 완화하는 데 유리하다.

또한, Gramian Angular Field (GAF)는 시계열 데이터의 시각적 구조를 이미지로 변환하여, 원본 데이터가 담지 못한 시각적 패턴과 시점 간 상관관계를 보완할 수 있다. 이를 통해 추가적인 Feature를 효과적으로 확보할 수 있다.

이와 함께, Knowledge Distillation (KD)는 대규모 Teacher 모델의 지식을 경량화된 Student 모델로 효과적으로 전달하여, 적은 파라미터로도 우수한 성능을 유지할 수 있는 방법이다.  
Knowledge Distillation은 크게 logits-based KD와 feature-based KD (SPKD)로 나뉘며, 본 연구에서는 두 가지 방법을 모두 통합하여 더 강력한 학습 신호를 제공하고자 한다.

따라서 본 연구에서는 다음과 같은 방법으로 문제를 해결하고자 한다.

- 노이즈에 대한 Robustness → TDA 기반 Teacher (Persistence Image) 도입
- fps 저하 및 리소스 제한 → Knowledge Distillation (logits-based + SPKD 기반 feature distillation) 적용
- 정보 보완 및 Multi-Modality → 원본 Signal, GAF, PI를 모두 활용한 Multi-Modality Teacher 설계

본 논문의 주요 기여점은 다음과 같다.

1. 3-Teacher Multi-Modality Knowledge Distillation Framework 제안  
    - 원본 시계열 데이터 (Signal), Gramian Angular Field (GAF), Persistence Image (PI)를 각각 독립적인 Teacher로 활용하는 멀티모달 지식 증류 구조를 새롭게 제안한다.
  
2. 멀티모달 정보 융합을 통한 Student 성능 향상  
    - 개별 Teacher의 성능이 Student보다 낮거나 유사하더라도, 멀티모달 지식 증류를 통해 최종 Student 모델의 성능이 더 우수해질 수 있음을 실험적으로 검증한다.
  
3. Edge 환경을 고려한 경량화 및 실용성 검증  
    - 제한된 리소스 환경에서도 높은 성능을 유지할 수 있는 경량 Student 모델을 구축하고, 이를 통해 실제 Edge 환경에 적합한 적용 가능성을 확인한다.

## Background

본 장에서는 본 연구의 이론적 배경이 되는 Topological Data Analysis (TDA), Gramian Angular Field (GAF), Knowledge Distillation, 그리고 Annealing 기법에 대해 구체적으로 설명한다.

- 1. Topological Data Analysis (TDA)

    - 1.1 TDA 개요
        - Topological Data Analysis (TDA)는 데이터의 형태(Shape)와 구조적 특성을 분석하는 기법으로, 데이터 내에 존재하는 노이즈를 효과적으로 필터링하고, 데이터의 전반적인 글로벌 구조를 파악하는 데 유용하다. 특히 TDA는 복잡한 고차원 데이터에서도 중요한 패턴을 안정적으로 추출할 수 있어 시계열 데이터 분석에 적합하다.

    - 1.2 Persistent Homology 생성 과정
        - TDA는 시계열 데이터를 먼저 고차원 Point Cloud로 임베딩하여 분석을 시작한다. 주로 Sliding Window Embedding 기법을 사용하여 시계열 데이터를 임베딩하며, 이를 통해 시계열 내의 시간적 구조를 공간적으로 재해석할 수 있다.

        - 이후 다음의 과정을 통해 Persistent Homology를 계산한다.
            - 임베딩된 Point Cloud에 다양한 반경을 갖는 필터를 적용하여 Simplicial Complex를 구성한다.
            - 반경이 증가함에 따라, 점, 선, 면 등 다양한 위상적 구조(Connected Components, Loops 등)가 생성되었다가 소멸된다.
            - 이러한 생성(Birth)과 소멸(Death)의 과정을 추적하여 Persistence Diagram을 작성한다.

    - 1.3 Persistence Image로의 변환
        - Persistence Diagram은 Birth-Death Pair를 2D 평면에 점으로 표현하는데, 머신러닝 모델이 직접 사용하기에는 입력 형식이 고정되지 않아 불편하다. 이를 해결하기 위해 Persistence Diagram을 Gaussian Kernel로 변환하여, 고정 크기의 Persistence Image (PI)를 생성한다.

        - Persistence Image는 머신러닝 모델이 학습할 수 있는 고정 차원의 Feature로 변환된 위상 정보이며, 시계열 데이터의 강건한 글로벌 패턴을 효과적으로 보존할 수 있다.

---

- 2. Gramian Angular Field (GAF)

    - 2.1 GAF 개요
        - Gramian Angular Field (GAF)는 시계열 데이터를 이미지로 변환하여, 시점 간의 상관관계를 시각적으로 표현할 수 있는 기법이다. GAF는 시계열 데이터의 시각적 패턴을 효과적으로 보완할 수 있으며, CNN 기반 모델이 시계열을 이미지처럼 처리할 수 있도록 해준다.

    - 2.2 GAF 생성 과정
        - GAF는 시계열 데이터의 각 Time Step 값을 먼저 Min-Max Scaling하여 [-1, 1] 구간으로 정규화한 뒤, 이를 극좌표계의 각도로 변환한다. 이때, 각 시점 \( x_t \)의 극좌표 각도 \( \phi_t \)는 다음과 같이 계산된다.
    
    - GAF는 두 각도를 더하거나 빼는 방식에 따라 GASF와 GADF로 나뉜다.
    - 본 연구에서는 Summation 방식인 GASF를 사용하며, 이를 통해 시계열 내 시점 간의 합성적 상관관계를 이미지로 시각화한다. GAF는 기존의 시계열 데이터가 가지는 일차원적 정보 외에, 시점 간의 상대적 패턴을 2차원 이미지로 추가 제공한다.

---

- 3. Knowledge Distillation (KD)

- 3.1 KD 개요
    - Knowledge Distillation은 복잡한 대규모 모델(Teacher)로부터 경량화된 모델(Student)로 지식을 전이하여, 적은 연산량으로도 높은 성능을 달성할 수 있도록 돕는 기법이다. 특히 Edge 환경에서 효율적인 모델 설계에 필수적인 전략으로 널리 활용된다.

    - KD는 주로 두 가지 형태로 수행된다.
        - Logits-based KD: Teacher의 Softmax 출력을 Student가 모방하도록 학습하며, Kullback-Leibler Divergence를 통해 손실을 계산한다.
  
        - Feature-based KD: Teacher와 Student의 Feature Map을 정렬시켜 유사한 Feature 공간을 학습하도록 유도한다.

    - 본 연구에서는 두 방식을 모두 결합하여, 더 강력한 학습 신호를 제공하고자 한다.

- 3.2 Multi-Teacher Knowledge Distillation 설계
    - 본 연구는 세 개의 Teacher 모델 (Signal, GAF, PI)을 사용하며, 각 Teacher로부터 별도로 KD Loss를 계산한다. 각 KD Loss는 가중치 \( \alpha_i \)를 적용하여 최종 Loss를 다음과 같이 설계한다.

\[
KD_{total} = \sum_{i=1}^{3} \alpha_i \cdot KD_{i}
\]

여기서 \( \alpha_1 + \alpha_2 + \alpha_3 = 1 \) 을 만족하도록 가중치를 설정하며, Multi-Modality 정보를 균형 있게 반영할 수 있도록 설계하였다.

- 3.3 Feature-based KD: SPKD
Feature-based KD는 Similarity-Preserving Knowledge Distillation (SPKD)를 기반으로 구현하였다.  
SPKD는 Teacher와 Student의 Feature Map에서 Similarity-Preserving Map (SP_Map)을 생성하여, Batch 간 Similarity 구조를 보존하도록 유도한다.

- 동일한 레이어 (3개 지점)에서 Teacher와 Student의 Feature Map을 추출
- Feature Map의 Batch 차원 간 내적을 통해 Batch × Batch 크기의 SP_Map 생성
- Teacher와 Student의 SP_Map 간 MSE Loss를 계산하여 Feature-based KD Loss 정의
- 이 손실을 Logits-based KD Loss와 동일한 가중치 체계로 최종 Loss에 통합

이를 통해 Student 모델이 Teacher의 Feature 간 유사성 구조를 효과적으로 모방하도록 학습시킨다.

---

- 4. Annealing 기법

- 4.1 Annealing 개요
Annealing 기법은 학습 초기에 Teacher의 지식 의존도를 높게 유지하고, 학습이 진행될수록 Student의 자율성을 점진적으로 증가시키는 기법이다. Knowledge Distillation에서는 주로 Soft Target Temperature와 Loss 가중치 조정을 통해 Annealing을 구현한다.

- 4.2 Temperature Scaling
KD에서 Softmax Temperature \( T \)를 조정하여 Teacher의 Soft Target 분포를 부드럽게 만들 수 있다. 초기에는 높은 Temperature를 사용하여 Soft Target을 평탄하게 만들고, 학습이 진행될수록 Temperature를 점진적으로 낮춘다.

- 4.3 Loss Weight Annealing
KD Loss에 적용되는 가중치 \( \lambda_{KD} \)를 학습 초기에 크게 설정하고, Epoch이 진행됨에 따라 이를 점진적으로 감소시킨다. 이를 통해 Teacher의 영향력을 학습 초반에 집중시키고, 후반부에는 Student가 자체적으로 일반화할 수 있도록 유도한다.

- 4.4 기대 효과
Annealing 기법을 통해 Student가 Teacher에 과도하게 의존하는 현상을 방지하고, 최종적으로 Student가 독자적인 Feature 표현을 학습하여 더 높은 일반화 성능을 달성할 수 있다.



## Proposed Method

본 장에서는 제안하는 3-Teacher Multi-Modality Knowledge Distillation 프레임워크의 주요 구성 요소를 설명한다. 시계열 데이터를 효과적으로 이미지로 변환하기 위해 Persistence Image (PI)와 Gramian Angular Field (GAF)를 어떻게 추출하였는지 기술하며, Multi-Teacher 기반 Knowledge Distillation의 전체 구조, 손실 함수 설계, 그리고 추가적으로 적용한 Annealing 기법에 대해 상세히 서술한다.

- 1. Persistence Image (PI) 생성 방법
    - 원본 시계열 데이터를 입력으로 받아 TDA 기반의 Persistence Diagram을 계산한다.
    - Persistence Diagram에서 각 Birth-Death Pair를 2D 평면에 매핑하고, 이를 Gaussian Kernel로 변환하여 Persistence Image를 생성한다.
    - 이를 통해 시계열 데이터의 위상적 특성을 이미지 형태로 효과적으로 추출한다.

- 2. Gramian Angular Field (GAF) 생성 방법
    - 시계열 데이터를 [-1, 1] 구간으로 Min-Max Scaling하여 극좌표계로 변환한다.
    - 변환된 값을 이용해 Gramian Angular Summation Field (GASF) 또는 Gramian Angular Difference Field (GADF)를 생성한다.
    - GAF는 시계열 데이터 내 시점 간 상관관계를 이미지로 시각화하며, 시계열 데이터의 시각적 패턴 정보를 보완한다.

- 3. Multi-Teacher Knowledge Distillation 설계
본 연구에서는 PI, GAF, 원본 Signal을 각각 독립적인 Teacher로 사용하며, Multi-Teacher 구조에서 Knowledge Distillation을 수행한다.

    - 3.1 Logits-based KD Loss 설계
        - 각 Teacher의 Soft Target과 Student의 출력 Logits 간의 Kullback-Leibler (KL) Divergence를 계산하여 Logits-based KD Loss를 구성한다.
        - Multi-Teacher 환경에서는 각 Teacher별 KD Loss를 가중합하여 최종 Logits-based Loss를 설계한다.

    - 3.2 Feature-based KD Loss 설계 (SPKD 에 맞춘 설명, 근데 그냥 Feature B)
        - Student와 각 Teacher의 Feature Map을 동일한 위치 (3개의 서로 다른 레이어)에서 추출한다.
        - 각 Feature Map으로부터 Similarity-Preserving Map (SP_Map)을 생성한다.
        - SP_Map은 Feature Map의 Batch 간 내적을 통해 Batch × Batch 크기의 Similarity Matrix로 계산된다.
        - Teacher와 Student의 SP_Map 간 Mean Squared Error (MSE)를 계산하여 Feature-based KD Loss로 정의한다.
        - Feature-based KD Loss 역시 각 Teacher별로 계산하며, Logits-based Loss와 동일한 가중치를 적용하여 최종 Loss에 통합한다.

    - 3.3 SP_Map 비교 실험 (Optional)
        - 각 Teacher (PI, GAF, Signal)에서 추출된 SP_Map을 동일한 샘플에 대해 시각적으로 비교하여, 멀티모달 Feature가 실제로 서로 다른 정보 패턴을 담고 있는지 확인한다.
        - 이를 통해 멀티모달 Teacher의 효과를 더욱 직관적으로 분석할 수 있다.

    - 3.4 최종 Loss 설계
        - 최종 Loss는 Cross Entropy (CE) Loss, Logits-based KD Loss, Feature-based KD Loss의 가중합으로 정의된다.

        Final Loss = CE Loss + λ₁ * KD_Loss (Logits) + λ₂ * KD_Loss (Feature)

- λ₁, λ₂는 Hyperparameter로서 Logits-based Loss와 Feature-based Loss의 중요도를 조절한다.

## 4. Annealing 기법 적용
- Knowledge Distillation에서 Soft Target의 효과를 조절하기 위해 Temperature Scaling과 함께 Annealing 기법을 적용한다.
- 초기 학습 단계에서는 Teacher의 영향을 강하게, 후반으로 갈수록 Teacher의 의존도를 줄이고 Student의 자율 학습을 유도한다.
- Annealing은 학습 Epoch에 따라 KD Loss의 가중치를 점진적으로 감소시키는 방식으로 구현한다.
- 이를 통해 Student 모델이 Teacher에 과도하게 의존하지 않도록 유도하며, 일반화 성능을 향상시킨다.

---

# Experiment

본 장에서는 제안한 방법의 성능을 검증하기 위해 수행한 실험 설계 및 결과를 설명한다.

## 1. 데이터셋 설명
- 사용한 시계열 데이터셋의 종류, 데이터 수집 방식, 샘플 수, 클래스 수 등을 상세히 설명한다.
- 데이터셋의 주요 통계량 (시계열 길이, Sampling Rate 등)을 제시한다.

## 2. 하이퍼파라미터 설정
- 각 데이터셋마다 적용한 주요 하이퍼파라미터 (Learning Rate, Batch Size, Temperature 등)를 제시한다.
- 하이퍼파라미터의 선택 근거를 실험 결과 또는 기존 연구를 인용하여 설명한다.

## 3. 실험 결과 및 분석
- Teacher별 성능, Student의 Knowledge Distillation 적용 전/후 성능을 비교하여 제시한다.
- Accuracy, Compression Ratio (모델 경량화 비율) 등을 표로 정리한다.
- Feature-based KD 적용 유무, Annealing 적용 유무에 따른 성능 차이도 함께 분석한다.

## 4. Ablation Study (선택 사항)
- PI, GAF, Signal 중 일부 Teacher를 제거했을 때 성능이 어떻게 변하는지 추가 실험을 통해 검증한다.
- Logits-based KD만 적용했을 때, Feature-based KD만 적용했을 때의 비교 실험을 수행한다.
- Annealing 기법 적용 여부에 따른 성능 차이를 분석한다.

