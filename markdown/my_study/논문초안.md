# 논문 초안

## Abstract
- 시계열 데이터 분석은 의료, 산업, IoT 등 다양한 분야에서 중요한 연구 주제로 주목받고 있다. 특히 Edge 단에서 데이터를 직접 수집하고 실시간으로 분석하는 환경에서는 다음과 같은 두 가지 주요 문제가 발생. 
    - 1. 센서 데이터에 포함된 심각한 노이즈로 인해 모델의 Robustness가 저하되는 문제,  
    - 2. 제한된 리소스 환경으로 인해 모델 성능이 떨어진다는 문제이다.  

- 최근 Topological Data Analysis (TDA)는 데이터의 구조적, 위상적 특성을 효과적으로 포착할 수 있어 노이즈에 강건한 특성을 제공하며 머신러닝 분야에서 주목받고 있다. 또한 Gramian Angular Field (GAF)는 시계열 데이터의 시각 간 상관관계를 효과적으로 이미지로 변환하여, 원본 시계열 데이터가 담지 못하는 중요한 정보를 보완할 수 있다.  

- 본 연구에서는 TDA 기반 Persistence Image (PI), GAF, 원본 Signal을 각각 Teacher로 사용하는 3-Teacher Multi-Modality Knowledge Distillation 프레임워크를 제안. 이를 통해 Edge 환경에서 발생하는 노이즈 및 리소스 제한 문제를 효과적으로 극복하고, 최종 Student 모델의 경량화 및 성능 향상을 동시에 달성하고자 .

---

## Introduction
- 시계열 데이터 기반의 Human Activity Recognition (HAR)은 의료 모니터링, 스마트 홈, 웨어러블 디바이스 등 다양한 분야에서 널리 활용되고 있으며, 특히 Edge Device (Wearable Sensor Device)에서 실시간 처리를 목표로 많은 연구가 진행되고 있다. 그러나 Edge 환경에서는 다음과 같은 핵심적인 문제들이 존재.
    - 센서 데이터에 포함된 노이즈로 인한 Robustness 저하
    - 낮은 fps 및 제한된 연산 자원으로 인한 성능 저하

- 이러한 문제를 해결하기 위해 Topological Data Analysis (TDA)가 주목받고 있다.  
    - TDA는 데이터의 위상적 구조를 분석하여 노이즈에 영향을 덜 받는 Global Feature를 추출할 수 있다는 장점이 있다. 특히, Persistence Image (PI)는 복잡한 시계열 데이터에서 중요한 위상 정보를 요약하여 이미지로 표현할 수 있어, 시계열 데이터의 노이즈를 효과적으로 완화하는 데 유리하다.

- 또한, Gramian Angular Field (GAF)는 시계열 데이터의 시각적 구조를 이미지로 변환하여, 원본 데이터가 담지 못한 시각적 패턴과 시점 간 상관관계를 보완할 수 있다. 이를 통해 추가적인 Feature를 효과적으로 확보할 수 있다.

- 이와 함께, Knowledge Distillation (KD)는 대규모 Teacher 모델의 지식을 경량화된 Student 모델로 효과적으로 전달하여, 적은 파라미터로도 우수한 성능을 유지할 수 있는 방법이다.  
- Knowledge Distillation은 크게 logits-based KD와 feature-based KD (SPKD)로 나뉘며, 본 연구에서는 두 가지 방법을 모두 통합하여 더 강력한 학습 신호를 제공하고자 한다.
- 따라서 본 연구에서는 다음과 같은 방법으로 문제를 해결하고자 힌디.
    - 노이즈에 대한 Robustness → TDA 기반 Teacher (Persistence Image) 도입
    - fps 저하 및 리소스 제한 → Knowledge Distillation (logits-based + SPKD 기반 feature distillation) 적용
    - 정보 보완 및 Multi-Modality → 원본 Signal, GAF, PI를 모두 활용한 Multi-Modality Teacher 설계

- 본 논문의 주요 기여점은 다음과 같다.
    1. 3-Teacher Multi-Modality Knowledge Distillation Framework 제안  
        - 원본 시계열 데이터 (Signal), Gramian Angular Field (GAF), Persistence Image (PI)를 각각 독립적인 Teacher로 활용하는 멀티모달 지식 증류 구조를 새롭게 제안.
    2. 멀티모달 정보 융합을 통한 Student 성능 향상  
        - 개별 Teacher의 성능이 Student보다 낮거나 유사하더라도, 멀티모달 지식 증류를 통해 최종 Student 모델의 성능이 더 우수해질 수 있음을 실험적으로 검증.
    3. Edge 환경을 고려한 경량화 및 실용성 검증  
        - 제한된 리소스 환경에서도 높은 성능을 유지할 수 있는 경량 Student 모델을 구축하고, 이를 통해 실제 Edge 환경에 적합한 적용 가능성을 확인.

## Background

본 장에서는 본 연구의 이론적 배경이 되는 Topological Data Analysis (TDA), Gramian Angular Field (GAF), Knowledge Distillation, 그리고 Annealing 기법에 대해 구체적으로 설명.

- 1. Topological Data Analysis (TDA)
    - 1.1 TDA 개요
        - Topological Data Analysis (TDA)는 데이터의 형태(Shape)와 구조적 특성을 분석하는 기법으로, 데이터 내에 존재하는 노이즈를 효과적으로 필터링하고, 데이터의 전반적인 글로벌 구조를 파악하는 데 유용하다. 특히 TDA는 복잡한 고차원 데이터에서도 중요한 패턴을 안정적으로 추출할 수 있어 시계열 데이터 분석에 적합하다.
    - 1.2 Persistent Homology 생성 과정
        - TDA는 시계열 데이터를 먼저 고차원 Point Cloud로 임베딩하여 분석을 시작. 주로 Sliding Window Embedding 기법을 사용하여 시계열 데이터를 임베딩하며, 이를 통해 시계열 내의 시간적 구조를 공간적으로 재해석할 수 있다.
        - 이후 다음의 과정을 통해 Persistent Homology를 계산.
            - 임베딩된 Point Cloud에 다양한 반경을 갖는 필터를 적용하여 Simplicial Complex를 구성.
            - 반경이 증가함에 따라, 점, 선, 면 등 다양한 위상적 구조(Connected Components, Loops 등)가 생성되었다가 소멸된다.
            - 이러한 생성(Birth)과 소멸(Death)의 과정을 추적하여 Persistence Diagram을 작성.
    - 1.3 Persistence Image로의 변환
        - Persistence Diagram은 Birth-Death Pair를 2D 평면에 점으로 표현하는데, 머신러닝 모델이 직접 사용하기에는 입력 형식이 고정되지 않아 불가능하다. 이를 해결하기 위해 Persistence Diagram을 Gaussian Kernel로 변환하여, 고정 크기의 Persistence Image (PI)를 생성.
        - Persistence Image는 머신러닝 모델이 학습할 수 있는 고정 차원의 Feature로 변환된 위상 정보이며, 시계열 데이터의 Robustness한 글로벌 패턴을 효과적으로 보존할 수 있다.

- 2. Gramian Angular Field (GAF)
    - 2.1 GAF 개요
        - Gramian Angular Field (GAF)는 시계열 데이터를 이미지로 변환하여, 시점 간의 상관관계를 시각적으로 표현할 수 있는 기법이다. GAF는 시계열 데이터의 시각적 패턴을 효과적으로 보완할 수 있으며, CNN 기반 모델이 시계열을 이미지처럼 처리할 수 있도록 해준다.
    - 2.2 GAF 생성 과정
        - GAF는 시계열 데이터의 각 Time Step 값을 먼저 Min-Max Scaling하여 [-1, 1] 구간으로 정규화한 뒤, 이를 극좌표계의 각도로 변환. 이때, 각 시점 x_t에서의 극좌표 각도(phi_t)는 arccos 함수를 이용하여 계산된다.
        - 이후 GAF는 두 시점의 각도를 더하거나 GASF(cos(phi_i + phi_j)), 빼는 GADF(cos(phi_i - phi_j)) 방식으로 나뉜다.
    - 본 연구에서는 Summation 방식인 GASF를 사용하며, 이를 통해 시계열 내 시점 간의 합성적 상관관계를 이미지로 시각화. GAF는 기존의 시계열 데이터가 가지는 일차원적 정보 외에, 시점 간의 상대적 패턴을 2차원 이미지로 추가 제공.

- 3. Knowledge Distillation (KD)
    - 3.1 KD 개요
        - Knowledge Distillation은 복잡한 대규모 모델(Teacher)로부터 경량화된 모델(Student)로 지식을 전이하여, 적은 연산량으로도 높은 성능을 달성할 수 있도록 돕는 기법이다. 특히 Edge 환경에서 효율적인 모델 설계에 필수적인 전략으로 널리 활용된다.
        - KD는 주로 두 가지 형태로 수행된다.
            - Logits-based KD: Teacher의 Softmax 출력을 Student가 모방하도록 학습하며, Kullback-Leibler Divergence를 통해 손실을 계산.
            - Feature-based KD: Teacher와 Student의 Feature Map을 정렬시켜 유사한 Feature 공간을 학습하도록 유도.
        - 본 연구에서는 두 방식을 모두 결합하여, 더 강력한 학습 신호를 제공하고자 .
    - 3.2 Multi-Teacher Knowledge Distillation 설계
        - 본 연구는 세 개의 Teacher 모델 (Signal, GAF, PI)을 사용하며, 각 Teacher로부터 별도로 KD Loss를 계산. 각 KD Loss는 가중치 alpha 를 적용하여 최종 Loss를 다음과 같이 설계.
        - KD_logits = alpha1 * KD_teacher1 + alpha2 * KD_teacher2 + alpha3 * KD_teacher3
        - 여기서 alpha1 + alpha2 + alpha3 = 1 을 만족하도록 가중치를 설정하며, Multi-Modality 정보를 균형있게 반영할 수 있도록 설계된다.

- 4. Annealing 기법
    - Knowledge Distillation에서 Annealing 기법은 학습 초기에는 Student가 Teacher의 출력을 적극적으로 모방하도록 유도하고, 학습이 진행될수록 Teacher의 영향을 점차 줄이면서 Student의 독립적인 학습을 강화하는 전략이다. 이를 위해 주로 Softmax의 Temperature를 조절하여 Teacher의 Soft Target을 부드럽게 만들고, Epoch이 증가함에 따라 Temperature를 점진적으로 낮추는 방식이 사용된다.
    - 또한, KD Loss의 가중치를 점진적으로 감소시키는 Loss Weight Annealing 기법을 함께 적용하여, 학습 후반부로 갈수록 Student가 Teacher에 의존하지 않고 스스로 일반화할 수 있도록 유도. Annealing 기법을 통해 Student가 Teacher의 정보를 효과적으로 학습하되, 최종적으로는 Teacher에 과도하게 의존하지 않고 높은 독립성과 일반화 성능을 달성할 수 있다.


## Proposed Method

본 장에서는 제안하는 3-Teacher Multi-Modality Knowledge Distillation 프레임워크의 주요 구성 요소를 설명. 시계열 데이터를 효과적으로 이미지로 변환하기 위해 Persistence Image (PI)와 Gramian Angular Field (GAF)를 어떻게 추출하였는지 기술하며, Multi-Teacher 기반 Knowledge Distillation의 전체 구조, 손실 함수 설계, 그리고 추가적으로 적용한 Annealing 기법에 대해 상세히 서술.

- 1. Persistence Image (PI) 생성 방법
    - 원본 시계열 데이터를 입력으로 받아 TDA 기반의 Persistence Diagram을 계산.
    - Persistence Diagram에서 각 Birth-Death Pair를 2D 평면에 매핑하고, 이를 Gaussian Kernel로 변환하여 Persistence Image를 생성.
    - 이를 통해 시계열 데이터의 위상적 특성을 이미지 형태로 효과적으로 추출.
    - 이 논문에서는 PI Image의 크기를 64 x 64로 고정하였다.

- 2. Gramian Angular Field (GAF) 생성 방법
    - 시계열 데이터를 [-1, 1] 구간으로 Min-Max Scaling하여 극좌표계로 변환.
    - 변환된 값을 이용해 Gramian Angular Summation Field (GASF) Image를 생성.
    - GAF는 시계열 데이터 내 시점 간 상관관계를 이미지로 시각화하며, 시계열 데이터의 시각적 패턴 정보를 보완.
    - 이 논문에서는 GAF Image의 크기를 64 x 64로 고정하였다.

- 3. Multi-Teacher Knowledge Distillation 설계
    - 본 연구에서는 PI, GAF, 원본 Signal을 각각 독립적인 Teacher로 사용하며, Multi-Teacher 구조에서 Knowledge Distillation을 수행.
    - 3.1 Logits-based KD Loss 설계
        - 각 Teacher의 Soft Target과 Student의 출력 Logits 간의 Kullback-Leibler (KL) Divergence를 계산하여 Logits-based KD Loss를 구성.
        - Multi-Teacher 환경에서는 각 Teacher별 KD Loss를 따로 계산한 뒤, 가중합하여 최종 Logits-based Loss를 설계.

    - 3.2 Feature-based KD Loss 설계 (SPKD 에 맞춘 설명, 근데 그냥 Feature B)
        - Student와 각 Teacher의 Feature Map을 동일한 위치 (3개의 서로 다른 레이어)에서 추출.
        - 각 Feature Map으로부터 Similarity-Preserving Map (SP_Map)을 생성.
        - SP_Map은 Feature Map의 Batch 간 내적을 통해 Batch × Batch 크기의 Similarity Matrix로 계산된다.
        - Teacher와 Student의 SP_Map 간 Mean Squared Error (MSE)를 계산하여 Feature-based KD Loss로 정의.
        - Feature-based KD Loss 역시 각 Teacher별로 계산하며, Logits-based Loss와 동일한 가중치를 적용하여 최종 Loss에 통합.

    - 3.3 SP_Map 비교 실험 (Optional)
        - 각 Teacher (PI, GAF, Signal)에서 추출된 SP_Map을 동일한 샘플에 대해 시각적으로 비교하여, 멀티모달 Feature가 실제로 서로 다른 정보 패턴을 담고 있는지 확인.
        - 이를 통해 멀티모달 Teacher의 효과를 더욱 직관적으로 분석할 수 있다.

    - 3.4 최종 Loss 설계
        - 최종 Loss는 Cross Entropy (CE) Loss, Logits-based KD Loss, Feature-based KD Loss의 가중합으로 정의된다.
        - Final Loss = CE Loss + λ₁ * KD_Loss (Logits) + λ₂ * KD_Loss (Feature)
        - λ₁, λ₂는 Hyperparameter로서 Logits-based Loss와 Feature-based Loss의 중요도를 조절.

- 4. Annealing 기법 적용
    - Teacher와 Student는 서로 다른 데이터를 학습하기도 하고, Model Architecture가 다른 경우도 있기에, Knowledge Gap이 생긴다. 이것을 완화시키기 위해서 Stduent를 학습하기 전에 Student와 같은 크기, 같은 형태의 작은 모델을 처음부터 학습(랜덤 초기화)을 한 뒤, 그렇게 학습된 모델을 Student모델의 초기 가중치값으로 지정해주는 방식을 사용하였다.
    - 이렇게 함으로써 Teacher의 지식을 더 잘 전이받을 수 있게 됨.

---

## Experiment

본 장에서는 제안한 방법의 성능을 검증하기 위해 수행한 실험 설계 및 결과를 설명.

- 1. 데이터셋 설명
    - 사용한 시계열 데이터셋의 종류, 데이터 수집 방식, 샘플 수, 클래스 수 등을 상세히 설명.
    - 데이터셋의 주요 통계량 (시계열 길이, Sampling Rate 등)을 제시.

- 2. 하이퍼파라미터 설정
    - 각 데이터셋마다 적용한 주요 하이퍼파라미터 (Learning Rate, Batch Size, Temperature 등)를 제시.
    - 하이퍼파라미터의 선택 근거를 실험 결과 또는 기존 연구를 인용하여 설명.

- 3. 실험 결과 및 분석
    - Teacher별 성능, Teacher 조합 및 개수 별 성능, Student의 Knowledge Distillation 적용 전/후 성능을 비교하여 제시.
    - Accuracy, Compression Ratio (모델 경량화 비율) 등을 표로 정리.
    - Feature-based KD 적용 유무, Annealing 적용 유무에 따른 성능 차이도 함께 분석.
    - Baseline을 Task에 적용한 성능 차이도 함께 분석.
    - PAMAP2에서 성능이 오히려 낮아진 것에 대한 분석이 가능하다면 좋을듯.

- 4. Ablation Study (선택 사항)
    - Teacher 간 Architecture가 다른 경우에서의 성능 확인.
    - 같은 WideResnet 내에서 Depth와 Width가 다른 경우에서의 성능 차이 확인.
    - 완전히 다른 Architecture를 가지는 경우(Resnet, VGG, MN.v2 등.)에서의 성능 차이 확인.
    - Teacher1, Teacher2, Teacher3의 SP_Map을 Merge한 것과 실제로 Student의 SP_Map이 비슷해지는지 확인.
    - 

## Conclusion
- GENE Activ 데이터셋에서는 한 번의 Test만 수행하기 때문에, GAF Teacher, PI Teacher, Signal Teacher 중 어떤 것이 무엇을 잘하는지에 대한 것이 고정되어있었어서 고정된 alpha 조합을 사용했을 때 성능이 계속 좋아지는 것을 확인할 수 있었음.
- 하지만 PAMAP2의 경우 어떤 Subject 가 Test Set이 되느냐에 따라서, PI Teacher가 성능이 더 좋거나, Signal Teacher가 더 성능이 좋거나, GAF Teacher가 더 성능이 좋거나 하는 경우가 매 Test Set에 따라 달랐다. 이것 때문에, PAMAP2에서 더 정확한 실험을 위해서는 Test Set이 되는 Subject에 따라서 다른 Alpha 조합을 사용해야 했을 것으로 추정되는데, 그건 데이터에 하이퍼파라미터를 맞추는 느낌이라 의미가 없음.
- 즉, PAMAP2에서 특정 Alpha 조합이 특정 Test Set에서는 성능을 더 높이는 원인이 되었겠지만, 특정 Test Set에서는 성능을 더 낮추는 원인이 되었기 때문에, 결과적으로 성능에 큰 차이가 없었을 것으로 예상된다.

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.10/alpha_.png" alt="PAMAP2 분석" width="700">
