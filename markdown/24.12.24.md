## 24.12.24 까지 공부할 내용
<li> GENE Active 데이터셋 교수님이 주신 파일을 통해 작동시켜보기 </li>
<li> 코드의 일부분이라도 이해해보기. </li>
<li> Auto Encoder에 대해서 공부하기 </li>
<li> Variational Auto Encoder </li>


### Auto Encoder
<li> Encoder : 입력 데이터를 차원이 더 적은 Latent Space로 축소하는 역할 </li>
<li> Decoder : Latent Space로 표현된 데이터를 다시 원본의 형태로 되돌리는 역할. 원본과 비슷할 수록 Latent Space가 잘 생성된 것이다. </li>
<li> Auto Encoder : 그 차원을 축소시키고, 원본으로 되돌리는 공식(과정)을 자동으로 학습하도록 만든 것. </li>

### Variational Auto Encoder
<li> 각 데이터 포인트를 더 낮은 차원의 확률 분포들의 조합으로 나타내서 Decoding 과정에서 확률적 샘플링을 이용하는 방식 </li>
<li> Decoding 과정에서 노이즈를 섞은 확률적 샘플링 방식을 이용함으로써, 약간씩 다른 z값을 생성한다. </li>
<li> 동일한 입력에 다른 Decoding 결과를 얻을 수 있음. </li>
<li> VAE는 또한 Generative AI에서 사용되는 이유가 이러한 확률적 샘플링을 통해 새로운 데이터를 얻을 수 있기 때문이다. </li>



### GENE 데이터셋 돌려보기.
<li> shell 명령어로 할 수 없었다.  </li>

![sh_command_result](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/sh_command_result.png)
<li> train_custum0 파일로 들어가서 parser와 여러 변수들 변경후 직접 돌려봄. </li>
<li> gene dataset에 대한 결과는 아레와 같이 나옴. </li>

![gene_data_result](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/gene_data_result.png)



## Trainning Environment
<li> Dataset = Cifar10, CINIC10, Tiny ImageNet </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1 + CUDA ??? </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results