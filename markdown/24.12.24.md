## 24.12.24 까지 공부할 내용
<li> GENE Active 데이터셋 교수님이 주신 파일을 통해 작동시켜보기 </li>
<li> 코드의 일부분이라도 이해해보기. </li>
<li> Auto Encoder에 대해서 공부하기 </li>
<li> Variational Auto Encoder </li>


### Auto Encoder
<li> Encoder : 입력 데이터를 차원이 더 적은 Latent Space로 축소하는 역할 </li>
<li> Decoder : Latent Space로 표현된 데이터를 다시 원본의 형태로 되돌리는 역할. 원본과 비슷할 수록 Latent Space가 잘 생성된 것이다. </li>
<li> Auto Encoder : 그 차원을 축소시키고, 원본으로 되돌리는 공식(과정)을 자동으로 학습하도록 만든 것. </li>

### Variational Auto Encoder
<li> 각 데이터 포인트를 더 낮은 차원의 확률 분포들의 조합으로 나타내서 Decoding 과정에서 확률적 샘플링을 이용하는 방식 </li>
<li> Decoding 과정에서 노이즈를 섞은 확률적 샘플링 방식을 이용함으로써, 약간씩 다른 z값을 생성한다. </li>
<li> 동일한 입력에 다른 Decoding 결과를 얻을 수 있음. </li>
<li> VAE는 또한 Generative AI에서 사용되는 이유가 이러한 확률적 샘플링을 통해 새로운 데이터를 얻을 수 있기 때문이다. </li>

### WAE도 보자.



### GENE 데이터셋 돌려보기. Window=500
<li> shell 명령어로 할 수 없었다.  </li>

![sh_command_result](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/sh_command_result.png)
<li> train_custum0 파일로 들어가서 parser와 여러 변수들 변경후 직접 돌려봄. </li>
<li> sh파일에 들어있는게 parser를 조정하는 명령어였다. </li>
<li> 그거에 맞게 train_custum0파일의 parser를 조절해주며 하나씩 돌려보자. </li>
<li> 아레 이미지는 뭣도 모르고 돌려봤을 때 resnet181 (resnet18을 기반으로, resnet.py파일의 resnet1을 기준으로 만든듯.) 로 GENE 데이터셋(window=500)을 돌려본 결과. </li>

![gene_data_result_181](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/GENE_data_result_181.png)


### GENE 데이터셋 2Teacher 돌려보기. Window=500
<li> 오류난 부분들.. </li>
<ul>
<li> 각종 경로 관련 부분. </li>
<li> GENE_add관련한 Result IMG </li>
<li> student, teacher1, teacher2학습 도중 sp1, sp2, sp3문제 얘들이 뭔진 모르겠지만 그냥 지움. </li>
</ul>

![sp1,2,3](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/sp1,2,3.png)







### 코드 분석
<li> adjust_learning_rate를 사용한다. </li>
<li> 초반에는 높은 학습률, 점점 작아지는 learning_rate를 통해 안정적으로 수렴하게 함. </li>
<li> ~10(10번) 까지는 0.05 </li>
<li> ~65(1/3 지점) 까지는 0.01 </li>
<li> ~132(2/3 지점) 까지는 0.001 </li>

![adjust_learning_rate_181](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/adjust_learning_rate_181.png)

![adjust_learning_rate_epochs](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/adjust_learning_rate_epochs.png)

<li> resnet.py의 ResNet, ResNet2는 Conv2d를 사용하는 것으로 보아 cifar10, cifar100을 위한 Resnet같음. </li>
<li> ResNet1, ResNet3가 Conv1d를 사용하므로 GENE데이터셋에 사용할 것이다. </li>
<li> resnet181은 resnet18기반으로 ResNet1로 만들어졌다는 의미. 이런걸 고려해서 parser를 잘 맞춰주면 코드가 작동된다. </li>
<li> parser의 default값 보존을 위한 스크린샷 </li>

![parser_default](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/parser_default.png)
<li> ResNet 81로 GENE를 돌리려고 했는데 create_cnn_model -> resnet_dict의 81에는 resnet81_cifar로 연결되는데 cifar라고 써있지만 Conv1d를 사용함. </li>
<li> Resnet8_1 student 독립적으로 돌린 결과 200epoch을 채울까 하다가 그만둠 </li>

![vanila_resnet8_1](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/vanila_resnet8_1.png)

<li> Teacher = WRN28-1_1, Student = WRN16-1_1로 GENE 데이터 돌린 결과 </li>
<li> python3 train_custom0.py --epochs 200 --teacher wrn2811 --teacher-checkpoint Teasig/wrn281_02_ep14_val69.5260_best.pth.tar --student wrn1611 --cuda 1 --dataset gene --batch_size 64 --trial gene_wrn281_wrn161_00 --save_weight 1 --seed 1234 </li>

![KD_WRN28-1_WRN16-1](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/KD_WRN28-1_WRN16-1.png)
<li> Teacher = WRN28-1_1, Student = ResNet8_1로 GENE 데이터 학습 결과 </li>
<li> python3 train_custom0.py --epochs 200 --teacher wrn2811 --teacher-checkpoint Teasig/wrn281_02_ep14_val69.5260_best.pth.tar --student resnet81 --cuda 1 --dataset gene --batch_size 64 --trial gene_wrn281_resnet81_00 --save_weight 1 --seed 1234 </li>

![KD_WRN28-1_ResNet8_1](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.12.24/KD_WRN28-1_1_ResNet8_1.png)


## Trainning Environment
<li> Dataset = Cifar10, GENE </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.05 - 0.0001 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results2