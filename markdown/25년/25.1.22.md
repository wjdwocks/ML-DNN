## 25.1.22 까지 공부할 내용
<li> 위에 논문을 이해해보기 위해서 원래 하려고 했던 Transformer 모델에 대해 공부해보려고 함. (RNN, LSTM, Seq2Seq, Attention, Transformer 순서로 공부할 것.) </li>
<li> Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions 이 논문 왜 읽어보라고 했는지 알아보기 </li>
<li> Q_GAF가 무엇이고, GAF와 뭐가 다른지, Q-GAF처럼 GAF의 응용 방법론이 무엇이 더 있을지 알아보기. </li>
<li> self-attention, Transformer 에 대해서 공부. </li>
<li> tranformer 교수님 종이 논문 읽어보기. </li>


## Transformer를 공부하기 위한 선행 공부들
### RNN
<li> Sequence 데이터를 처리하기 위한 신경망 알고리즘. </li>
<li> 1. 입력 데이터를 입력 Sequence(Vector)로 표현하여 넣어줘야 한다. </li>
<li> 2. 아레와 같이 W, V, U 행렬들이 우리가 학습해야 할 파라미터이다. </li>
<li> 3. W, V, U의 초기값은 랜덤으로 설정이 될 것이고, 경사하강법을 통해서 업데이트 됨. </li>
<li> 4. h는 hidden state로, 현재까지의 Sequence 정보를 축적하고 있는 상태이다. RNN의 시간축을 따라 순차적으로 업데이트되고, 이전 hidden state(h_{t-1})와 현재 입력(x_t)를 결합하여 계산한다. </li>
<li> 5. o는 output vector로, Hidden State를 바탕으로 계산된 출력 값으로, 모델의 최종 예측값을 얻기 위한 중간 단계이다. o_t는 hidden state h_t를 기반으로, 현재 time step에서 모델이 생성한 가중치 기반의 해석이다. </li>
<li> 일단 이렇게 알고만 있으라는디 </li>
 
 ![RNN](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/RNN.png)

### LSTM
<li> 기존 RNN은 장기 의존성이라는 문제가 있었다. </li>
<li> 장기 의존성 문제란? : time sequence가 점점 길어진다면, 뒤로 갈 수록 chain rule에 의해 계산되는 식이 0에 수렴한다는 것이다. </li>

![장기 의존성 문제](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/long-term.png)
<li> 장기 의존성 문제의 사례 : '당신의 내면의 힘을 과소평가하지 마세요' 를 번역할 때 'Don't underestimate your inner strength'로 해석이 될텐데, 마세요는 마지막이지만, Don't는 첫 번째 time step이기 때문에, 서로 의미적으로 가까운 단어이지만, Long Term dependency에 의해 학습이 잘 되지 않을 것이다. </li>
<li> LSTM이 기존 RNN과 다른 점 : Gate Algorithm(forget gate, input gate, candidate gate, output gate) </li>
<li> forget gate : 이전 Hidden State와 현재 입력(x)를 받아서 Forgot Gate를 통해 잊을 것을 파악한 뒤 이전 Cell State에서 잊을 정보를 지운다. </li>
<li> Input Gate : 이전 Hidden State와 현재 입력 x를 받아서 Input Gate를 통해 기억할 것을 파악한다. </li>
<li> Candidate Gate : 값을 -1 ~ 1 사이의 값으로 정규화 시키는 역할을 하며, Input Gate와 곱하여서 Cell State에 기억할 정보를 추가하도록 한다. </li>
<li> Output Gate : 이전 Hidden State와 현재 입력 x를 받아서 Output을 출력하고, 업데이트된 Cell State와 결합하여 다음 Hidden State를 생성하는 역할을 한다. </li>
<li> 즉, Gate 알고리즘으로 Cell State(장기기억)을 관리하고, Hidden State(단기 기억)은 그대로 또 유지하는 것이 LSTM이다. </li>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/gate_algorithm.png" alt="게이트 알고리즘" width="500">

<li> Multi Layer LSTM </li>
<li> 첫 번째 Layer에서는 위와 똑같이 동작하지만, 출력을 내지는 않음. </li>
<li> 첫 번째 Layer에서의 모든 Time Step을 지난 후에 다음 Layer로 넘어간다. </li>
<li> 첫 번째 Layer에서는 이전 time step의 Hidden State와 이번 time step의 입력 x를 이용하여 파라미터들을 학습한다. </li>
<li> 이후의 Layer에서는 이번 time step의 hidden state를 계산하기 위해 이전 time step의 Hidden State(처음은 똑같이 0)와 이전 Layer에서의 같은 time step에서의 Hidden State를 사용한다. </li>
<li> 최상위 layer에서만 출력을 낸다. </li>


### Sequence To Sequence 알고리즘
<li> 이 알고리즘은 Sequence to Sequence Learning with Neural Networks 논문에서 처음 제안되었다. </li>
<li> 기계 번역에서의 주요 문제 중 하나인 장기 기억(long-term dependency)문제를 해결하기 위해 LSTM을 기본 단위로 사용한다. </li>
<li> 또한, 기계 번역에서의 두 번째 주요 문제인 문장 내 어순 및 단어 개수의 불일치를 해결하기 위한 접근 방식으로 도입되었다. </li>
<li> 기본 구조 </li>
<ul>
<li> Seq2Seq 모델은 LSTM을 기반으로 설계된 Encoder-Decoder 구조로 입력 시퀀스를 처리한다. </li>
<li> Encoder : 입력 Sequence를 처리하며 최종 Hidden State를 생성한다. </li>
<li> Context Vector : 마지막 LSTM의 Hidden State를 요약된 정보로 사용하는데, 이를 Context Vector라고 한다. </li>
<li> Decoder : Context Vector를 초기 상태로 받아, 번역된 문장을 생성한다. (Encoder와 Decoder는 당연히 서로 다른 가중치 파라미터를 사용한다.) </li>
</ul>
<li> 개선될 부분 : Seq2Seq 모델은 Context Vector(Encoder-Decoder 사이의 Cell/Hidden State)가 고정된 크기라는 점에서 입력 Sequence가 긴 경우 정보 손실이 발생할 수 있다는 문제가 있다. </li>
<li> 내가 이해한 것. (정리 해보자.) </li>
<ul>
<li> Encoder 부분도, Decoder 부분도 모두 LSTM으로 되어있다. </li>
<li> Encoder 부분에서는 출력을 내는 대신에, Hidden State와 Cell State만 반복해서 업데이트를 수행한다. </li>
<li> 그렇게 되면 마지막 Hidden State와 Cell State는 문맥에 대한 모든 정보가 들어가게 됨. 이것을 Context Vector라고 한다.(Context Vector = Cell State + Hidden State) </li>
<li> Decoder 부분에서는 이 Context Vector와 SOS(Start of Sentence)의 초기값을 이용(Decoder의 Hidden State와 Cell State의 초기 가중치로 설정)하여 단어를 출력하게 되고, 이전 Hidden State와 Cell State를 이용하여 이번 LSTM(time step)의 Cell State와 Hidden State를 업데이트하여 단어를 출력하는 것을 반복한다. </li>
<li> 여기에서 입력 sequence와 출력 sequence의 길이가 달라질 수 있는 이유는 EOS가 나올 확률이 가장 높아질 때까지 단어 생성을 반복하기 때문이다. </li>
</ul>

![Seq2Seq](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/Seq2Seq.png)


### Attention Model
<li> Seq2Seq에서는 입력 시퀀스가 길어지더라도 Context Vector의 크기가 고정되어있기에, Sequence들의 정보가 손실된다는 문제가 있다. </li>
<li> 이전의 LSTM 기반의 Decoder에서 출력을 생성할 때 참고하는 것 </li>
<ul>
<li> 1. 이전 Time Step의 Hidden State </li>
<li> 2. 현재 레이어의 Cell State </li>
<li> 3. 같은 Time Step에서 하위 레이어의 Hidden State </li>
<li> 4. 입력 단어(이전 Time Step에서 출력한 단어) </li>
</ul>
<li> Attemtion의 아이디어 : Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder에서의 전체 입력 문장을 다시 한번 참고한다. 이 때, Decoder의 예측 시점(time step)에 연관 있는 단어에 더 집중(Attention) 하여 참고를 하게 된다는 것이다. (매 출력마다 Encoder의 모든 Hidden State를 동적으로 활용한다.) </li>
<li> 어떻게 Attention이 작동하는가? </li>
<ul>
<li> Attention Score를 계산한다 - Decoder의 현재 Hidden State와 Encoder의 각 Hidden State 간의 연관성을 계산하여 Attention Score를 계산한다. (대표적으로 dot product) </li>
<li> 위의 과정을 거쳐서 현재 Time Step의 Decoder에 대한 Hidden State와 모든 입력 Sequence의 Hidden State간의 값 Attention Score(T개)가 생성이 됨. </li>
<li> 위에서 얻어진 Attention Score를 Softmax를 거치게 하여 확률값으로(합이 1) 바꿔준다. </li>
<li> 기존의 Seq2Seq에 있던 Context Vector는 Decoder의 각 Layer의 초기 Hidden/Cell State를 초기화하는데에만 사용하게 됨. </li>
<li> Attention Score를 기반으로, 입력 Sequence의 Hidden State를 가중합하여 Weighted Context Vector를 생성한다.(벡터임) </li>
<li> 얻어진 Weighted Context Vector와 이전 출력의 벡터 임베딩(벡터)를 결합하여 하나의 입력 벡터를 생성한다. (Concatenation) </li>
<li> 위의 입력 벡터와 이전 Time Step의 Hidden State, Cell State를 고려하여 이번 Time Step의 Hidden/Cell State를 계산한다. </li>
<li> 현재 time step의 Hidden State와 Weighted Context Vector를 결합하여 출력 단어의 확률 분포를 계산한다. </li>
<li> Softmax로 출력 단어 분포를 확률을 계산하고, 최종적으로 출력할 단어를 선택하게 된다. </li>
</ul>

![Attention](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/Attention.png)
<li> 내가 이해한 Attention Mechanism </li>
<ul>
<li> 기존의 Seq2Seq의 Context Vector는 Decoder에서 각 첫 번째 Time Step에서 초기 Hidden/Cell State를 정하는 데에만 사용한다. </li>
<li> 각 Time Step(t번쨰)의 입력으로는 이전 Weighted Context Vector(t-1 번쨰)와 이전 Time Step의 출력의 Embedding Vector를 Concatenate한 것을 사용한다. </li>
<li> Weighted Context Vector는 현재 time step의 Hidden State와 Encoder의 모든 Hidden State를 비교(dot product 등의 방법으로)하여 Attention Score를 얻고, 이를 Softmax로 확률값으로 바꿔준 다음, Encoder의 모든 Hidden State를 이 확률값들과 가중합하여 벡터로 얻을 수 있다. </li>
<li> 이제 입력값과 Hidden State(t-1), Cell State(t-1)을 사용하여 이번 time step의 Hidden State와 Cell State를 새로 얻는다. </li>
<li> 이렇게 얻어진 이번 time step의 Hidden State를 이용하여 다시 Weighted Context Vector(t번째)를 다시 계산한다. </li>
<li> 이렇게 얻어진 이번 time step의 Weighted Context Vector(t번째)와 Hidden State(t번째)를 결합하여 출력 단어의 출력 분포를 계산한다.  </li>
<li> 계산된 출력 분포를 확률로 바꿔서 최종 출력할 단어를 선택한다. </li>
</ul>


### Self-Attention

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/Attention.png" alt="Attention" width="500">

### Cross-Attention



### Transformer
<li> Attention is all you need 논문 리뷰. </li>
<li> Transformer는 RNN이나 CNN을 전혀 사용하지 않는다. - 문장안에 포함된 각각의 단어에 대한 순서 정보를 알려주기 위해서 Positional Encoding을 사용한다.  </li>
<li> Transformer 또한 Encoder와 Decoder로 구성된다. Attention 과정을 여러 Layer에서 반복함. </li>


### Transformer의 동작 원리.
<li> 0. 트랜스포머 이전의 전통적인 단어 임베딩 </li>
<ul>
<li> 그냥 각 단어를 Input Embedding Matrix를 통과시켜서 원하는 크기의 차원을 가진 벡터로 바꾸는 방식이었다. </li>
<li> 네트워크에 입력을 넣기 전에 이러한 방식을 통해 각 단어(입력)을 벡터 임베딩 하는 과정을 거침. </li>
<li> 하지만, RNN을 사용하지 않으려면, 위치 정보(단어의 순서)를 포함하고 있는 Embedding을 사용해야 함. </li>
<li> Transformer에서는 RNN이나 LSTM을 사용하지 않는다고 했으니, 트랜스포머에서는 Positional Encoding을 사용한다. </li>
</ul>

<li> 1. Positional Encoding을 사용하는 Transformer의 입력 값 임베딩 </li>
<ul>
<li> Input Embedding Matrix와 같은 크기(차원)을 가지는 위치 정보를 가진 벡터(?)를 단순히 합쳐서(element wise sum) 네트워크에게 전달해주게 된다. </li>
</ul>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/positional_encoding.png" alt="PE" width="500">

<li> 2. Encoding에서의 원리 - Self Attention </li>
<ul>
<li> 각각의 입력 단어들끼리 쌍을 지어서, 각각의 단어가 서로 어느정도의 연관성을 가지는지 Attention Score를 매기게 된다. </li>
<li> 이 Self-Attention을 통해서 입력 문장 내의 문맥에 대한 정보를 Encoding 부분에서 잘 학습하게 된다. </li>
<li> 또한, 성능 향상을 위해 Residual Learning(Resnet에서 사용하는 잔차 학습)을 사용한다. </li>
<li> Multihead Attention : Self-Attention을 여러 번 수행하는 것? </li>
</ul>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/Transformer_Encoder.png" alt="Transformer_Encoder.png" width="500">

<li> 3.  </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/트랜스포머_구조.png" alt="트랜스포머_구조.png" width="500">

### 현재까지 GAF 연구 결과.
<li> 전체 공통 참고 사항 </li>
<ul>
<li> GAF이미지는 64 x 64로 설정하였다. (500 x 500으로 한번 해보고싶은데 ) </li>
<li> gene_dataset은 3channel Accelerometer로 되어있기에 각 축을 따로 나누어서 GASF로 변환한 후, 다시 Channel로 합쳤다. </li>
<li> batch_size = 64 </li>
<li> epoch = 200 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum(0.9) + SGD + λ(0.0001, L2정규화항 가중치) </li>
<li> loss : T = 4, lambda = 0.7 (KD_loss 항에 더 큰 가중치) </li>
<li> 각 학습 결과와 parameter는 TeaGAF폴더에 같은 format으로 저장해두었다. </li>
<br>
</ul>

<li> WRN16-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 63.0491 </li>
<li> 63.3270 </li>
<li> 62.9797 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> WRN16-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 64.0389 </li>
<li> 63.9521 </li>
<li> 63.9173 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> WRN28-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 63.2922 </li>
<li> 64.0042 </li>
<li> 63.3096 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> WRN28-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 64.1604 </li>
<li> 64.8029 </li>
<li> 65.3933 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN16-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 68.9703 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN16-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 68.2063 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN28-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 67.0082 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN28-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 67.5464 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>
<ol>

### 하려고 생각하는 남은 것들.
<li> 일단 GAF 단일 네트워크 3번씩 학습해서 평균내기 </li>
<li> GAF Teacher 하나만 사용해서 각 조합마다 3번씩 학습해서 평균내기. (Teacher는 WRN-281,283,161,163, Student는 WRN-161만.) </li>
<li> Tranformer 모델 공부 후 교수님이 종이로 주신 Transformer 논문 이해해보기. </li>
<li> 코드 추가하여, 위의 학습이 끝나자 마자 T : (Sig + GAF), S : Sig 의 2 Teacher 학습 바로 진행할 수 있도록 하기. </li>
<li> SPKD 논문 읽어보기 </li>
<li> 시간이 된다면 Diffusion Model에 대해서 공부를 한 뒤 두 번째 논문 이해해보기. </li>


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>