## 25.1.22 까지 공부할 내용
<li> 위에 논문을 이해해보기 위해서 원래 하려고 했던 Transformer 모델에 대해 공부해보려고 함. (RNN, LSTM, Seq2Seq, Attention, Transformer 순서로 공부할 것.) </li>
<li> Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions 이 논문 왜 읽어보라고 했는지 알아보기 </li>
<li> Q_GAF가 무엇이고, GAF와 뭐가 다른지, Q-GAF처럼 GAF의 응용 방법론이 무엇이 더 있을지 알아보기. </li>
<li> self-attention, Transformer 에 대해서 공부. </li>
<li> tranformer 교수님 종이 논문 읽어보기. </li>
<li> Imaging Time-Series to Improve Classification and Imputation - Discord로 교수님이 공유해주심 (GAF나오는걸 보니 나한테 말씀하신듯.) </li>



## Imaging Time-Series to Improve Classification and Imputation
<li> Time-Series 데이터를 시각적으로 표현하여(Imaging) Classfication과 Imputation(결측값 대체) 성능을 올리는 방법에 대해 연구하는 논문인것 같다. </li>

### Introduction
<li> 이전에도 시계열 데이터의 학습 성능을 올리기 위한 다양한 노력들이 있었지만, 그 방법들을 통해 시계열 데이터를 변환했을 때 원래 시계열 데이터와의 관계를 해석하는 데 한계가 있었다. </li>
<li> 우리는 GASF/GADF, MTF라는 방법을 사용하여 Time Series 데이터를 시각화하고, Classification/Imputation Tast에서 더 나은 성능을 보여줄 수 있다. </li>
<li> 이 GAF, MTF 방법론이 기존의 전통적인 방법들보다 더 나은 성능을 가진다는 것을 20개 중 9개의 데이터셋에 대해서 증명함. </li>

### Background
<li> GASF/GADF </li>
<ul>
<li> 1. 각 데이터에 대해서 Min-Max Scaling을 통해서 [-1, 1]이나 [0, 1]의 범위를 갖게 한다. x' = minmax(x) </li>
<li> 2. 위의 값 분포를 갖는 데이터들을 극좌표계(polar coordinates)에 표현한다. φ = arccos(x') </li>
<li> 3. 이 바뀐 값들이 각 time step을 대표하는 값이 된다. </li>
<li> 4. 각 time step들끼리의 상관관계를 cos(φi + φj) - GASF로 표현하여 모든 time step 간의 상관관계를 표현하여 2차원의 이미지로 나타낸다. </li>
<li> 5. GADF는 sin(φi - φj)로 GAF이미지를 구성하게 된다. </li>
</ul>
<li> GAF의 장점1 - 시간적 의존성을 보존해준다. </li>
<ul>
<li> GAF는 시간적 순서를 가진 데이터를 2D 이미지로 변환하면서, 데이터의 시간적 의존성을 유지해준다. </li>
<li> 행렬의 왼쪽 위는 초기 시간 단계의 정보를 나타내고, 오른쪽 아래는 후반부 시간 단계의 정보를 포함한다. </li>
</ul>
<li> GAF의 장점2 - 시간 단계 간 상관관계를 포함한다. </li>
<ul>
<li> (i, j)위치의 값은 i번째 time step과 j번째 time step의 상관관계를 나타낸다. </li>
</ul>
<li> GAF의 장점3 - 주 대각선을 이용한 높은 수준의 재구성 </li>
<ul>
<li> GAF 행렬의 주대각은 시간 간격이 0인 경우를 나타냄. </li>
<li> 이 값에는 time series data의 원래 값(각도)을 반영하며 시계열 데이터의 특징을 재구성할 때 높은 수준을 기대할 수 있게 해준다. </li>
</ul>
<li> GAF의 단점 - 크기 문제와 효율성 </li>
<ul>
<li> GAF행렬은 n x n의 크기를 가지며, 데이터가 길어질수록(time step의 길이가) 행렬의 크기도 기하급수적으로 증가함. </li>
<li> 행렬의 크기 증가는 메모리와 계산 시간 측면에서도 비효율적이다. </li>
</ul>

### MTF(Markov Trasition Field)
<li> MTF는 각 time step마다 다른 time step으로의 전이 확률(Transition Probability)를 계산한 행렬이라고 생각하면 된다. </li>
<li> MTF의 과정 </li>
<ul>
<li> 1. 주어진 데이터 X = {x1, x2 ... xn}을 Q개의 구간으로 나눈다. </li>
<li> 2. 각 데이터 포인트 xi는 자신이 속한 구간 (qj)에 매핑된다. </li>
<li> 3. {q1, q2 ... qQ}는 데이터의 값이 Q개의 구간 중 어디에 속하는지를 나타냄. </li>
<li> 4. 각 데이터 포인트 xi가 q1에 있을 때 x(i+1)이 q2에 있는 경우를 센다. </li>
<li> 5. 즉, qi -> qj로의 전이가 몇 번 발생했는지를 세서 빈도를 계산함. </li>
<li> 6. 그렇게 계산된 빈도 Matrix를 확률로 나타내서(합이 1이 되도록 간단하게 나눔.) MTF를 구성하게 된다. </li>
<li> 7. 이렇게 나온 MTF 이미지는 Q x Q 크기이므로, 구간별로 전이 횟수를 계산한다는 것을 알 수 있다. </li>
<li> 8. 또한 Q x Q에서 Q는 데이터의 값 or label이나 그룹 이라고 볼 수 있다. time과는 관계가 없어야 한다. </li>
</ul>

### 결과 요약
<li> 20개의 데이터셋에 대해서 여러 가지 방법론으로 결과를 비교함. </li>
<li> 비교는 Error Rate로 진행함. (0.301은 30.1% 로 틀림.) </li>
<li> 그 중 MTF/GASF/GADF가 9번 1등을 했다고 함.  </li>
<li> PPA는 Piecewise Aggregate Approximation은 시계열 데이터를 요약하여 GAF 이미지 사이즈를 줄이는 역할을 한다고 함. (resize보다 더 생생한 정보라고 함.) </li>
<li> GAF는 PPA로 요약된 데이터를 기반으로 생성됨. 나도 몰랐는데 코드에서 알아서 해준거라고 함. </li>
<li> gaf = GramianAngularField(image_size=32, method='summation') # 이렇게 하면 PPA가 적용된다. </li>

### Results and Discussion
<li> 결과 1 - MTF는 일반적으로 GAF(GASF, GADF)에 비해 높은 에러율을 보였다. </li>
<ul>
<li> 이유 1 : MTF는 시계열 데이터를 MTF로 변환한 후 원래 데이터로 복원하기가 어렵다. </li>
<li> 특정 시간 단계에서 다른 시간 단계로의 전이 확률만 표현하고, 각 시점의 데이터 크기 자체에 대한 정보를 잃어버린다. </li>
<li> GAF는 주대각에 원래 정보를 어느정도 기억한다. </li>
</ul>
<li> 결과 2 - GAF는 상대적으로 강하다. </li>
<ul>
<li> GASF와 GADF는 데이터의 값 간의 각도를 이용해서 데이터의 정적 특징을 보존한다. </li>
<li> GAF는 데이터의 각도를 기반으로 하기에 복원 가능성이 더 높다. </li>
</ul>
<li> 결과 3 - MTF와 GAF가 서로 보완적인 역할을 한다. </li>
<ul>
<li> GAF는 정적 정보를 잘 표현하고, MTF는 동적 정보를 더 잘 표현한다. </li>
<li> MTF는 데이터의 시간적 전이를 나타내는 데 유용하지만, 단독으로는 인식/분류 작업에 충분하지 않다. </li>
<li> RGB채널처럼 각각 GASF, GADF, MTF를 각각의 채널처럼 간주하여 학습을 진행했다. </li>
</ul>

### Qunantumized GAF란 
<li> Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field and CNNs for Stock Return Predictions 논문에서 말함. </li>
<li> 양자 컴퓨팅 기술과 딥러닝을 결합하여 시계열 데이터 분류 및 예측의 정확도를 높이는 방법. </li>
<li> 전통적인 GAF 변환과 다르게 QGAF는 데이터 정규화나 역cos 계산이 필요 없고, 시계열 데이터를 2차원 이미지로 변환하는 과정을 단순화 시킨다. </li>
<li> 시계열 데이터 분류 작업은 고유한 어려움을 제시합니다.
시계열 데이터의 **순차적 특성(sequential nature)**은 일반적으로 **비순차적(static)**이고 정적 데이터를 위해 설계된 표준 머신러닝 알고리즘이 처리하기 어렵게 만듭니다. - 시간 순서대로 나와있지만 그 간격이 얼마나 될지는 랜덤이라서 예측이 어렵다는 것 같음. </li>



## 서버 구축해본 경험 정리
### Linux를 다운로드 받는다.
<li> USB에 linux를 다운받으면 그 usb에는 그 linux만 남고, 다 지워짐. </li>
<li> 서버에 꽂고 reboot하면 알아서 linux를 다운로드 받는다. </li>
<li> 간단한 설정들 모두 다 넘겨주고, 컴퓨터가 켜진다. </li>

### NVIDIA Driver 다운로드.
<li> ubuntu-drivers devices 명령어를 통해 NVIDIA 추천 Driver를 본다. recommended가 쓰여있음. </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/ubuntu-drivers devices.png" alt="ubuntu-drivers devices" width="500">
<li> 거기서 추천받은 NVIDIA Driver를 다운로드받는다. sudo apt install NVIDIA... 이런식으로 하면 됨. </li>
<li> NVIDIA-Driver와 호환이 되는 CUDA를 다운로드 받아야함. 나는 NVIDIA-Driver 560 이랑 CUDA 12.6으로 다운로드 받음. </li>
<li> (여기서 한번 Driver 다시 다운받으려고 지웠다가 화면 해상도 이상해지고 와이파이도 갑자기 안됨. - wifi문제는 아직도 모르겠다.) </li>
<li> 이렇게 CUDA와 Driver를 모두 다운받은 뒤 Cudnn을 다운받는다. </li>
<li> 인터넷에 cudnn 다운로드 치니까 내꺼 cuda 버전과, 운영체제, 환경 등을 물어보고 최종적으로 나온 bash script만 쭉 입력하니 cudnn이 다운로도 되었다. cuda와 호환 가능한 버전으로. </li>

### anaconda 가상환경 모두 나누기.
<li> 일단 서버에서 anaconda를 다운로드 받았다. </li>
<li> anaconda를 다운받아놓으면 그 명령어를 가지고 가상환경(각 user들마다 각기다른)을 만들어서 각자 관리할 수 있기 때문. </li>
<li> 서버에서 anaconda를 받았는데, 위치가 /home/root/anaconda3 이래가지고, 계정을 만들어도 conda 명령어를 사용할 수가 없었음. </li>
<li> 그래서 우리 연구실 얘들에게 group을 만들어서 거기에 넣은다음, 그 group에게 /home/root/anaconda3의 실행가능한 권한을 주었다. </li>
<li> 그런 후 user 로 로그인 conda conda create --name my_env python=3.9 이런식으로 환경을 각자 만들었다. </li>
<li> 그 다음. conda activate my_env를 하면 이제부터 내 환경으로 들어가서 사용이 가능하다. </li>

### 느낀점
<li> 위에 과정을 간추려보니 매우 아무것도 아닌 것 같지만, 아무것도 모른 채로 하려고 하니 죽을맛이었다. </li>
<li> 계속 뭐 하나 하려고 하면 한쪽에서 문제가 생기고 하는게 정말 읍읍마려웠다. </li>
<li> 가만히 냅뒀는데 혼자 갑자기 꺼졌다. 내쪽에서는 server suspend? 이렇게 뜸. - 절전모드 설정되어있나 해서 꺼봄. </li>



## Transformer를 공부하기 위한 선행 공부들
### RNN
<li> Sequence 데이터를 처리하기 위한 신경망 알고리즘. </li>
<li> 1. 입력 데이터를 입력 Sequence(Vector)로 표현하여 넣어줘야 한다. </li>
<li> 2. 아레와 같이 W, V, U 행렬들이 우리가 학습해야 할 파라미터이다. </li>
<li> 3. W, V, U의 초기값은 랜덤으로 설정이 될 것이고, 경사하강법을 통해서 업데이트 됨. </li>
<li> 4. h는 hidden state로, 현재까지의 Sequence 정보를 축적하고 있는 상태이다. RNN의 시간축을 따라 순차적으로 업데이트되고, 이전 hidden state(h_{t-1})와 현재 입력(x_t)를 결합하여 계산한다. </li>
<li> 5. o는 output vector로, Hidden State를 바탕으로 계산된 출력 값으로, 모델의 최종 예측값을 얻기 위한 중간 단계이다. o_t는 hidden state h_t를 기반으로, 현재 time step에서 모델이 생성한 가중치 기반의 해석이다. </li>
<li> 일단 이렇게 알고만 있으라는디 </li>
 
 ![RNN](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/RNN.png)

### LSTM
<li> 기존 RNN은 장기 의존성이라는 문제가 있었다. </li>
<li> 장기 의존성 문제란? : time sequence가 점점 길어진다면, 뒤로 갈 수록 chain rule에 의해 계산되는 식이 0에 수렴한다는 것이다. </li>

![장기 의존성 문제](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/long-term.png)
<li> 장기 의존성 문제의 사례 : '당신의 내면의 힘을 과소평가하지 마세요' 를 번역할 때 'Don't underestimate your inner strength'로 해석이 될텐데, 마세요는 마지막이지만, Don't는 첫 번째 time step이기 때문에, 서로 의미적으로 가까운 단어이지만, Long Term dependency에 의해 학습이 잘 되지 않을 것이다. </li>
<li> LSTM이 기존 RNN과 다른 점 : Gate Algorithm(forget gate, input gate, candidate gate, output gate) </li>
<li> forget gate : 이전 Hidden State와 현재 입력(x)를 받아서 Forgot Gate를 통해 잊을 것을 파악한 뒤 이전 Cell State에서 잊을 정보를 지운다. </li>
<li> Input Gate : 이전 Hidden State와 현재 입력 x를 받아서 Input Gate를 통해 기억할 것을 파악한다. </li>
<li> Candidate Gate : 값을 -1 ~ 1 사이의 값으로 정규화 시키는 역할을 하며, Input Gate와 곱하여서 Cell State에 기억할 정보를 추가하도록 한다. </li>
<li> Output Gate : 이전 Hidden State와 현재 입력 x를 받아서 Output을 출력하고, 업데이트된 Cell State와 결합하여 다음 Hidden State를 생성하는 역할을 한다. </li>
<li> 즉, Gate 알고리즘으로 Cell State(장기기억)을 관리하고, Hidden State(단기 기억)은 그대로 또 유지하는 것이 LSTM이다. </li>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/gate_algorithm.png" alt="게이트 알고리즘" width="500">

<li> Multi Layer LSTM </li>
<li> 첫 번째 Layer에서는 위와 똑같이 동작하지만, 출력을 내지는 않음. </li>
<li> 첫 번째 Layer에서의 모든 Time Step을 지난 후에 다음 Layer로 넘어간다. </li>
<li> 첫 번째 Layer에서는 이전 time step의 Hidden State와 이번 time step의 입력 x를 이용하여 파라미터들을 학습한다. </li>
<li> 이후의 Layer에서는 이번 time step의 hidden state를 계산하기 위해 이전 time step의 Hidden State(처음은 똑같이 0)와 이전 Layer에서의 같은 time step에서의 Hidden State를 사용한다. </li>
<li> 최상위 layer에서만 출력을 낸다. </li>


### Sequence To Sequence 알고리즘
<li> 이 알고리즘은 Sequence to Sequence Learning with Neural Networks 논문에서 처음 제안되었다. </li>
<li> 기계 번역에서의 주요 문제 중 하나인 장기 기억(long-term dependency)문제를 해결하기 위해 LSTM을 기본 단위로 사용한다. </li>
<li> 또한, 기계 번역에서의 두 번째 주요 문제인 문장 내 어순 및 단어 개수의 불일치를 해결하기 위한 접근 방식으로 도입되었다. </li>
<li> 기본 구조 </li>
<ul>
<li> Seq2Seq 모델은 LSTM을 기반으로 설계된 Encoder-Decoder 구조로 입력 시퀀스를 처리한다. </li>
<li> Encoder : 입력 Sequence를 처리하며 최종 Hidden State를 생성한다. </li>
<li> Context Vector : 마지막 LSTM의 Hidden State를 요약된 정보로 사용하는데, 이를 Context Vector라고 한다. </li>
<li> Decoder : Context Vector를 초기 상태로 받아, 번역된 문장을 생성한다. (Encoder와 Decoder는 당연히 서로 다른 가중치 파라미터를 사용한다.) </li>
</ul>
<li> 개선될 부분 : Seq2Seq 모델은 Context Vector(Encoder-Decoder 사이의 Cell/Hidden State)가 고정된 크기라는 점에서 입력 Sequence가 긴 경우 정보 손실이 발생할 수 있다는 문제가 있다. </li>
<li> 내가 이해한 것. (정리 해보자.) </li>
<ul>
<li> Encoder 부분도, Decoder 부분도 모두 LSTM으로 되어있다. </li>
<li> Encoder 부분에서는 출력을 내는 대신에, Hidden State와 Cell State만 반복해서 업데이트를 수행한다. </li>
<li> 그렇게 되면 마지막 Hidden State와 Cell State는 문맥에 대한 모든 정보가 들어가게 됨. 이것을 Context Vector라고 한다.(Context Vector = Cell State + Hidden State) </li>
<li> Decoder 부분에서는 이 Context Vector와 SOS(Start of Sentence)의 초기값을 이용(Decoder의 Hidden State와 Cell State의 초기 가중치로 설정)하여 단어를 출력하게 되고, 이전 Hidden State와 Cell State를 이용하여 이번 LSTM(time step)의 Cell State와 Hidden State를 업데이트하여 단어를 출력하는 것을 반복한다. </li>
<li> 여기에서 입력 sequence와 출력 sequence의 길이가 달라질 수 있는 이유는 EOS가 나올 확률이 가장 높아질 때까지 단어 생성을 반복하기 때문이다. </li>
</ul>

![Seq2Seq](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/Seq2Seq.png)


### Attention Model
<li> Seq2Seq에서는 입력 시퀀스가 길어지더라도 Context Vector의 크기가 고정되어있기에, Sequence들의 정보가 손실된다는 문제가 있다. </li>
<li> 이전의 LSTM 기반의 Decoder에서 출력을 생성할 때 참고하는 것 </li>
<ul>
<li> 1. 이전 Time Step의 Hidden State </li>
<li> 2. 현재 레이어의 Cell State </li>
<li> 3. 같은 Time Step에서 하위 레이어의 Hidden State </li>
<li> 4. 입력 단어(이전 Time Step에서 출력한 단어) </li>
</ul>
<li> Attemtion의 아이디어 : Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder에서의 전체 입력 문장을 다시 한번 참고한다. 이 때, Decoder의 예측 시점(time step)에 연관 있는 단어에 더 집중(Attention) 하여 참고를 하게 된다는 것이다. (매 출력마다 Encoder의 모든 Hidden State를 동적으로 활용한다.) </li>
<li> 어떻게 Attention이 작동하는가? </li>
<ul>
<li> Attention Score를 계산한다 - Decoder의 현재 Hidden State와 Encoder의 각 Hidden State 간의 연관성을 계산하여 Attention Score를 계산한다. (대표적으로 dot product) </li>
<li> 위의 과정을 거쳐서 현재 Time Step의 Decoder에 대한 Hidden State와 모든 입력 Sequence의 Hidden State간의 값 Attention Score(T개)가 생성이 됨. </li>
<li> 위에서 얻어진 Attention Score를 Softmax를 거치게 하여 확률값으로(합이 1) 바꿔준다. </li>
<li> 기존의 Seq2Seq에 있던 Context Vector는 Decoder의 각 Layer의 초기 Hidden/Cell State를 초기화하는데에만 사용하게 됨. </li>
<li> Attention Score를 기반으로, 입력 Sequence의 Hidden State를 가중합하여 Weighted Context Vector를 생성한다.(벡터임) </li>
<li> 얻어진 Weighted Context Vector와 이전 출력의 벡터 임베딩(벡터)를 결합하여 하나의 입력 벡터를 생성한다. (Concatenation) </li>
<li> 위의 입력 벡터와 이전 Time Step의 Hidden State, Cell State를 고려하여 이번 Time Step의 Hidden/Cell State를 계산한다. </li>
<li> 현재 time step의 Hidden State와 Weighted Context Vector를 결합하여 출력 단어의 확률 분포를 계산한다. </li>
<li> Softmax로 출력 단어 분포를 확률을 계산하고, 최종적으로 출력할 단어를 선택하게 된다. </li>
</ul>

![Attention](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/Attention.png)
<li> 내가 이해한 Attention Mechanism </li>
<ul>
<li> 기존의 Seq2Seq의 Context Vector는 Decoder에서 각 첫 번째 Time Step에서 초기 Hidden/Cell State를 정하는 데에만 사용한다. </li>
<li> 각 Time Step(t번쨰)의 입력으로는 이전 Weighted Context Vector(t-1 번쨰)와 이전 Time Step의 출력의 Embedding Vector를 Concatenate한 것을 사용한다. </li>
<li> Weighted Context Vector는 현재 time step의 Hidden State와 Encoder의 모든 Hidden State를 비교(dot product 등의 방법으로)하여 Attention Score를 얻고, 이를 Softmax로 확률값으로 바꿔준 다음, Encoder의 모든 Hidden State를 이 확률값들과 가중합하여 벡터로 얻을 수 있다. </li>
<li> 이제 입력값과 Hidden State(t-1), Cell State(t-1)을 사용하여 이번 time step의 Hidden State와 Cell State를 새로 얻는다. </li>
<li> 이렇게 얻어진 이번 time step의 Hidden State를 이용하여 다시 Weighted Context Vector(t번째)를 다시 계산한다. </li>
<li> 이렇게 얻어진 이번 time step의 Weighted Context Vector(t번째)와 Hidden State(t번째)를 결합하여 출력 단어의 출력 분포를 계산한다.  </li>
<li> 계산된 출력 분포를 확률로 바꿔서 최종 출력할 단어를 선택한다. </li>
</ul>


### Self-Attention
<li> 공부 해야함. </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/Attention.png" alt="Attention" width="500">


### Cross-Attention
<li> 공부 해야함. </li>


### Transformer
<li> Attention is all you need 논문 리뷰. </li>
<li> Transformer는 RNN이나 CNN을 전혀 사용하지 않는다. - 문장안에 포함된 각각의 단어에 대한 순서 정보를 알려주기 위해서 Positional Encoding을 사용한다.  </li>
<li> Transformer 또한 Encoder와 Decoder로 구성된다. Attention 과정을 여러 Layer에서 반복함. </li>


### Transformer의 동작 원리.
<li> 0. 트랜스포머 이전의 전통적인 단어 임베딩 </li>
<ul>
<li> 그냥 각 단어를 Input Embedding Matrix를 통과시켜서 원하는 크기의 차원을 가진 벡터로 바꾸는 방식이었다. </li>
<li> 네트워크에 입력을 넣기 전에 이러한 방식을 통해 각 단어(입력)을 벡터 임베딩 하는 과정을 거침. </li>
<li> 하지만, RNN을 사용하지 않으려면, 위치 정보(단어의 순서)를 포함하고 있는 Embedding을 사용해야 함. </li>
<li> Transformer에서는 RNN이나 LSTM을 사용하지 않는다고 했으니, 트랜스포머에서는 Positional Encoding을 사용한다. </li>
</ul>

<li> 1. Positional Encoding을 사용하는 Transformer의 입력 값 임베딩 </li>
<ul>
<li> Input Embedding Matrix와 같은 크기(차원)을 가지는 위치 정보를 가진 벡터(?)를 단순히 합쳐서(element wise sum) 네트워크에게 전달해주게 된다. </li>
</ul>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/positional_encoding.png" alt="PE" width="500">

<li> 2. Encoding에서의 원리 - Self Attention </li>
<ul>
<li> 각각의 입력 단어들끼리 쌍을 지어서, 각각의 단어가 서로 어느정도의 연관성을 가지는지 Attention Score를 매기게 된다. </li>
<li> 이 Self-Attention을 통해서 입력 문장 내의 문맥에 대한 정보를 Encoding 부분에서 잘 학습하게 된다. </li>
<li> 또한, 성능 향상을 위해 Residual Learning(Resnet에서 사용하는 잔차 학습)을 사용한다. </li>
<li> Multihead Attention : Self-Attention을 여러 번 수행하는 것? </li>
</ul>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/Transformer_Encoder.png" alt="Transformer_Encoder.png" width="500">

<li> 3.  </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.22/트랜스포머_구조.png" alt="트랜스포머_구조.png" width="500">

### 현재까지 GAF 연구 결과.
<li> 전체 공통 참고 사항 </li>
<ul>
<li> GAF이미지는 64 x 64로 설정하였다. (500 x 500으로 한번 해보고싶은데 ) </li>
<li> gene_dataset은 3channel Accelerometer로 되어있기에 각 축을 따로 나누어서 GASF로 변환한 후, 다시 Channel로 합쳤다. </li>
<li> batch_size = 64 </li>
<li> epoch = 200 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum(0.9) + SGD + λ(0.0001, L2정규화항 가중치) </li>
<li> loss : T = 4, lambda = 0.7 (KD_loss 항에 더 큰 가중치) </li>
<li> 각 학습 결과와 parameter는 TeaGAF폴더에 같은 format으로 저장해두었다. </li>
<br>
</ul>

<li> WRN16-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 63.0491 </li>
<li> 63.3270 </li>
<li> 62.9797 </li>
<li> 평균 :  63.1186</li>
<li> 표준편차 : 0.1501 </li>
<br>
</ul>

<li> WRN16-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 64.0389 </li>
<li> 63.9521 </li>
<li> 63.9173 </li>
<li> 평균 : 63.9694 </li>
<li> 표준편차 : 0.0511 </li>
<br>
</ul>

<li> WRN28-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 63.2922 </li>
<li> 64.0042 </li>
<li> 63.3096 </li>
<li> 평균 : 63.5353 </li>
<li> 표준편차 : 0.3316 </li>
<br>
</ul>

<li> WRN28-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 64.1604 </li>
<li> 64.8029 </li>
<li> 65.3933 </li>
<li> 평균 : 64.7855 </li>
<li> 표준편차 : 0.5035 </li>
<br>
</ul>

<li> T : WRN16-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 68.9703 </li>
<li> 67.6680 </li>
<li> 67.0429 </li>
<li> 평균 : 67.8937 </li>
<li> 표준편차 : 0.8029 </li>
<br>
</ul>

<li> T : WRN16-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 68.2063 </li>
<li> 67.7201 </li>
<li> 68.3278 </li>
<li> 평균 : 68.0847 </li>
<li> 표준편차 : 0.2626 </li>
<br>
</ul>

<li> T : WRN28-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 67.0082 </li>
<li> 66.4004 </li>
<li> 67.0603 </li>
<li> 평균 : 66.8230 </li>
<li> 표준편차 : 0.2996 </li>
<br>
</ul>

<li> T : WRN28-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 67.5464 </li>
<li> 66.4872 </li>
<li>  </li>
<li> 평균 :  </li>
<li> 표준편차 : </li>
<br>
</ul>
<ol>

### 하려고 생각하는 남은 것들. 25.1.22 이후로 이어가기.
<li> 일단 GAF 단일 네트워크 3번씩 학습해서 평균내기 (완) </li>
<li> GAF Teacher 하나만 사용해서 각 조합마다 3번씩 학습해서 평균내기. (Teacher는 WRN-281,283,161,163, Student는 WRN-161만.) (거의 끝남.) </li>
<li> Tranformer 모델 공부 후 교수님이 종이로 주신 Transformer 논문 이해해보기. (Transformer 아직 이해 부족함.) </li>
<li> 코드 추가하여, 위의 학습이 끝나자 마자 T : (Sig + GAF), S : Sig 의 2 Teacher 학습 바로 진행할 수 있도록 하기. (거의 완.) </li>
<li> 시간이 된다면 Diffusion Model에 대해서 공부를 한 뒤 두 번째 논문 이해해보기. (Transformer 다 하고 하려고 했는데 그냥 중간중간에 해야할듯.) </li>
<li> 서버에 코드 다 올린 뒤, 경로 수정한 후 쉘로 한번에 학습 돌릴 수 있도록 하여, 설 연휴때 학습 다 돌려버리기. </li>


### 해야 할 것
<li> 2 Teacher (GAF + Sig) 에 대해서 14cls에서 코드를 만들어야 함. (서버에서 돌아가도록 공부해서 만들기.) </li>
<li> alpha값 (KD1과 KD2 간의 비중을 0.1, 0.3, 0.5, 0.7, 0.9에 대해서 비교해보기 (3번씩 계산 후 평균 + 표준편차)) </li>
<li> 이제는 7cls에 대해서도 GAF를 만들고, GAF Teacher를 만들고, 지금까지 했던 것들을 모두 똑같이 해야 함. </li>
<li> 7cls-500w, 7cls-1000w 에 대해서 각각 GAF만들고, GAF Teacher 학습하고(vanila), 1 Teacher(GAF) KD도 결과를 내보는 것이 목표. </li>
<li> TPKD 논문 9 ~ 11p 부분을 보고, 14cls와 7cls의 결과를 확인하며 비교해보자. </li>



## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>