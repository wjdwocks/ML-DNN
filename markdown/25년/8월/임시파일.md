오늘 4.4까지는 무조건 다 쓰는걸 목표로... # 만약 실험때문에 안된다고 한다면, 그부분제외 후 다 쓰고, 실험 다 돌려놓기.

4.4 Analysis on Sensitivity and Compatibility
We conducted additional experiments to investigate both the sensitivity of our framework and its compatibility with other knowledge distillation (KD) techniques. In this section, we analyze the effect of the hyperparameter $\alpha$, which controls the weighting between the two teacher losses in the multi-teacher setting. We also employ parametric plots to further investigate the relationships between different KD strategies. In addition, we use t-SNE visualizations to explore how different KD methods shape the feature space learned by the student. Finally, we evaluate how the proposed methods perform when combined with prior KD approaches such as AT and SP.

4.4.1 Sensitivity Analysis
We evaluated the sensitivity of the multi-teacher setting to the hyperparameter $\alpha$, which controls the relative contribution of each teacher in the distillation loss. The results indicate that for the GAF+Signal combination, both the Base and Ann settings achieved their highest performance at $\alpha=0.3$, after which the accuracy gradually declined as $\alpha$ increased. In contrast, for the PI+Signal combination, only the Ann setting reached its peak performance at $\alpha=0.3$, while the Base setting did not show its maximum at this point. Based on these observations, we adopted $\alpha=0.3$ for our experiments.
\begin{figure}
\end{figure}

To further analyze the impact of knowledge distillation strategies, we employed parametric plots, which illustrate the model’s behavior when two independently trained solutions are interpolated. Specifically, given two sets of model parameters obtained from independently trained students, we evaluate the classification accuracy of interpolated models defined as  
\begin{equation}
\psi \big((1-\eta)x_a^{*} + \eta x_b^{*}\big), \quad \eta \in [-1, 2],
\end{equation}
where $\eta$ controls the interpolation ratio between the two models, and $x_a^{*}$ and $x_b^{*}$ denote the model parameters of the two solutions. Through this approach, parametric plots allow us to examine the stability and similarity of different solutions under varying interpolation ratios.

\begin{figure}
\end{figure}


In Figure~X.(a), $\eta = 0$ and $\eta = 1$ correspond to the cases where the parameters of only one model are used. At both endpoints, the training accuracy reaches nearly 100\%, while the gap between training and testing accuracy is about 30\%. When $\eta$ lies between 0 and 1, the accuracy gradually decreases and drops to below 10\% around $\eta = 0.5$. This sharp decline indicates that the independently trained model parameters represent substantially different solutions, leading to severe misalignment when interpolated.  
%
In Figure~X.(b), the student trained with the annealing strategy is compared against the baseline student. Since the annealed student is initialized with the same weights as the baseline student trained from scratch, the interpolation between $\eta = 0$ and $\eta = 1$ maintains relatively higher accuracy compared to Figure~X.(a). In this case, both students also achieve training accuracies close to 100\%, showing consistent optimization despite the different training strategies.  
%
In Figure~X.(c), the comparison is made between the baseline student and the student trained with IRMKD(GAF). While the interpolation still preserves a reasonable level of accuracy between $\eta = 0$ and $\eta = 1$, the performance drop in the intermediate region is larger than in Figure~X.(b). This suggests that the parameters of the student trained with IRMKD(GAF) diverge more noticeably from those of the baseline student. Moreover, unlike Figures~X.(a) and (b), where the training accuracy of both models approaches 100\%, the student trained with IRMKD(GAF) only reaches about 90\% training accuracy. This may be because certain classes that were well distinguished when using Signal or PI representations became more confounding when the data were transformed into GAF images, thereby increasing ambiguity for the model.
%
Figures~X.(d) and (e) show interpolations comparing TSKD with IRMKD(PI) and TSKD with IRMKD(GAF), respectively. In both cases, although annealing was applied in IRMKD, the accuracy between $\eta = 0$ and $\eta = 1$ drops sharply, which is due to the fact that the comparison is made against TSKD rather than the baseline student. This also indicates that the solutions obtained by the students trained with IRMKD(PI) and IRMKD(GAF) differ substantially from that of the student trained with TSKD.













돌리는게
pamap - ebkd, camkd (0~6)
gene - SP, AT (모든 SP, AT), alpha_ann 전부다.