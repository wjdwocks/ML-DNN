For the extraction of PIs, we follow the methodology described in prior work~\cite{reference} and use the Ripser library to compute persistence diagrams (PDs). The Gaussian kernel parameter in PD computation is set to $0.25$ for GENEActiv and $0.015$ for PAMAP2. The birth-time ranges for PIs are fixed at $[-10, 10]$ and $[-1, 1]$ for GENEActiv and PAMAP2, respectively. Each generated PI is normalized by its maximum value and resized to $64 \times 64$ pixels. For the GAF images, we adopt the GASF method, ensuring that their size matches the PIs at $64 \times 64$ pixels.

We train all models using stochastic gradient descent (SGD) with a momentum of $0.9$, a weight decay of $1 \times 10^{-4}$, and a batch size of $64$. The total number of training epochs is $200$. For time-series models on both datasets, the initial learning rate $lr$ is set to $0.05$, decaying by a factor of $0.2$ at epoch $10$, and subsequently reduced by a factor of $0.1$ every $\frac{t}{3}$ epochs, where $t$ denotes the total number of epochs.

Both teacher and student networks employ the WideResNet (WRN) architecture~\cite{reference}, which is widely used in knowledge distillation research for performance evaluation~\cite{references}. Time-series models utilize 1D convolutional layers, whereas image-based models adopt 2D convolutional layers.

Regarding hyperparameters, the temperature parameter $\tau$ is fixed at $4$ for both GENEActiv and PAMAP2, while the balancing weight $\lambda$ is set to $0.7$ and $0.99$, respectively. For two-teacher setups using PI and signal modalities, the balancing coefficient $\alpha$ is set to $0.7$ for GENEActiv and $0.3$ for PAMAP2. For two-teacher configurations with GAF and signal modalities, $\alpha$ is set to $0.3$ for GENEActiv and $0.7$ for PAMAP2. In the three-teacher framework incorporating PI, GAF, and signal modalities, the balancing coefficients are set to $0.3:0.2:0.5$ for GENEActiv and $0.3:0.4:0.3$ for PAMAP2.

For baseline comparisons, we evaluate several traditional knowledge distillation techniques, including vanilla KD~\cite{reference}, attention transfer (AT)~\cite{reference}, similarity-preserving KD (SP)~\cite{reference}, and simple knowledge distillation (SimKD)~\cite{reference}. The hyperparameters $\alpha_{AT}$ and $\gamma_{SP}$ are set to $1500$ and $1000$ for GENEActiv, and $3500$ and $700$ for PAMAP2, respectively. We also include DIST~\cite{reference}, which models intra- and inter-class relationships for knowledge transfer, and compare against multi-teacher approaches such as AVER~\cite{reference}, EBKD~\cite{reference}, CA-MKD~\cite{reference}, and AdTemp~\cite{reference}. To ensure fairness, only the final-layer logits are used for all distillation baselines.
