내일 해야하는거 
- Discussion 파트 및 Conclusion 작성

5. Discussion
Compared with using a single time-series modality, image-representation–driven KD required longer training time, yet the distilled students achieved higher performance. Converting signals to image representations reveals complementary information that is not easily exploited from raw time series alone. The confusion matrices in Fig. \ref{figure:confusion_matrix} reveal a class-dependent effect. Adding an IR-trained teacher improves some classes but slightly degrades others. This pattern suggests that a single global mixing weight is sub-optimal. A sample-wise scheme that applies a different $\alpha$ for each sample may yield better results. Such adaptivity would consume more resources, but it could deliver higher performance. 
Beyond these findings, since most experiments used WideResNet variants, evaluating cross-architecture distillation (e.g., Resnet→VGG or WRN→MobileNet) would better assess the generality of our experiments.

In future work, we aim to build a stronger student by distilling from three teachers—PI, GAF, and signal—so that all three data forms are leveraged simultaneously. In addition to logit-based distillation, we will incorporate feature-based method to improve performance.





In this study, we investigated image-representation–driven multi-teacher KD for HAR. Image-driven KD consistently outperformed signal-only training. Although GAF generally achieved higher accuracy, when feature-based losses (AT/SP) were added, PI tended to achieve higher final accuracy, indicating better compatibility in feature space. To mitigate the modality and architecture mismatch between teacher and student, we used an annealing strategy. In most cases, Ann surpassed Base, indicating that this annealing strategy is effective. Overall, IR-driven multi-teacher KD is a practical way to improve performance because it consistently achieved higher accuracy than signal-only training in our experiments.