오늘 4.4까지는 무조건 다 쓰는걸 목표로... # 만약 실험때문에 안된다고 한다면, 그부분제외 후 다 쓰고, 실험 다 돌려놓기.

4.4 Analysis on Sensitivity and Compatibility
We conducted additional experiments to investigate both the sensitivity of our framework and its compatibility with other knowledge distillation (KD) techniques. In this section, we analyze the effect of the hyperparameter $\alpha$, which controls the weighting between the two teacher losses in the multi-teacher setting. We also employ parametric plots to further investigate the relationships between different KD strategies. In addition, we use t-SNE visualizations to explore how different KD methods shape the feature space learned by the student. Finally, we evaluate how the proposed methods perform when combined with prior KD approaches such as AT and SP.

4.4.1 Sensitivity Analysis
We evaluated the sensitivity of the multi-teacher setting to the hyperparameter $\alpha$, which controls the relative contribution of each teacher in the distillation loss. The results indicate that for the GAF+Signal combination, both the Base and Ann settings achieved their highest performance at $\alpha=0.3$, after which the accuracy gradually declined as $\alpha$ increased. In contrast, for the PI+Signal combination, only the Ann setting reached its peak performance at $\alpha=0.3$, while the Base setting did not show its maximum at this point. Based on these observations, we adopted $\alpha=0.3$ for our experiments.
\begin{figure}
\end{figure}

To further analyze the impact of knowledge distillation strategies, we employed parametric plots, which illustrate the model’s behavior when two independently trained solutions are interpolated. Specifically, given two sets of model parameters obtained from independently trained students, we evaluate the classification accuracy of interpolated models defined as  
\begin{equation}
\psi \big((1-\eta)x_a^{*} + \eta x_b^{*}\big), \quad \eta \in [-1, 2],
\end{equation}
where $\eta$ controls the interpolation ratio between the two models, and $x_a^{*}$ and $x_b^{*}$ denote the model parameters of the two solutions. Through this approach, parametric plots allow us to examine the stability and similarity of different solutions under varying interpolation ratios.

\begin{figure}
\end{figure}


In Figure~X.(a), $\eta = 0$ and $\eta = 1$ correspond to the cases where the parameters of only one model are used. At both endpoints, the training accuracy reaches nearly 100\%, while the gap between training and testing accuracy is about 30\%. When $\eta$ lies between 0 and 1, the accuracy gradually decreases and drops to below 10\% around $\eta = 0.5$. This sharp decline indicates that the independently trained model parameters represent substantially different solutions, leading to severe misalignment when interpolated.  
%
In Figure~X.(b), the student trained with the annealing strategy is compared against the baseline student. Since the annealed student is initialized with the same weights as the baseline student trained from scratch, the interpolation between $\eta = 0$ and $\eta = 1$ maintains relatively higher accuracy compared to Figure~X.(a). In this case, both students also achieve training accuracies close to 100\%, showing consistent optimization despite the different training strategies.  
%
In Figure~X.(c), the comparison is made between the baseline student and the student trained with IRMKD(GAF). While the interpolation still preserves a reasonable level of accuracy between $\eta = 0$ and $\eta = 1$, the performance drop in the intermediate region is larger than in Figure~X.(b). This suggests that the parameters of the student trained with IRMKD(GAF) diverge more noticeably from those of the baseline student. Moreover, unlike Figures~X.(a) and (b), where the training accuracy of both models approaches 100\%, the student trained with IRMKD(GAF) only reaches about 90\% training accuracy. This may be because certain classes that were well distinguished when using Signal or PI representations became more confounding when the data were transformed into GAF images, thereby increasing ambiguity for the model.
%
Figures~X.(d) and (e) show interpolations comparing TSKD with IRMKD(PI) and TSKD with IRMKD(GAF), respectively. In both cases, although annealing was applied in IRMKD, the accuracy between $\eta = 0$ and $\eta = 1$ drops sharply, which is due to the fact that the comparison is made against TSKD rather than the baseline student. This also indicates that the solutions obtained by the students trained with IRMKD(PI) and IRMKD(GAF) differ substantially from that of the student trained with TSKD.


4.4.1 t-SNE
We examined the t-SNE plots and V-Scores of student models trained with different methods to evaluate the effect of each approach. Figure X.(a) shows the student model trained from scratch, where significant confusion can be observed among classes 2, 3, 10, 11, and 12, as well as between classes 4 and 5. In Figure X.(b), which depicts the student trained with KD(TS), a similar confusion pattern persists, although there appears to be a slight improvement for class 12. Figure X.(c) presents the results of the student trained with PI+Sig $KD_m$, where the V-Score increases to 0.6118. Unlike Figures X.(a) and (b), classes 3 and 12 are more clearly separated, and the clustering distance between classes 4 and 5 becomes wider. Finally, Figure X.(d) illustrates the student trained with GAF+Sig $KD_m$, which achieves the highest V-Score of 0.6380. In this case, the separation between classes 4 and 5 becomes even more distinct, but classes 2, 10, and 11 still remain difficult to distinguish.



4.4.2 Distillation Compatibility
"우리는 우리의 방법론이 다른 기존의 KD 방법론들(SP, AT)와 얼마나 잘 호환되는지(Compatibility)에 대한 실험을 진행하였다. Figure X를 보면, Time Series의 1Teacher의 실험 결과와, IRMKD(PI), IRMKD(GAF)에 SP, AT의 방법론을 적용한 결과를 비교해두었다. SP와 AT 방법론은 Signal을 사용하는 Teacher에 대해서만 적용하여 loss항을 추가하였고, 실험에 사용한 Teacher는 WRN16-3, WRN28-1로 Width가 더 클 때와, Depth가 더 클 때로 선정하였다. (나는 WRN16-1도 추가하여, Student와 크기가 같을 때도 넣고싶긴 함.) Figure X. (a)는 Image Representation Multi Teacher(PI)의 Aver, Base, Ann에 AT를 적용한 것인데, 모든 경우에 대해서 TSKD보다 높은 성능을 달성하였다. 또한 대부분의 경우 Ann까지 적용한 경우에 가장 높은 성능을 달성하였지만, WRN16-3에서는 Aver에서 가장 높은 성능을 보였다. 또한, 앞서 보였던 실험 결과에서는 Teacher의 크기가 WRN16-3이나 WRN16-1에서 가장 높은 성능을 보였던 것과 달리, WRN28-3에서 가장 높은 성능을 보인 것도 주목할만 하다.
Figure X. (b)는 Image Representation Multi Teacher (GAF)에서의 결과인데, 이 또한, 대부분의 경우에서 TSKD보다 월등히 높은 성능을 기록하였고, WRN16-3을 제외한 모든 Teacher 조합에서 Ann이 적용된 경우 가장 높은 성능을 달성하였다(왤까?). 여기에서 또한, WRN28-3 Teacher 조합에서 가장 높은 성능을 달성하였고, 그 다음으로, WRN16-1, WRN16-3 순서로 성능이 높았다.
Figure X. (c)는 Image Representation Multi Teacher (PI)에 SP방법론을 추가한 결과를 보여주는데, 이 또한, 모든 결과에 대해서 TSKD보다 월등히 높은 결과를 보여주고, WRN28-3 Teacher 조합을 제외한 다른 경우에서 Aver의 Acc가 Base보다 높은 것으로 보아, SP 방법론을 추가했을 경우, 최적의 Alpha 하이퍼파라미터 값이 변경될 수 있음을 시사한다. 또한, WRN16-3 Teacher 조합에서 가장 높은 성능을 보였으며, WRN28-3에서 또한 상당히 높은 성능을 보여주었다. 
Figure X. (d)는 Image Representation Multi Teacher (GAF)에 SP방법론을 추가한 결과를 보여주는데, 마찬가지로 모든 경우에서 TSKD보다 높은 성능을 달성하였고, 여기서 또한, Aver가 Base보다 높은 성능을 달성하는 경우가 있는 거로 보인다. 또한 이전 실험의 결과와 달리 WRN28-3에서도 높은 성능을 보여주고 있다.

[t-SNE plot은, (Original, AT, SP)를 한 번에, GAF, PI, WRN281, WRN283 각각에 대한 4개씩 작성.]
[검정색 막대 바는 제거하고, 증가는 +및 red, 감소는 -및 blue로 설정.]
[이거에 맞춰서 다시 생성 및 작성 후, Processing Time에 대한것까지 내일 해서 작성하는 것을 목표.]
[주말에는 처음부터 읽어보며 다시 작성 후, Reference 다는 작업 수행.]









돌리는게
pamap - ebkd, camkd (0~6)
gene - SP, AT (모든 SP, AT), alpha_ann 전부다.

Augmentation쪽 WRN281 추가
이름 다 바꾸기
F-1 Score 추가
SP AT 281, 283만 기록 교수님이 보내주신 Figure 참고.
GAF 참고문헌 [-1, 1]