## 25.1.9 까지 공부할 내용
<li> GAF 코드로 단일 네트워크 학습 돌려보기. (완전한 이해 + 코드 분석) </li>
<li> TPKD (Topological Persistence Guided Knowledge Distillation for Wearable Sensor Data) 논문 다시 읽고 이해. </li>
<li> HAR(인간 행동 분석)에 Deep Learning이 사용된 논문 읽어보기 - Sensor Data를 이용한. </li>
<ol>
<li> Deep Learning for Sensor-based Activity Recognition: A Survey - (큰 도움 안됨.) </li>
<li> Human Activity Recognition Based on Gramian Angular Field and Deep Convolutional Neural Network </li>
<li> Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors </li>
</ol>
<li> 위의 논문들에서 GAF를 이용할 때 GASF를 사용했는지, GADF를 사용했는지 보고, 그 이유와 당위성 체크. </li>

## Human Activity Recognition Based on Gramian Angular Field and Deep Convolutional Neural Network 논문 리뷰.

### Introduction
<li> Human Activity Recognition에서 논의되는 문제들. </li>
<ul>
<li> Shallow Features의 한계 : 기존의 HAR은 사람이 설계한 특징을 기반으로 머신러닝에 활용하는데, 이는 낮은 수준의 활동 인식에는 적합하지만, 높은 수준의 활동 (요리하기/커피 마시기 등)을 인식하는 데 한계가 있다. </li>
<li> 시간적 특성을 충분히 반영하지 못함 : Sensor Data는 시간적 연속성이 중요한데, 기존의 Shallow Feature기반의 방법들은 이러한 시간적 관계를 충분히 반영하지 못한다. </li>
<li> 전통적인 머신러닝의 한계 : 기존의 머신 러닝 알고리즘은 데이터의 전역적 패턴을 학습하는 데 적합하지 않고, 높은 수준의 Feature를 학습하는데 어려움을 겪는다. </li>
<li> Raw Data의 복잡성 : Sensor Data는 고차원적이고, 복잡한 형태를 가지며, 이를 효과적으로 처리하기 위한 모델 설계가 필요하다. </li>
</ul>

<li> 이 논문에서 해결하고자 하는 방법 </li>
<ul>
<li> GAF를 활용한 시간적 특성 인코딩 : 1D 시계열 데이터를 2D 이미지로 변환하여, 시간적 연속성을 효과적으로 반영할 수 있고, 데이터의 전역적(global) 및 지역적(local) 관계를 모두 학습 가능하다. </li>
<li> CNN 사용 : 변환된 GAF 이미지를 입력받아 CNN을 통해 자동으로 Feature를 학습한다. (Shallow Feature 문제를 해결 가능.) </li>
</ul>

### Related Work
<li> GAF에 관하여 </li>
<ul>
<li> GAF는 1D time series 데이터를 2D 이미지의 형태로 만든 것이다. </li>
</ul>
<li> GAF로 변환하는 과정. </li>
<ul>
<li> 입력 데이터를 Min-Max Normalization을 통해 [-1, 1]로 정규화한다. </li>
<li> 정규화된 데이터를 극좌표로 변환한다. 각 데이터는 극좌표에서 각도로 나타나게 되는데, 이는  𝜙 = arccos(x)로 계산된다.</li>
<li> GAF 행렬 아레와 같이 계산한다. </li>
</ul>

![GAF_Matrix](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.9/GAF_Matrix.png)

### 논문에서 GASF와 GADF의 비교.
<li> GASF를 사용했는지, GADF를 사용했는지는 안나와있고, 각각에 대한 비교도 나와있지 않음. </li>
<li> 둘 다 사용했거나 다른 논문을 찾아봐야 할듯. </li>

## GAF 코드로 단일 네트워크 학습.
<li> Parser 부분에 gene_gaf로 놓고 학습을 돌리면 train_custum_gaf의 Main에서 GAF로 변환된 데이터 로더를 불러오도록 바꿈. </li>
<li> get_gene_gaf함수를 통해 gene데이터셋을 각 x, y, z축에 대한 GASF이미지를 각각 변환하여, Depth 방향으로 합쳐서 3channel image형태로 변환. </li>
<li> 3channel image의 형태를 띈 데이터가 되므로, 학습 모델은 Cifar-10/PI-img 와 같은 WRN16-1(2d CNN을 이용하는), WRN16-3, WRN28-1, WRN28-3으로 사용함. </li>
<li> 그리고 Validation Accuracy가 가장 높았을 때의 모델 가중치를 pth.tar파일로 저장함. </li>
<li> 이렇게 저장한 가중치로 Teacher를 두고, 다음에 KD를 설계하여 학습할 것이다. </li>
<li> 개선/수정 해야할 것. </li>
<ul>
<li> 왜 GAF이미지의 크기를 64 x 64로 하였는가? : 선행 연구의 PI-IMG가 64 x 64였기 때문. (그렇다면 이 논문에서는 왜 PI-img를 64 x 64로 했나?) - TPKD논문 참조해보자. </li>
<li> 왜 geneActiv 데이터를 각 Accelerometer 축에 대해서 flatten()으로 하지 않고, 각 축마다 GAF를 따로 변환하여 channel로 합쳤는가? - flatten한게 channel이 적어서 더 가벼울텐데 </li>
<li> 왜 Gramian Angular Summation Field를 채택했는가? : Gramian Angular Difference Field와의 차이점과 다른 논문을 참조해보자. </li>
</ul>

## GAF를 이용한 학습 성능 분석.
<ol>

<li> 전체 공통 참고 사항 </li>
<ul>
<li> GAF이미지는 64 x 64로 설정하였다. </li>
<li> gene_dataset은 3channel Accelerometer로 되어있기에 각 축을 따로 나누어서 GASF로 변환한 후, 다시 Channel로 합쳤다. </li>
<li> 각 학습 결과와 parameter는 TeaGAF폴더에 같은 format으로 저장해두었다. </li>
</ul>

<li> WRN16-1의 결과 </li>
<ul>
<li>  </li>
</ul>

<li> WRN16-3의 결과 </li>
<ul>
<li>  </li>
</ul>

<li> WRN28-1의 결과 </li>
<ul>
<li>  </li>
</ul>

<li> WRN28-3의 결과 </li>
<ul>
<li>  </li>
</ul>

<ol>

## 각 채널별로 GASF를 계산한 것과 Flatten으로 이어붙인 후 GASF를 얻은 것의 시각적 비교.
<li> 각 채널별로 GASF를 계산한 결과 </li>

![GAF_img_3ch](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.9/GAF_img_3ch.png)
<li> Flatten으로 각 채널을 펼쳐서 합친 후 GASF변환 결과. </li>

![GAF_img_3ch](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.9/GAF_img_3ch.png)

## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 100 </li>
<li> batch size = 32 </li>
<li> learning rate = 0.05 - 0.0001 </li>
<li> optimizer = Adam </li>
</ul>