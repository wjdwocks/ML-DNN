## 25.1.9 까지 공부할 내용
<li> GAF 코드로 단일 네트워크(Teacher Model) 학습 돌려보기.  </li>
<li> TPKD (Topological Persistence Guided Knowledge Distillation for Wearable Sensor Data) 논문 다시 읽어보기. </li>
<li> HAR(인간 행동 분석)에 Deep Learning이 사용된 논문 읽어보기 - Sensor Data를 이용한. </li>
<ol>
<li> Deep Learning for Sensor-based Activity Recognition: A Survey - (큰 도움 안됨.) </li>
<li> Human Activity Recognition Based on Gramian Angular Field and Deep Convolutional Neural Network </li>
<li> Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors </li>
</ol>
<li> 위의 논문들에서 GAF를 이용할 때 GASF를 사용했는지, GADF를 사용했는지 보고, 그 이유와 당위성 체크. </li>



## Human Activity Recognition Based on Gramian Angular Field and Deep Convolutional Neural Network 논문 리뷰.
### Introduction
<li> Human Activity Recognition에서 논의되는 문제들. </li>
<ul>
<li> Shallow Features의 한계 : 기존의 HAR은 사람이 설계한 특징을 기반으로 머신러닝에 활용하는데, 이는 낮은 수준의 활동 인식에는 적합하지만, 높은 수준의 활동 (요리하기/커피 마시기 등)을 인식하는 데 한계가 있다. </li>
<li> 시간적 특성을 충분히 반영하지 못함 : Sensor Data는 시간적 연속성이 중요한데, 기존의 Shallow Feature기반의 방법들은 이러한 시간적 관계를 충분히 반영하지 못한다. </li>
<li> 전통적인 머신러닝의 한계 : 기존의 머신 러닝 알고리즘은 데이터의 전역적 패턴을 학습하는 데 적합하지 않고, 높은 수준의 Feature를 학습하는데 어려움을 겪는다. </li>
<li> Raw Data의 복잡성 : Sensor Data는 고차원적이고, 복잡한 형태를 가지며, 이를 효과적으로 처리하기 위한 모델 설계가 필요하다. </li>
</ul>

<li> 이 논문에서 해결하고자 하는 방법 </li>
<ul>
<li> GAF를 활용한 시간적 특성 인코딩 : 1D 시계열 데이터를 2D 이미지로 변환하여, 시간적 연속성을 효과적으로 반영할 수 있고, 데이터의 전역적(global) 및 지역적(local) 관계를 모두 학습 가능하다. </li>
<li> CNN 사용 : 변환된 GAF 이미지를 입력받아 CNN을 통해 자동으로 Feature를 학습한다. (Shallow Feature 문제를 해결 가능.) </li>
</ul>

### Related Work
<li> GAF에 관하여 </li>
<ul>
<li> GAF는 1D time series 데이터를 2D 이미지의 형태로 만든 것이다. </li>
</ul>
<li> GAF로 변환하는 과정. </li>
<ul>
<li> 입력 데이터를 Min-Max Normalization을 통해 [-1, 1]로 정규화한다. </li>
<li> 정규화된 데이터를 극좌표로 변환한다. 각 데이터는 극좌표에서 각도로 나타나게 되는데, 이는  𝜙 = arccos(x)로 계산된다.</li>
<li> GAF 행렬 아레와 같이 계산한다. </li>
</ul>

![GAF_Matrix](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.9/GAF_Matrix.png)

### 논문에서 GASF와 GADF의 비교.
<li> GASF를 사용했는지, GADF를 사용했는지는 안나와있고, 각각에 대한 비교도 나와있지 않음. </li>
<li> 둘 다 사용했거나 다른 논문을 찾아봐야 할듯. </li>



## Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors 논문 리뷰
### 왜 보려고 했냐?
<li> 같은 Wearable Sensor Data를 분석하고, GAF를 이용하기에 여기서는 GAF를 어떻게 사용하는지, GASF, GADF중 무엇을 사용하는지 등을 참고하고 싶어서 </li>
<li> 내용이 재밌어보여서 </li>

### Introduction
<li> 파킨슨 병의 주요 증상 중 하나인 FOG를 잘 감지하고 싶다. </li>
<li> 이 논문에서는 GAF 변환과 Federated Learning을 통해 기존 방식의 한계를 극복하고, 실제 환경에서도 신뢰할 수 있게 하고자 함. </li>
<li> 기존 문제 </li>
<ul>
<li> 1. 환자 내, 환자 간 변동성 </li>
<li> 2. 잘 통제된 환경 </li>
<li> 3. 단일 센서만을 사용(가속도 센서) </li>
<li> 4. 다중 센서 시스템의 복잡성 (실패 지점이 많음.) </li>
<li> 5. 결측치에 대한 민감성. </li>
</ul>

### 논문에서 사용한 GAF 이미지
<li> 이 논문에서는 GASF만 사용한 것 같다. (Related Work 부분에 GAF 이미지를 만드는 방식에 GASF밖에 없음.) </li>
<li> 또한 하드웨어의 부담을 줄이고자 가속도 센서의 세 축중 한 축만 이용하여 GAF를 생성하고, Federated Learning 방식을 이용하여 로컬에서 다 가능하도록 함. </li>
<li> 즉, GASF만을 사용한 것 같다. (근데 채택을 한 이유나 그에 대한 설명이 있진 않음.) </li>
<li> 또한 가속도 센서의 AccV(수직방향), AccML(좌우방향), AccAP(전후방향) 가속도 데이터 중 하나를 선택하여 GA(S)F로 변환하고 학습을 돌리는 듯. (오히려 하나의 Channel만 사용했을 때 Acc가 높음.) </li>



## GAF 코드로 단일 네트워크 학습.
<li> Parser 부분에 gene_gaf로 놓고 학습을 돌리면 train_custum_gaf의 Main에서 GAF로 변환된 데이터 로더를 불러오도록 바꿈. </li>
<li> get_gene_gaf함수를 통해 gene데이터셋을 각 x, y, z축에 대한 GASF이미지를 각각 변환하여, Depth 방향으로 합쳐서 3channel image형태로 변환. </li>
<li> 3channel image의 형태를 띈 데이터가 되므로, 학습 모델은 Cifar-10/PI-img 와 같은 WRN16-1(2d CNN을 이용하는), WRN16-3, WRN28-1, WRN28-3으로 사용함. </li>
<li> 그리고 Validation Accuracy가 가장 높았을 때의 모델 가중치를 pth.tar파일로 저장함. </li>
<li> 이렇게 저장한 가중치로 Teacher를 두고, 다음에 KD를 설계하여 학습할 것이다. </li>
<li> 개선/수정 해야할 것. </li>
<ul>
<li> 왜 GAF이미지의 크기를 64 x 64로 하였는가? : 선행 연구의 PI-IMG가 64 x 64였기 때문. (그렇다면 이 논문에서는 왜 PI-img를 64 x 64로 했나?) - TPKD논문 참조해보자. </li>
<li> 왜 geneActiv 데이터를 각 Accelerometer 축에 대해서 flatten()으로 하지 않고, 각 축마다 GAF를 따로 변환하여 channel로 합쳤는가? - flatten한게 channel이 적어서 더 가벼울텐데 </li>
<li> 왜 Gramian Angular Summation Field를 채택했는가? : Gramian Angular Difference Field와의 차이점과 다른 논문을 참조해보자. </li>
</ul>

## GAF를 이용한 학습 성능 분석.
<ol>

<li> 전체 공통 참고 사항 </li>
<ul>
<li> GAF이미지는 64 x 64로 설정하였다. (500 x 500으로 한번 해보고싶은데 ) </li>
<li> gene_dataset은 3channel Accelerometer로 되어있기에 각 축을 따로 나누어서 GASF로 변환한 후, 다시 Channel로 합쳤다. </li>
<li> batch_size = 32 </li>
<li> epoch = 100 (시간이 너무 오래걸려서 답답해서 줄였다.) </li>
<li> 각 학습 결과와 parameter는 TeaGAF폴더에 같은 format으로 저장해두었다. </li>
<br>
</ul>

<li> WRN16-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> epoch-42, val : 71.9470 에서 최대 Acc </li>
<br>
</ul>

<li> WRN16-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> epoch-75, val : 73.8157 에서 최대 Acc </li>
<br>
</ul>

<li> WRN28-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> epoch-40, val : 70.4911 에서 최대 Acc </li>
<br>
</ul>

<li> WRN28-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> epoch-97, val : 74.8805 에서 최대 Acc </li>
<br>
</ul>

<li> T : WRN16-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> epoch-69, val : 82.6597 에서 최대 Acc </li>
<br>
</ul>

<li> T : WRN16-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> epoch-77, val : 85.9409 에서 최대 Acc </li>
<br>
</ul>

<li> T : WRN28-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> epoch-68, val : 84.2243 에서 최대 Acc </li>
<br>
</ul>

<li> 번외로 T1 : WRN163(GAF img), T2 : WRN1631(sig), S : WRN1611(sig) Student 최종 결과 </li>
<ul>
<li> epoch-45, val : 72.9683 에서 최대 Acc </li>
<li> **오르긴 했는데, GENE_7cls 파일로 돌려본건데 뭔가 잘못돌렸거나 Train 과정에서 loss 설정하는 부분을 다시 봐야할듯** </li>
<br>
</ul>

<ol>

## 각 채널별로 GASF를 계산한 것과 Flatten으로 이어붙인 후 GASF를 얻은 것의 시각적 비교.
<li> 각 채널별로 GASF를 계산한 결과 </li>

![GAF_img_3ch](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.9/GAF_img_3ch.png)
<li> Flatten으로 각 채널을 펼쳐서 합친 후 GAF변환 결과. (펼쳤기 때문에 크기가 1500 x 1500이다.) </li>

![GASF_img_flatten](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.9/GASF_img_flatten.png)




## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 100 </li>
<li> batch size = 32 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>