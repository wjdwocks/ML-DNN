## VQShape 생각해봐야 하는 것.
### Sub-Action 방식으로 했을 때 Remind.
- [사전에 해줘야 할 것]
    - LLM을 이용하여 Dataset의 각 Class에 대해서 Sub-Actions를 추출한다. (prompt : 이 Action을 수행하기 위한 Sub-Actions를 N개 추출해줘.)
    - 얻어진 Sub-Actions를 LLM Text Encoder/Tokenizer를 통해 Embedding Vector로 변환한다.
    - 얻어진 Vector들 사이의 Similarity 를 계산하여, 임계값 이상인 것들은 하나만 남기고 제거한다.
- [Training 과정]
    - Sample X 마다 그 Label을 이용해서 얻은 Sub-Actions를 Text Guidance로 사용함.
    - LSA방식을 통해서 Signal쪽 Token과 Text쪽 Token을 Cross Attention + Residual Connection을 통해서 Multi Modality Alignment를 달성함.
    - Signal Token을 이용해서 VQShape의 Shape쪽 framework를 수행한다.
    - Text Token을 이용해서 Text관련 framework를 수행한다.
- [Text쪽 구체화]
    - Text Token은 나중에 CrossModal Alignment에 의해서 Signal Token과 같은 의미를 가지게 될 것이다.
    - 이 Text Token은 Input으로 얻어진 Sub-Action Vector들의 순서와 같을 것이기 때문에 각 Token이 어떤 Sub-Action을 의미해야 하는지 이미 앎.
    - 전체 Sub-Actions(Action Codebook)에서 어떤 Sub-Action인지 알기 때문에 CrossEntropy로 직접 비교 가능.

### Sub-Action추출하는 것 까지는 된다고 생각.
- 그래서 그거로 뭐할건데? 이것까지 알고 있다고 해서 잘 되는거 알겠는데, 이거로 어떤 참신한 것을 할 수 있을지 생각해보아야 함.
- Signal을 Token으로 쪼개서 Subshape알고, SubAction알아. 그래서 이걸 이용해서 뭘 할 수 있을까?
- ex1) Video + Signal 데이터셋에서, 이 Signal의 Embedding을 보고, Video의 어떤 부분인지 맞출 수 있다.? → 이거는 내 생각은 아니고, 최대한 지양해보자.
    - 그러면, 실제로 이 Signal의 Embedding으로 맞춘 Sub-Action이 "팔로 무언가를 휘두른다"고 예측했을 때, 그 Token(embedding)이 Video Data Sample의 이 Embedding과 비슷한대? 해서 그 Embedding을 가져왔을 때 실제로 그 Video의 time-indices를 보고 뭔가를 휘두르고 있다면 이거대로 좀 참신할 수 있다.
    - MMAct, UTD-MHAD, Berkeley-MHAD 데이터셋 참고.
- ex2) Time-series 데이터의 Sub-Shape과 Sub-Action을 알게 되었을 때 그걸 이용해서 뭐를 해볼 수 있을까?
    - 


## 3Teacher 논문 진행상황
### 그림 고치기.
- 

### Discussion & Conclusion Part
- 

### 앞에서부터 Rewrite 진행상황
- 




### 해야 할 것
- VQShape - 1
- 제안서 - 2
- 캡디 - 2
- 과제 - 2
- 토익 - 3
- 3Teacher - 3 넌 나중에 보자.