## VQShape 생각해봐야 하는 것.
### Sub-Action 방식으로 했을 때 Remind.
- [사전에 해줘야 할 것]
    - LLM을 이용하여 Dataset의 각 Class에 대해서 Sub-Actions를 추출한다. (prompt : 이 Action을 수행하기 위한 Sub-Actions를 N개 추출해줘.)
    - 얻어진 Sub-Actions를 LLM Text Encoder/Tokenizer를 통해 Embedding Vector로 변환한다.
    - 얻어진 Vector들 사이의 Similarity 를 계산하여, 임계값 이상인 것들은 하나만 남기고 제거한다.
- [Training 과정]
    - Sample X 마다 그 Label을 이용해서 얻은 Sub-Actions를 Text Guidance로 사용함.
    - LSA방식을 통해서 Signal쪽 Token과 Text쪽 Token을 Cross Attention + Residual Connection을 통해서 Multi Modality Alignment를 달성함.
    - Signal Token을 이용해서 VQShape의 Shape쪽 framework를 수행한다.
    - Text Token을 이용해서 Text관련 framework를 수행한다.
- [Text쪽 구체화]
    - Text Token은 나중에 CrossModal Alignment에 의해서 Signal Token과 같은 의미를 가지게 될 것이다.
    - 이 Text Token은 Input으로 얻어진 Sub-Action Vector들의 순서와 같을 것이기 때문에 각 Token이 어떤 Sub-Action을 의미해야 하는지 이미 앎.
    - 전체 Sub-Actions(Action Codebook)에서 어떤 Sub-Action인지 알기 때문에 CrossEntropy로 직접 비교 가능.

### Sub-Action추출하는 것 까지는 된다고 생각.
- 그래서 그거로 뭐할건데? 이것까지 알고 있다고 해서 잘 되는거 알겠는데, 이거로 어떤 참신한 것을 할 수 있을지 생각해보아야 함.
- Signal을 Token으로 쪼개서 Subshape알고, SubAction알아. 그래서 이걸 이용해서 뭘 할 수 있을까?
- ex1) Video + Signal 데이터셋에서, 이 Signal의 Embedding을 보고, Video의 어떤 부분인지 맞출 수 있다.? → 이거는 내 생각은 아니고, 최대한 지양해보자.
    - MMAct, UTD-MHAD, Berkeley-MHAD 데이터셋 참고.
    - Signal Embedding에 어떤 Projection Layer를 둬서 Video Embedding과 비슷해지도록 함.
        - 이미 Signal과 Video가 매핑되어있는 데이터셋이기 때문에 Video Encoder를 Frozen하면, Signal Embedding을 가지고 Video Embedding을 예측하게 할 수 있다.
    - 그 다음, 이 예측한 Video Embedding과 비슷한 Embedding들을 Similarity 기반으로 Top 5개를 찾는다. (검색이라고 볼 수 있나?)
        - Batch 내의 다른 Sample들의 Video Embedding들에서 찾을 수도 있고, 어디서 찾을지는 구현에 따라 다를듯?
    - 그렇게 찾아진 Video Embedding 5개를 원래 영상으로 복원했을 때 실제로 이 Signal Embedding의 Sub-Action과 같다면 이걸 보여주는 것이 가능한가?

- 위의 것이 되려면 다음과 같은 것들을 먼저 고려해야 함.
    1. 같은 샘플 X로 부터 얻어진 Signal과 Video가 있을 것인데, 이것을 각각 Token화 했을 때 각 Token끼리 같은 것을 의미하는가?
        - 예를 들어 1~500까지의 Time-Step/Frame이 있는 샘플 x는 원본 자체로는 100번 Time-step과 100번 Frame은 분명히 같은 정보를 의미할 것임. → 여기까진 OK
        - 근데, 서로 별개의 Encoder를 통해 Token화된 10번 Signal Token과 10번 Video Token도 같은 정보를 의미할 것인가? → X
            - → 이건 False임. Token의 Index가 같다고 해서 같은 "순간"이나 같은 "Sub-Action"을 의미한다고 볼 수 없다는 것임. 즉, 32개의 Tokens는 500 Time-Step/Frame을 32개로 압축한거지, 각각의 Index가 정확이 어떤 frame-index를 대표한다는 보장이 없다. 각 Modality에서 얻어진 Token의 index가 같은 위치일 리도 없다.
            - 그렇기 때문에, 애초부터 Signal Token을 Projection Layer로 같은 Index의 Video Token의 표현을 만들어낼 수조차 없게 됨.

    2. Video Embedding(Token)을 가지고 원본 영상을 복원할 수 있는가?
        - 이것도 아마 안될거임.

- 어떻게 Token들이 Mapping되게 할 수 있나?
    - 뇌를 비우고 내일 다시보자

- ex2) Time-series 데이터의 Sub-Shape과 Sub-Action을 알게 되었을 때 그걸 이용해서 뭐를 해볼 수 있을까?
    - 위의 것과 다른 방식을 생각해보자.


## 3Teacher 논문 진행상황
### 그림 고치기.
- Confusion Matrix에 글꼴 통일(아랫첨자로 수정)
- 큰 히스토그램 (GENE에서 2Teacher WRN다른거) 글씨크기 키우기 완

### Discussion & Conclusion Part
- Discussion은 작성을 하긴 했는데, 더 다듬고, 내용도 좀 바꾸거나 해야 할 것으로 보입니다.

### 해야할 남은 것.
- 수정된 것들에 대해서 추가작성 혹은 수정 필요.
- 2Teacher WRN 다른거 표에서 그림으로 바뀌어서 이거 수정 필요.
- SP Maps들 비교하는거 Table은 넣었는데, 그거에 맞춰서 SP Map visualization 파트 수정 필요.
- 3Teacher WRN 다른거 GENEActiv + PAMAP2로 합쳐야 하는데 아직 못함.
- Discussion 좀 더 잘 쓰고 싶은데, 부족한듯 + Conclusion도 써야함.
- Reference 달면서 글 내용/일관성 맞춰야 함.




### 해야 할 것
- VQShape - 1 → 토요일까지 생각해보고 일요일에 그림그려보는거로..
- 제안서 - 2 → 1차년도 초안느낌으로 썼는데, 내용고도화 필요할듯. (주말에)
- 캡디 - 2 → 오늘 대부분 끝내고, 판넬은 나중에 ㅇ
- 과제 - 2 → 오늘 안에 끝내자.. ㅇ
- 토익 - 3 → 안할 순 없어서 원래 하려던거 ㅇ
- 3Teacher - 3 해야할 것만 정리해두고, 나중에 다시..