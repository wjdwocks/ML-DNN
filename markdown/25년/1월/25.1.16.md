## 25.1.16 까지 공부할 내용
<li> GAF 코드로 단일 네트워크 (Teacher의 가중치를 위한) Train Manager부분 분석  </li>
<li> GENE_7cls 에 있는 코드로 2 Teacher KD (sig + GAF)를 하면 성능이 이상하게 나옴. 이쪽 코드를 좀 더 보고, KD쪽이랑, sp1t 이런얘들 뭐였는지 공부. </li>
<li> 그 후 GENE_7cls 부분의 코드를 내꺼랑 맞춰서 학습 돌려보고, 2 Teacher KD (sig + GAF) 성능 뽑아보기. </li>
<li> 논문 리뷰 (너무 자세히는 말고) </li>
<ol>
<li> Semantics-Aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition (같은 GAF를 사용한 논문) </li>
<li> CROSS-MODAL Knowledge Distillation for Vision-To-Sensor Action Recognition / 여까지 읽음. </li> 
<li> Similarity-Preserving Knowledge Distillation (SP_Loss를 제안한 논문인듯. 중간 Layer에서 Feature Map을 비교하는), 궁금해서 읽어보고 싶어짐 </li>
<li> Diffusion Model + KD를 이용한 Object Detection 논문 </li>
<li> 발바닥 압력에 기반한 이상치 인식을 Multi Scale의 Cross Attention으로 해보는 논문 - Cross Attention, Fig6를 완전히 이해해보자. (Attention 매커니즘과 (Transformer 이해 필요.)) </li>
</ol>
<li> 위에 논문을 이해해보기 위해서 원래 하려고 했던 Transformer 모델에 대해 공부해보려고 함. (RNN, LSTM, Seq2Seq, Attention, Transformer 순서로 공부할 것.) </li>

## 갑자기 생각난 전에 공부하려고 했던 것.
### GAP, 1x1Conv Layer, Flatten, Pooling의 차이.
<li> GAP : Fully Connected 레이어 없이 출력 크기를 줄일 수 있다.(학습 파라미터가 없다.) 각 채널의 가로 x 세로 영역의 평균값을 계산한다.(8 x 8 x 512 -> 1 x 512) </li>
<li> 1x1 Conv Layer : 채널 수를 늘리거나 줄이고 싶을 때 사용함 (8 x 8 x 512 -> 8 x 8 x Output_channel) </li>
<li> Flatten : 그냥 전부 다 펼쳐서 8 x 8 x 512 → 1 x (8 x 8 x 512) 로 만듦. </li>
<li> Pooling : Feature Map의 크기를 줄이고, Channel 수를 조절하기 위해 사용. Kernel 크기에 따라 결과가 다름. (8 x 8 AvgPooling을 사용하면 GAP와 결과가 같다.) </li>

## Semantics-Aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition 논문
### Abstract & Introduction
<li> Vision 기반의 행동 인식은 카메라의 특성상 가려짐, 밝기 변화, 시점의 변화 등에 취약하지만, Wearable Sensor Data의 경우 1차원 시계열 신호를 통해 인간의 동작을 포착함으로써 이러한 문제를 완화할 수 있다. </li>
<li> 동일한 행동(class)에 대해 Vision Sensor Data와 Wearable Sensor Data는 서로 비슷하거나 상호 보완적인 경우가 많음. </li>
<li> 하지만, Vision Sensor Data와 Wearable Sensor Data 사이에는 큰 Modality의 차이가 존재한다. (2차원과 1차원 인것도 있고 그 이상의 차이가 존재함.) </li>
<li> 이 논문에서 주장하는 SAKDN은 Wearable Sensor - Teacher, Vision Sensor Data (RGB) - Student 에게 주고, 학습시킨다. </li>
<li> 또한 Multi Teacher를 사용하여, T1에는 원시 시계열 데이터를, T2에는 GAF로 변환한 시계열 이미지를 전달해준다. </li>
<li> 이 논문에서의 KD-Loss 항 구성 </li>
 <ol>
 <li> T1과 T2의 Intermediate Layer에서 Feature Map을 추출함. </li>
 <li> 두 Feature Map을 SPAMFM의 방법론에 의하여 Fusion한다.(유사성을 보존하고, T1, T2의 가중치를 잘 조절함.) </li>
 <li> 그렇게 합쳐서 나온 Feature Map과 Student의 Intermediate representation(Feature Map)을 비교하여 첫 번째 Loss항을 구성한다. </li>
 <li> 두 번째로, 기존의 KD_loss항인 T1과 S의 Logits 비교, T2와 S의 Logits을 비교하는 Loss 항. </li>
 <li> 세 번째로, Student의 출력을 정답과 비교하는 Cross Entropy Loss 항. </li>
 </ol>

### Related Work
<li> Multi Modal Action Recognition </li>
<ul>
<li> 서로 다른 모달리티(데이터 구조, 여기선 Vision Data, Sensor Data) 간의 행동 인식을 처리하는 것을 의미함. </li>
<li> 기존의 대부분은 1차원 시계열 신호만 사용하여 행동을 인식해왔다. </li>
<li> 하지만, time series data는 지역적 시간 관계, color, Texture 정보가 부족하다. </li>
<li> 또한 이전의 방법론들은 Wearable Sensor와 Vision Sensor간의 의미적 관계를 무시해왔다. </li>
<li> 이 논문에서 제시한 SAKDN에서는 Semantics-aware information(의미 인식 정보)를 활용하여 Multimodal Feature를 융합하고, 지식 증류, 표현 학습을 유도한다. </li>
<li> 또한, SPAMFM(플러그 앤 플레이 모듈)을 제안하여 아레의 세가지를 통합한다. </li>
<li> 모달리티 내 유사성 : Time Series Data를 GAF와 원시 형태로 사용했을 때의 유사성? </li>
<li> 의미적 임베딩(semantic embeddings) : 데이터의 의미를 벡터 공간에 표현하는 기법. (데이터의 상황적 정보나 라벨의 의미적 관계를 의미함.) (행동 클래스 걷기 vs 뛰기 는 유사한 행동이므로 벡터 공간에서 가깝게 나타난다.) </li>
<li> 다중 관계 지식(multiple relational knowledge) : 데이터 간의 여러 관계적 특성을 학습하고, 활용하는 것을 의미함. (시간적 관계 : 원시 시계열 데이터에서 시간적 패턴), (시각적 관계 : Vision Sensor Data에서 시각적 패턴을 학습), (의미적 관계 : 행동 클래스와 관련된 정보를 기반으로 Sensor-Vision 데이터간 관계를 학습.) </li>
</ul>

### Proposed Method
<li> 1. SAKDN 프레임워크 구성 : Wearable Sensor Data를 기반으로 가상 이미지를 생성함 (GAF) </li>
<li> 2. 다중 Teacher 네트워크의 학습 및 지식 증류를 위해 Student 네트워크 학습. </li>
<li> 3. 다중 Loss 항목들을 구성함. : SP Loss, CE Loss, GSDM Loss, Soft-target Loss </li>
<li> SPAMFM(Similarity Preserving Adaptive Multi-Modal Fusion) : Attention 기반 지식 전이 기법을 이용하여 Modality 내의 유사성을 계산한다. </li>
<li> Modality 내의 Similarity Matrix행렬을 얻은 뒤, 유사성을 보존한 Global Context Embedding을 학습한다? </li>
<li> 뭔소린지 모르겠다. Attention 기반 지식 전이 기법을 공부해보자. </li>

### 내 생각.
<li> 이 논문에서도 GASF를 사용한 것으로 보인다. (GAF를 설명할 때 삼각함수의 합을 이용하는 GASF가 나옴.) </li>
<li> GASF를 사용한 이유는 GASF가 데이터의 시간적 상관관계를 강하게 표현하고, 시계열 데이터의 전역적인 구조를 학습하기에 적합하기 때문으로 보임. </li>
<li> GASF는 항상 대칭 행렬이다. 또한 시계열 데이터의 전체적인 시간적 상관관계를 학습하는 데 적합하다. </li>
<li> GADF는 대칭행렬이 아님. 또한 두 시간 단계 간의 변화 정도나 데이터의 미세한 시간적 변화나 국소적인 특징을 학습할 때 활용된다. </li>
<li> 이 논문에서는 x, y, z축의 GAF이미지를 따로 생성한 뒤, x, y, z축을 각각 RGB 이미지처럼 결합하여 n x n x 3의 이미지 형태로 변환하여 시각화해줬다. 나도 각각 변환한 후 depth 방향으로 합쳐줬으니 똑같이 구성하긴 한듯 하다. </li>
<li> Attention 기반 지식 전이가 무엇인지 알아봐야 이 논문의 proposed Method를 이해할 수 있을 것 같다. </li>


## CROSS-MODAL Knowledge Distillation for Vision-To-Sensor Action Recognition
### Abstract & Introduction
<li> Input Data가 Video Stream과 Sensor Data 두가지로 나뉜다. </li>
<li> KD를 사용하기 위해 Student에는 GAF를 이용한 Sensor Data를 사용한다. </li>
<li> Teacher는 Video Stream으로 학습을 함. </li>
<li> KD 과정에서 사용하는 Loss함수로 DASK(Distance and Angle-wised Semantic Knowledge Loss)를 아레에서 설명해줄 것임. </li>
<li> 논문에서 사용된 GAF이미지 (클래스별로 대표 이미지를 보여준듯 하다.)는 그림이 y = -x 대각선에 대칭인 것으로 보아 GASF를 사용한 것 같다. (딱히 언급이나, 이유는 적혀져 있지 않다.), (cos(θi + θj)나 cos(θj + θi)나 같기에 GASF는 대칭임.) </li>

![VSKD](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/1월/25.1.16/VSKD.png)

### Methodology
<li> DASK Loss (Distance and Angle-wised Semantic Knowledge Loss) </li>
<ul>
<li> DASK Loss는 L_kd + β(L_d + L_a) + γL_s 로 나타난다. </li>
<li> L_kd 는 기존 KD의 logits을 비교하는 loss 함수. </li>
<li> L_d : Teacher(Vision Modality), Student(Sensor Modality)에서 같은 Action Activity에 대한 Feature Representation 간의 거리(Euclidean Distance)를 정렬한다. (서로 다른 Modality에서 생성된 Feature들은 Representation의 분포가 다를 것이기 때문에, (T, S) 간의 샘플 쌍 거리를 일치시키도록 설계함.) </li>
<li> L_a : Teacher, Student간 Feture Representation 벡터의 각도 차이를 최소화하여 두 모달리티 간 관계 정보(Semantic Relationships)를 정렬함. (Vector로 표현한 각도 차이는 크기의 차이에 덜 민감하고, Modality간 구조적 관계를 유지하는 데 유리하다.) </li>
<li> L_s : Teacher와 Student의 최종 Feature Map이 같은 의미(Semantic Information)을 공유하도록 강제한다. (서로 다른 Modality는 일반적으로 Raw Data의 표현 방법이 다르기 때문에, 마지막 Layer에서 Semantic Alignment를 보장해야 한다.) </li>
</ul>


## Transformer를 공부하기 위한 선행 공부들
### RNN
<li> Sequence 데이터를 처리하기 위한 신경망 알고리즘. </li>
<li> 1. 입력 데이터를 입력 Sequence(Vector)로 표현하여 넣어줘야 한다. </li>
<li> 2. 아레와 같이 W, V, U 행렬들이 우리가 학습해야 할 파라미터이다. </li>
<li> 3. W, V, U의 초기값은 랜덤으로 설정이 될 것이고, 경사하강법을 통해서 업데이트 됨. </li>
<li> 4. h는 hidden state로, 현재까지의 Sequence 정보를 축적하고 있는 상태이다. RNN의 시간축을 따라 순차적으로 업데이트되고, 이전 hidden state(h_{t-1})와 현재 입력(x_t)를 결합하여 계산한다. </li>
<li> 5. o는 output vector로, Hidden State를 바탕으로 계산된 출력 값으로, 모델의 최종 예측값을 얻기 위한 중간 단계이다. o_t는 hidden state h_t를 기반으로, 현재 time step에서 모델이 생성한 가중치 기반의 해석이다. </li>
<li> 일단 이렇게 알고만 있으라는디 </li>
 
 ![RNN](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/1월/25.1.16/RNN.png)

### LSTM
<li> 기존 RNN은 장기 의존성이라는 문제가 있었다. </li>
<li> 장기 의존성 문제란? : time sequence가 점점 길어진다면, 뒤로 갈 수록 chain rule에 의해 계산되는 식이 0에 수렴한다는 것이다. </li>

![장기 의존성 문제](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.1.16/long-term.png)
<li> 장기 의존성 문제의 사례 : '당신의 내면의 힘을 과소평가하지 마세요' 를 번역할 때 'Don't underestimate your inner strength'로 해석이 될텐데, 마세요는 마지막이지만, Don't는 첫 번째 time step이기 때문에, 서로 의미적으로 가까운 단어이지만, Long Term dependency에 의해 학습이 잘 되지 않을 것이다. </li>
<li> LSTM이 기존 RNN과 다른 점 : Gate Algorithm(forget gate, input gate, candidate gate, output gate) </li>
<li> forget gate : 이전 Hidden State와 현재 입력(x)를 받아서 Forgot Gate를 통해 잊을 것을 파악한 뒤 이전 Cell State에서 잊을 정보를 지운다. </li>
<li> Input Gate : 이전 Hidden State와 현재 입력 x를 받아서 Input Gate를 통해 기억할 것을 파악한다. </li>
<li> Candidate Gate : 값을 -1 ~ 1 사이의 값으로 정규화 시키는 역할을 하며, Input Gate와 곱하여서 Cell State에 기억할 정보를 추가하도록 한다. </li>
<li> Output Gate : 이전 Hidden State와 현재 입력 x를 받아서 Output을 출력하고, 업데이트된 Cell State와 결합하여 다음 Hidden State를 생성하는 역할을 한다. </li>
<li> 즉, Gate 알고리즘으로 Cell State(장기기억)을 관리하고, Hidden State(단기 기억)은 그대로 또 유지하는 것이 LSTM이다. </li>

![게이트 알고리즘](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/1월/25.1.16/gate_algorithm.png)
<li> Multi Layer LSTM </li>
<li> 첫 번째 Layer에서는 위와 똑같이 동작하지만, 출력을 내지는 않음. </li>
<li> 첫 번째 Layer에서의 모든 Time Step을 지난 후에 다음 Layer로 넘어간다. </li>
<li> 첫 번째 Layer에서는 이전 time step의 Hidden State와 이번 time step의 입력 x를 이용하여 파라미터들을 학습한다. </li>
<li> 이후의 Layer에서는 이번 time step의 hidden state를 계산하기 위해 이전 time step의 Hidden State(처음은 똑같이 0)와 이전 Layer에서의 같은 time step에서의 Hidden State를 사용한다. </li>
<li> 최상위 layer에서만 출력을 낸다. </li>


### Sequence To Sequence 알고리즘
<li> 이 알고리즘은 Sequence to Sequence Learning with Neural Networks 논문에서 처음 제안되었다. </li>
<li> 기계 번역에서의 주요 문제 중 하나인 장기 기억(long-term dependency)문제를 해결하기 위해 LSTM을 기본 단위로 사용한다. </li>
<li> 또한, 기계 번역에서의 두 번째 주요 문제인 문장 내 어순 및 단어 개수의 불일치를 해결하기 위한 접근 방식으로 도입되었다. </li>
<li> 기본 구조 </li>
<ul>
<li> Seq2Seq 모델은 LSTM을 기반으로 설계된 Encoder-Decoder 구조로 입력 시퀀스를 처리한다. </li>
<li> Encoder : 입력 Sequence를 처리하며 최종 Hidden State를 생성한다. </li>
<li> Context Vector : 마지막 LSTM의 Hidden State를 요약된 정보로 사용하는데, 이를 Context Vector라고 한다. </li>
<li> Decoder : Context Vector를 초기 상태로 받아, 번역된 문장을 생성한다. (Encoder와 Decoder는 당연히 서로 다른 가중치 파라미터를 사용한다.) </li>
</ul>
<li> 개선될 부분 : Seq2Seq 모델은 Context Vector(Encoder-Decoder 사이의 Cell/Hidden State)가 고정된 크기라는 점에서 입력 Sequence가 긴 경우 정보 손실이 발생할 수 있다는 문제가 있다. </li>
<li> 내가 이해한 것. (정리 해보자.) </li>
<ul>
<li> Encoder 부분도, Decoder 부분도 모두 LSTM으로 되어있다. </li>
<li> Encoder 부분에서는 출력을 내는 대신에, Hidden State와 Cell State만 반복해서 업데이트를 수행한다. </li>
<li> 그렇게 되면 마지막 Hidden State와 Cell State는 문맥에 대한 모든 정보가 들어가게 됨. 이것을 Context Vector라고 한다.(Context Vector = Cell State + Hidden State) </li>
<li> Decoder 부분에서는 이 Context Vector와 SOS(Start of Sentence)의 초기값을 이용(Decoder의 Hidden State와 Cell State의 초기 가중치로 설정)하여 단어를 출력하게 되고, 이전 Hidden State와 Cell State를 이용하여 이번 LSTM(time step)의 Cell State와 Hidden State를 업데이트하여 단어를 출력하는 것을 반복한다. </li>
<li> 여기에서 입력 sequence와 출력 sequence의 길이가 달라질 수 있는 이유는 EOS가 나올 확률이 가장 높아질 때까지 단어 생성을 반복하기 때문이다. </li>
</ul>

![Seq2Seq](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/1월/25.1.16/Seq2Seq.png)


### Attention Model
<li> Seq2Seq에서는 입력 시퀀스가 길어지더라도 Context Vector의 크기가 고정되어있기에, Sequence들의 정보가 손실된다는 문제가 있다. </li>
<li> 이전의 LSTM 기반의 Decoder에서 출력을 생성할 때 참고하는 것 </li>
<ul>
<li> 1. 이전 Time Step의 Hidden State </li>
<li> 2. 현재 레이어의 Cell State </li>
<li> 3. 같은 Time Step에서 하위 레이어의 Hidden State </li>
<li> 4. 입력 단어(이전 Time Step에서 출력한 단어) </li>
</ul>
<li> Attemtion의 아이디어 : Decoder에서 출력 단어를 예측하는 매 시점(time step)마다, Encoder에서의 전체 입력 문장을 다시 한번 참고한다. 이 때, Decoder의 예측 시점(time step)에 연관 있는 단어에 더 집중(Attention) 하여 참고를 하게 된다는 것이다. (매 출력마다 Encoder의 모든 Hidden State를 동적으로 활용한다.) </li>
<li> 어떻게 Attention이 작동하는가? </li>
<ul>
<li> Attention Score를 계산한다 - Decoder의 현재 Hidden State와 Encoder의 각 Hidden State 간의 연관성을 계산하여 Attention Score를 계산한다. (대표적으로 dot product) </li>
<li> 위의 과정을 거쳐서 현재 Time Step의 Decoder에 대한 Hidden State와 모든 입력 Sequence의 Hidden State간의 값 Attention Score(T개)가 생성이 됨. </li>
<li> 위에서 얻어진 Attention Score를 Softmax를 거치게 하여 확률값으로(합이 1) 바꿔준다. </li>
<li> 기존의 Seq2Seq에 있던 Context Vector는 Decoder의 각 Layer의 초기 Hidden/Cell State를 초기화하는데에만 사용하게 됨. </li>
<li> Attention Score를 기반으로, 입력 Sequence의 Hidden State를 가중합하여 Weighted Context Vector를 생성한다.(벡터임) </li>
<li> 얻어진 Weighted Context Vector와 이전 출력의 벡터 임베딩(벡터)를 결합하여 하나의 입력 벡터를 생성한다. (Concatenation) </li>
<li> 위의 입력 벡터와 이전 Time Step의 Hidden State, Cell State를 고려하여 이번 Time Step의 Hidden/Cell State를 계산한다. </li>
<li> 현재 time step의 Hidden State와 Weighted Context Vector를 결합하여 출력 단어의 확률 분포를 계산한다. </li>
<li> Softmax로 출력 단어 분포를 확률을 계산하고, 최종적으로 출력할 단어를 선택하게 된다. </li>
</ul>

![Attention](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/1월/25.1.16/Attention.png)
<li> 내가 이해한 Attention Mechanism </li>
<ul>
<li> 기존의 Seq2Seq의 Context Vector는 Decoder에서 각 첫 번째 Time Step에서 초기 Hidden/Cell State를 정하는 데에만 사용한다. </li>
<li> 각 Time Step(t번쨰)의 입력으로는 이전 Weighted Context Vector(t-1 번쨰)와 이전 Time Step의 출력의 Embedding Vector를 Concatenate한 것을 사용한다. </li>
<li> Weighted Context Vector는 현재 time step의 Hidden State와 Encoder의 모든 Hidden State를 비교(dot product 등의 방법으로)하여 Attention Score를 얻고, 이를 Softmax로 확률값으로 바꿔준 다음, Encoder의 모든 Hidden State를 이 확률값들과 가중합하여 벡터로 얻을 수 있다. </li>
<li> 이제 입력값과 Hidden State(t-1), Cell State(t-1)을 사용하여 이번 time step의 Hidden State와 Cell State를 새로 얻는다. </li>
<li> 이렇게 얻어진 이번 time step의 Hidden State를 이용하여 다시 Weighted Context Vector(t번째)를 다시 계산한다. </li>
<li> 이렇게 얻어진 이번 time step의 Weighted Context Vector(t번째)와 Hidden State(t번째)를 결합하여 출력 단어의 출력 분포를 계산한다.  </li>
<li> 계산된 출력 분포를 확률로 바꿔서 최종 출력할 단어를 선택한다. </li>
</ul>




<li> 전체 공통 참고 사항 </li>
<ul>
<li> GAF이미지는 64 x 64로 설정하였다. (500 x 500으로 한번 해보고싶은데 ) </li>
<li> gene_dataset은 3channel Accelerometer로 되어있기에 각 축을 따로 나누어서 GASF로 변환한 후, 다시 Channel로 합쳤다. </li>
<li> batch_size = 64 </li>
<li> epoch = 200 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum(0.9) + SGD + λ(0.0001, L2정규화항 가중치) </li>
<li> loss : T = 4, lambda = 0.7 (KD_loss 항에 더 큰 가중치) </li>
<li> 각 학습 결과와 parameter는 TeaGAF폴더에 같은 format으로 저장해두었다. </li>
<br>
</ul>

<li> WRN16-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 63.0491 </li>
<li> 63.3270 </li>
<li> 62.9797 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> WRN16-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 64.0389 </li>
<li> 63.9521 </li>
<li> 63.9173 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> WRN28-1의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 63.2922 </li>
<li> 64.0042 </li>
<li> 63.3096 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> WRN28-3의 결과 (GAF 단일 네트워크) </li>
<ul>
<li> 64.1604 </li>
<li> 64.8029 </li>
<li> 65.3933 </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN16-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 68.9703 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN16-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 68.2063 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN28-1(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 67.0082 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>

<li> T : WRN28-3(GAF), S : WRN16-1(sig) 결과 </li>
<ul>
<li> 67.5464 </li>
<li>  </li>
<li>  </li>
<li> 평균 :  </li>
<br>
</ul>
<ol>

### 하려고 생각하는 남은 것들.
<li> 일단 GAF 단일 네트워크 3번씩 학습해서 평균내기 </li>
<li> GAF Teacher 하나만 사용해서 각 조합마다 3번씩 학습해서 평균내기. (Teacher는 WRN-281,283,161,163, Student는 WRN-161만.) </li>
<li> Tranformer 모델 공부 후 교수님이 종이로 주신 Transformer 논문 이해해보기. </li>
<li> 코드 추가하여, 위의 학습이 끝나자 마자 T : (Sig + GAF), S : Sig 의 2 Teacher 학습 바로 진행할 수 있도록 하기. </li>
<li> SPKD 논문 읽어보기 </li>
<li> 시간이 된다면 Diffusion Model에 대해서 공부를 한 뒤 두 번째 논문 이해해보기. </li>


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>