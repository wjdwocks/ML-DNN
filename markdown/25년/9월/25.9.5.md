## Abstracted Shapes as Tokens- A Generalizable and Interpretable Model for Time-series Classification 논문 읽기
### Introduction
- Times series는 sampling ratio, window length, noise 등의 문제로 단일 모델로 여러 데이터셋에 공용으로 사용하는게 어려움.
- 이로 인해 현재의 다수 TS 사전학습 모델은 트랜스포머 백본으로 마스크/다음 구간 예측을 하지만, 표현은 대체로 블랙박스이며 LLM처럼 “이산 토큰” 개념이 빈약하다는 문제가 있다.
- 즉, 한 모델이 성인의 행동과 어린 아이의 행동 모두에 적용되기가 어렵다고 예시를 들었다.
- 기존의 방법 1
    - Shapelet
        - 방식 : 시계열에서 분류에 유용한 여러 개의 대표 부분구간(shapelet)을 학습하고, 각 입력 시계열과 모든 shapelet 사이의 최소거리(혹은 존재 확률)을 계산해 특징 벡터로 만든 뒤 일반 분류기에 넣어 예측을 수행한다.
        - 장점 : 해석 가능하고, 사람이 이해할 수 있는 패턴 단위로 설명 가능함.
        - 단점 : 고정된 길이의 shapelet에 맞춰 최적화 되므로 데이터셋 특화가 되기 쉽고, 같은 동작이라도 성인/아동처럼 오프셋, 스케일, 지속시간이 다른 경우 여러 shapelet이 따로 필요하다.
- 기존의 방법 2
    - VQ-VAE 토크나이저 계열 (TOTEM)
        - 방식 : 원시 시계열을 CNN 등 인코더로 latent space(token, code)로 바꾼 뒤, VQ(벡터 양자화)로 이산 코드북을 만들어 토큰처럼 사용하여 self-supervised(masked, 재구성 등) 목표로 사전학습 한다.
        - 장점 : 다양한 도메인에 공용으로 쓸 수 있는 이산 코드북(discreted codebook)을 제공하고, 표현력이 높아 downstream task에서도 성능이 좋다.
        - 단점 : latent space가 잠재 벡터일 뿐 물리적/형상적 의미가 없어 사람이 해석하기에 어렵다. black box모델임.
- 이 논문의 목적
    - 목표 : 위 두 방식의 장점을 결합해 "해석 가능하면서도 데이터-불변성(전이성)과 표현력"을 모두 갖춘 토큰화를 제안함. 이를 위해 시계열 부분구간을 "추상화된 모양 코드(이산 코드북)" 과 "속성(offset μ, scale σ, 시작 지점 t, 길이 l)"로 분해하고, VQ로 학습한 공용 코드북을 통해 여러 도메인을 하나의 모양 어휘로 기술함.
    - 효과 : 성인/아동 처럼 길이, 스케일, 오프셋이 달라도 "같은 모양 코드(코드북) + 다른 속성"으로 설명이 가능해 interpretable과 generalization을 동시에 확보함.

### Background
- 