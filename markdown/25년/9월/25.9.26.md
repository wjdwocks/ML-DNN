## 해야할 것.
- 논문쓰기
    - Main Table, WRN 다른조합쪽 Experiment쪽 작성목표
    - GENEActiv 2Teacher들 MTKD_RL 추가실험해보기. (14cls, 7cls 모두 완)
- VQShape 논문 더 잘 이해해보기.
    - Codebook에 대해서 더 자세히
    - 전체적인 Process에 대해서 더 자세히 공부.
    - 한번 코드를 돌려보고싶기는 함. (주말에 시간이 된다면)

## VQShape 더 공부.
### Codebook에 대한 더 깊은 고찰
- CodeBook은 그냥 (512, 8)의 파라미터라고 보면 됨.
- 학습되고, Prediction에 이용되는 512개의 Concept이고, 각 차원이 8임.
- 이 Codebook은 두 가지 Loss에 의해서 학습이됨 (학습 도중 업데이트)
    1. L_vq : loss = torch.mean((z_q.detach() - z)^2) + self.commit_loss * torch.mean((z_q - z.detach())^2)
        - Token의 z_e(shape 표현)이 z_q와 비슷해지도록 하는 항.
        - Code book의 z_q가 Token의 Shape 표현과 비슷해지도록 하는 항의 합.
        - mean을 하는 이유는 8차원 각각에 대해서 다 비슷해지도록 하기 위해서.
    2. L_s : s_loss = nn.functional.mse_loss(s_hat, s.detach())
        - s_hat : 치환된 Codebook Code z_q의 Shape Decoder 표현에 Offset, Scale을 적용한 것.
        - s : Start Time, Length로 원본 시계열에서 얻어온 Shape을 Interpolation한 것.

### 어째서 Slot 기반 논문들과 비슷한가?
1. 학습 및 예측 과정에서 판단의 근거가 되는 학습 가능한 Slot/Code 로서 구현됨. (공통점),
    - Codebook이라고 하는것은 결국 nn.parameter를 통해 (512, 8)로 구현이 됨.
    - 이것은 512개의 Knowledge Slot이 있고 각각이 8차원으로 표현된다고 볼 수 있음.
    - 지금까지 본 논문들의 Slot 업데이트 및 활용 방식 (CCT, mGLW 등)
        - Embedding Feature를 Slot이 Slot Attention하여 업데이트함. (GRU사용)
        - 최종적으로 Prediction을 수행하는 Model의 Embedding Feature를 Slot에 대한 표현으로 Cross Attention하여, Slot기반의 표현으로 변환한 후 예측에 사용(분류기 등)
    - 내가 이해한 Codebook의 활용 방식
        - L_vq (z_q와 z_e가 서로 비슷해지기 위한 Loss)와, L_s (z_q를 Shape Decoder에 넣은 뒤, Attributes들로 복원해서 원본 시계열과 비교하는 Loss)를 통해 학습 중 업데이트됨.
        - 최종적으로 Prediction을 수행하기 위해 Input을 Attribute Decoder를 통해 (z_e, sigma, mu, t, l)로 분해함.
            1. Tokens 방식 : (z_q, sigma, mu, t, l)을 Classifier에 넣어서 최종 분류.
            2. Code Histogram 방식 : 얻어진 z_e를 가장 가까운 z_q로 치환한 후, 어떤 code(z_q)가 얼마나 선택되었는지의 빈도 vector 표현을 Classifier에 넣어서 최종 분류.
    - 즉, 각 Code에 포함되어 있는 z_q들은 Task Prediction에 도움이되는 정보이자, 사람이 이해할 수 있게 표현될 수 있다는 점에서 비슷함.
    - 최종 Prediction을 위해 Input Features를 Slot에 대한 표현(Code Histogram)으로 바꾼 뒤, 최종 예측을 수행하는 것이 비슷함.

2. 해석 가능하도록 학습된다는 점. (공통점)
    - 각 Slot 및 각 Code들은 여러 샘플을 거쳐서 같은 의미를 가지도록 학습이 된다.
        - 샘플의 어떤 Feature 표현이 있을 때 Slot은 Attention을 통해서, 이 Feature를 잘 설명할 수 있는 Slot이 더 이 Feature를 잘 설명할 수 있도록 학습됨.
        - Codebook은 Feature 표현(z_e)과 각 code와의 거리 계산을 통해서 이 Feature와 가장 닮아있는 Code(z_q)로 치환되고 L_vq를 통해 (z_e, z_q)가 서로 더욱 비슷해지도록 학습된다.
    - 그러면 "해석 가능하다" 라는 것은 샘플의 이 Feature가 이 Concept과 유사하기 때문에 이렇게 예측을 수행했다. 라는 것을 의미한다는건가? (잘 모르겠음.)

3. 각 Slot/Code가 의미하는 Concept이 서로 다름. (다른점)
    - Slot Attention : 각 Slot이 Feature의 정보를 토대로 업데이트할 때 Softmax를 Slot방향으로 해서 각 Slot이 경쟁적으로 Feature 정보를 가져가도록 함. 
        - 이런 방식으로 각 Slot이 서로다른 정보를 "알아서" 학습할 수 있게 함.
    - Codebook : Codebook 학습에서는 사실 각 Code들이 서로 알아서 다른 정보를 찾아가게 하는 방법은 없다. 
        - 그래서, 논문에서 512개를 Codebook이 가지고 있게 했지만, 비슷한 것을 제외한 64개의 Code만 보여줌.
        - 반대로, Attribute Decoder로 얻어진 z_e가 하나의 z_q에만 가까운 거리를 유지하도록 하는 Entropy Term은 존재함.


### 각 Loss에 대해서 더 자세히 파고들기.
- L_vq : Vector Quantization Loss
    - VQ-VAE에 있는 Objective 를 가져왔다고 함. quantization term과 entropy term으로 나뉘어진다.
    - quantization term : z_e와 선택된 z_q가 서로 비슷해지도록 하는 항. |z_e - z_q|^2 + |z_q - z_e|^2 과 비슷. gradient 문제때문에 이렇게 구현됨.
    - entropy term : z_e가 codebook 내의 모든 code들 중 하나의 code와만 가까이 유지하고, 나머지와는 멀어지게 하는 목적의 항.
- L_recon : Subsequence 및, 전체 Time-series의 재구성 Loss.
    - L_s : Attribute Decoder를 통해 얻어진 (z_e, mu, sigma, t, l)로 얻어지는 항. z_e는 z_q로 치환되고, Shape Decoder에 z_q를 넣어 time-series를 얻은 뒤, sigma와 mu를 이용하여 크기를 조정한 s_k와 원본 시계열 x의 t, l위치의 subsequence를 가져온 뒤, s_k와 크기를 맞추기 위해 interpolation한 target(s_k)를 비교하는 항.
    - L_x : tau^_k 를 이용해서 복원한 x^ 와 원본 시계열 x를 비교하는 항.
    - 이 항들을 통해서 각 모듈들의 기본적인 학습이 이루어진다.
- L_div : Token들이 Attribute Decoder를 통해 예측한 start time, length가 서로 다른 구간을 담당하도록 강제하는 항.
    - 수식이 어려워서 자세한 내용은 물어보는게 나을듯 함.


### 만약 VQShape이 아키텍처에 다른 Modality(Text or Img)를 넣는다면? 어떻게 구현해볼 수 있을지? (코드를 보면서 생각해보자.)
- 그런 데이터셋이 있는지? → Signal과 Txt or Image 가 같이 매핑이 되어있는 데이터셋이 있는가?
    - 저번에 본 CTPD데이터셋에서 Time-series와 Clinical Note이 같이 있었다는거로 기억함. (MIMIC-III 데이터셋)
- Cross Attention으로 살짝의 Guidance만 넣어줄 수도 있다고 함. (Language Semantic Augmentation 같은 이름의 논문?)



### 아레 3가지의 방법을 더 깊이 생각해보는 것을 목표로 주말에 공부.
0. 어떻게 Text Guidance를 넣어줄 지 결국 Text를 어떻게 만들지와 어떻게 Guidance를 줄지 이 둘이 문제임
    - 결국 이 VQShape 방법론에서 중요한 것은 codebook을 잘 업데이트 하는거라고 생각함.
    - codebook을 업데이트할 때 Text Guidance를 포함하게 업데이트하면 더 좋을 것임.
    - 그러면 결국 Attribute Decoder로 얻어진 z(subsequence shape)에 Text Guidance가 들어가야 한다
        - 이 z와 가장 가까운 codebook code로 치환할거고, L_vq로 이 둘이 더 비슷해지도록 업데이트 될거라서.
    - 첫 번째) Attribute Decoder중 z를 얻는 MLP의 입력만 text를 cross attention으로 첨가해준다.
        - 이렇게 하면 z와 매핑될 codebook z^도 이와 맞춰서 업데이트 될것임.
    - 두 번째) Attribute Decoder 중 z, sigma, mu의 MLP 입력에 text를 cross attention 해준다.
        - 혹시 offset, scale정보도 text로 얻어질 수 있는 상황이라면? 더 좋을듯.

1. Text - Time-series가 매핑되어있는 HAR 데이터셋이 있다는 가정. (CTPD 논문에서 봤던 Medical 데이터(MIMIC-III)같은거 (time-series + medical note))
    - 이미 의미적으로 Text와 time-series가 정렬되어있다는 의미.
        1. Text의 정보를 잘 고도화 해서, Vector형태로 변환.
        2. Attribute Decoder의 z MLP, mu, sigma MLP 들의 입력으로 들어갈 Time-series Token에 Cross Attention을 수행해서 Text 정보를 첨가한다.
            - q : TS, k/v : Text
        3. Shape z가 Text를 반영한 표현이 될 것이고, 가장 가까운 codebook code z^도 L_vq에 의해서 Text 정보를 반영하도록 업데이트될 것이다.
        4. 근데, mu, sigma를 통해서 subsequence recon loss를 얻고, 이것도 Codebook을 업데이트시키기 때문에, 얘네도 text를 포함해주면 좋을수도 있을 것 같다.

        - 아레의 방법과 동일하긴 하지만, 샘플별로 다른 Text가 존재하기 때문에, 개인차 문제를 완화해줄 수 있을 것임.


2. 텍스트 Guidance가 없더라도, 각 Label에 대한 간단한 Text를 만들거나(Label을 이용한 Rule-based Text), LLM을 통해 자동으로 만든다
    - 방법1 : LLM 이용 
        1. Time-series를 입력으로 받으면 그에 대한 Text를 출력해주는 LLM에 넣어서 train sample을 모두 다 넣는다.
        2. 각 Text를 text encoder를 통해서 Time-series의 Token 차원과 맞춰줌. (B, d_token)
        3. 이 텍스트는 Sample하나에 대한 표현이기 때문에, 모든 Token에 같은 Text 표현으로 복사해준다. (B, token_num, d_token)
        4. 이렇게 되면 각 Sample마다 Text는 전역적인 표현을 잡아주고, Time-series는 Token마다 분해된 지역적인 표현을 잡아주게 될거라고 생각함.
        5. Time-series token을 Query, Text를 Key/Value로 하여 Text표현을 Time-series에 첨가해준다.
        6. 그러면 codebook에도 text에 대한 정보가 포함되도록 학습이 될 테고, inference시에 time-series만 넣어도 text의 표현도 같이 사용하는 느낌이 됨.
    - 방법1 문제점(안될 것 같은 이유)
        1. Time-series를 입력으로 받고 Text를 출력해주는 LLM이 없음. (Clip처럼 img(sig) - text 가 정렬되어있는게 없다.)
        2. Inference 시에는 Time-series만 받게 하고 싶었는데, 결국 Attribute Decoder의 입력으로 들어갈 Token이 Time-series + Text 였는데, 이것부터 달라져버리면 Codebook 정보랑 align이 안되서 오히려 망칠 수도 있을듯.

    - 방법2 : Label마다 Rule-based로 Text를 만들어서 사용.
        1. 각 Label마다 Text를 임의로 생성한다. ex) Label 1 : "walking on treadmills (3mph)"
        2. 그 Text를 Encoder에 넣어서 Time-series의 Token 차원과 일치시킴. (B, d_token)
        3. 이 또한, Sample 전체에 대한 표현이기 때문에 Token마다 동일하게 복사. (B, token_num, d_token)
        4. Time-series token을 Query, Text를 Key/Value로 하여 Text표현을 Time-series에 첨가해준다.
        5. 합쳐진 Token을 Attribute Decoder에 넣고, 이후는 동일하게 동작.
    - 방법2 문제점(안될 것 같은 이유)
        1. Label마다 동일한 Text를 사용한다? → One-hot Embedding이랑 다를게 없음. 
            - 원하는 것은 같은 label의 다른 샘플이 동일하게 인식되게 하고 싶은데, 같은 label의 모든 샘플에 동일한 text guidance가 첨가되기 때문에 큰 효력이 없을 것 같다.
            - 성인과 아이가 동일한 label을 수행했을 때, 진폭이나, 길이 등이 다를텐데, 이걸 동일하게 인식하도록 text가 guidance를 주고 싶었는데 모든 label에 동일하게 줘버리면 큰 의미가 없을 것 같다.
        2. 이것도 결국 Attribute Decoder에 넣어지는건 두 정보가 혼합된 Token이라서, 그에 맞춰서 Codebook이 업데이트될텐데, Student는 Time-series만 입력으로 받는다면, Inference 시에 오히려 혼동이 올 수도 있음.
            - Student는 Label을 예측하는게 목표니까 그냥 time-series만 넣을텐데, 그러면 Attribute Decoder에 넣어지는 표현자체가 달라지는거라서 학습에 오히려 방해가 됨.
            - 그러면 CrossAttention을 하는 위치를 바꿔야하나? → Attribute Decoder 이후에 얻어진 z(subsequence shape)에 text표현을 첨가한다면?
                - → Codebook에 포함된 code들은 각각이 어떤 label에 포함되는 것을 의미하는게 아니라, 이 code들의 빈도수/조합이 어떻게 되는지로 Label이 결정되는 것이기 때문에 큰 의미가 없을 것 같다.
            - time-series만으로 얻어진 z와 time-series + text로 얻어진 z가 비슷해지도록 하는 loss를 또 하나 추가해서, codebook을 text정보를 포함해서 업데이트해도, Inference시에 time-series만 받아도 text정보의 이점을 살릴 수 있게 해본다? → 되는지 모르겠음.

    - 방법3 : Label을 통해 Text를 만들고, Prediction에서 time-series만 사용할 수 있게 KD를 사용.
        - 방법2 에서 codebook을 만들 때에는 Text를 첨가한 token을 사용하는데, Prediction 과정에서는 time-series token만 사용해서 오히려 악영향이 있을 수 있다고 생각했음.
        - 그래서, KD방식을 사용할 수 있을 것 같음.
            1. Teacher는 Time-series를 똑같이 사용하고, Attribute Decoder의 z를 얻을 때 Text를 첨가해서 Codebook이 Text guidance를 포함하도록 학습함.
            2. Student는 Attribute Decoder의 입력으로 Time-series만 사용하도록 하고, Codebook은 Teacher가 학습한 그대로 freeze 해놓는다.
            3. Time-series만으로 학습을 진행하고, Attribute Decoder z가 똑같이 L_vq, L_x로 업데이트되면서 여기에 L_KD를 추가함. (Teacher의 z생성을 따라가도록.)
            4. 그러면 Text정보가 포함된 codebook을 사용하면서, Student는 Time-series만 입력으로 받아서 Prediction이 가능함. 


### 논문의 뒷부분(Experiments와 그 뒤도) 자세히 다시 읽어보자.
- 생각보다 무슨 내용이 있거나 하진 않았다.