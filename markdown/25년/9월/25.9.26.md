## 해야할 것.
- 논문쓰기
    - Main Table, WRN 다른조합쪽 Experiment쪽 작성목표
    - GENEActiv 2Teacher들 MTKD_RL 추가실험해보기. (14cls, 7cls 모두 완)
- VQShape 논문 더 잘 이해해보기.
    - Codebook에 대해서 더 자세히
    - 전체적인 Process에 대해서 더 자세히 공부.
    - 한번 코드를 돌려보고싶기는 함. (주말에 시간이 된다면)

## VQShape 더 공부.
### Codebook에 대한 더 깊은 고찰
- CodeBook은 그냥 (512, 8)의 파라미터라고 보면 됨.
- 학습되고, Prediction에 이용되는 512개의 Concept이고, 각 차원이 8임.
- 이 Codebook은 두 가지 Loss에 의해서 학습이됨 (학습 도중 업데이트)
    1. L_vq : loss = torch.mean((z_q.detach() - z)^2) + self.commit_loss * torch.mean((z_q - z.detach())^2)
        - Token의 z_e(shape 표현)이 z_q와 비슷해지도록 하는 항.
        - Code book의 z_q가 Token의 Shape 표현과 비슷해지도록 하는 항의 합.
        - mean을 하는 이유는 8차원 각각에 대해서 다 비슷해지도록 하기 위해서.
    2. L_s : s_loss = nn.functional.mse_loss(s_hat, s.detach())
        - s_hat : 치환된 Codebook Code z_q의 Shape Decoder 표현에 Offset, Scale을 적용한 것.
        - s : Start Time, Length로 원본 시계열에서 얻어온 Shape을 Interpolation한 것.

### 어째서 Slot 기반 논문들과 비슷한가?
1. 학습 및 예측 과정에서 판단의 근거가 되는 학습 가능한 Slot/Code 로서 구현됨. (공통점),
    - Codebook이라고 하는것은 결국 nn.parameter를 통해 (512, 8)로 구현이 됨.
    - 이것은 512개의 Knowledge Slot이 있고 각각이 8차원으로 표현된다고 볼 수 있음.
    - 지금까지 본 논문들의 Slot 업데이트 및 활용 방식 (CCT, mGLW 등)
        - Embedding Feature를 Slot이 Slot Attention하여 업데이트함. (GRU사용)
        - 최종적으로 Prediction을 수행하는 Model의 Embedding Feature를 Slot에 대한 표현으로 Cross Attention하여, Slot기반의 표현으로 변환한 후 예측에 사용(분류기 등)
    - 내가 이해한 Codebook의 활용 방식
        - L_vq (z_q와 z_e가 서로 비슷해지기 위한 Loss)와, L_s (z_q를 Shape Decoder에 넣은 뒤, Attributes들로 복원해서 원본 시계열과 비교하는 Loss)를 통해 학습 중 업데이트됨.
        - 최종적으로 Prediction을 수행하기 위해 Input을 Attribute Decoder를 통해 (z_e, sigma, mu, t, l)로 분해함.
            1. Tokens 방식 : (z_q, sigma, mu, t, l)을 Classifier에 넣어서 최종 분류.
            2. Code Histogram 방식 : 얻어진 z_e를 가장 가까운 z_q로 치환한 후, 어떤 code(z_q)가 얼마나 선택되었는지의 빈도 vector 표현을 Classifier에 넣어서 최종 분류.
    - 즉, 각 Code에 포함되어 있는 z_q들은 Task Prediction에 도움이되는 정보이자, 사람이 이해할 수 있게 표현될 수 있다는 점에서 비슷함.
    - 최종 Prediction을 위해 Input Features를 Slot에 대한 표현(Code Histogram)으로 바꾼 뒤, 최종 예측을 수행하는 것이 비슷함.

2. 해석 가능하도록 학습된다는 점. (공통점)
    - 각 Slot 및 각 Code들은 여러 샘플을 거쳐서 같은 의미를 가지도록 학습이 된다.
        - 샘플의 어떤 Feature 표현이 있을 때 Slot은 Attention을 통해서, 이 Feature를 잘 설명할 수 있는 Slot이 더 이 Feature를 잘 설명할 수 있도록 학습됨.
        - Codebook은 Feature 표현(z_e)과 각 code와의 거리 계산을 통해서 이 Feature와 가장 닮아있는 Code(z_q)로 치환되고 L_vq를 통해 (z_e, z_q)가 서로 더욱 비슷해지도록 학습된다.
    - 그러면 "해석 가능하다" 라는 것은 샘플의 이 Feature가 이 Concept과 유사하기 때문에 이렇게 예측을 수행했다. 라는 것을 의미한다는건가? (잘 모르겠음.)

3. 각 Slot/Code가 의미하는 Concept이 서로 다름. (다른점)
    - Slot Attention : 각 Slot이 Feature의 정보를 토대로 업데이트할 때 Softmax를 Slot방향으로 해서 각 Slot이 경쟁적으로 Feature 정보를 가져가도록 함. 
        - 이런 방식으로 각 Slot이 서로다른 정보를 "알아서" 학습할 수 있게 함.
    - Codebook : Codebook 학습에서는 사실 각 Code들이 서로 알아서 다른 정보를 찾아가게 하는 방법은 없다. 
        - 그래서, 논문에서 512개를 Codebook이 가지고 있게 했지만, 비슷한 것을 제외한 64개의 Code만 보여줌.
        - 반대로, Attribute Decoder로 얻어진 z_e가 하나의 z_q에만 가까운 거리를 유지하도록 하는 Entropy Term은 존재함.


### 각 Loss에 대해서 더 자세히 파고들기.
- L_vq : Vector Quantization Loss
    - VQ-VAE에 있는 Objective 를 가져왔다고 함. quantization term과 entropy term으로 나뉘어진다.
    - quantization term : z_e와 선택된 z_q가 서로 비슷해지도록 하는 항. |z_e - z_q|^2 + |z_q - z_e|^2 과 비슷. gradient 문제때문에 이렇게 구현됨.
    - entropy term : z_e가 codebook 내의 모든 code들 중 하나의 code와만 가까이 유지하고, 나머지와는 멀어지게 하는 목적의 항.
- L_recon : Subsequence 및, 전체 Time-series의 재구성 Loss.
    - L_s : Attribute Decoder를 통해 얻어진 (z_e, mu, sigma, t, l)로 얻어지는 항. z_e는 z_q로 치환되고, Shape Decoder에 z_q를 넣어 time-series를 얻은 뒤, sigma와 mu를 이용하여 크기를 조정한 s_k와 원본 시계열 x의 t, l위치의 subsequence를 가져온 뒤, s_k와 크기를 맞추기 위해 interpolation한 target(s_k)를 비교하는 항.
    - L_x : tau^_k 를 이용해서 복원한 x^ 와 원본 시계열 x를 비교하는 항.
    - 이 항들을 통해서 각 모듈들의 기본적인 학습이 이루어진다.
- L_div : Token들이 Attribute Decoder를 통해 예측한 start time, length가 서로 다른 구간을 담당하도록 강제하는 항.
    - 


### 만약 VQShape 이 아키텍처에 다른 Modality(Text or Img)를 넣는다면? 어떻게 구현해볼 수 있을지? (코드를 보면서 생각해보자.)
- 그런 데이터셋이 있는지? → Signal과 Txt or Image 가 같이 매핑이 되어있는 데이터셋이 있는가?
- Cross Attention으로 살짝의 Guidance만 넣어줄 수도 있음. (Language Semantic Augmentation 같은 이름의 논문?)



### 아레 3가지의 방법을 더 깊이 생각해보는 것을 목표로 주말에 공부.
- Token들을 Attribute Decoder(더 앞에서부터 해도 될지도)에 넣을 때 Text Guidance를 넣어줘서 L_vq를 통해서 각 Text 정보가 Codebook Code에도 반영될 수 있도록.
    - 근데, Cross Attention으로 어떻게 넣어줄 수 있을까? (간단한 방식이지만 자세하게 생각해가면 좋을듯.)


- 다른 방법으로는, Time-series와 Text가 매핑되어있는 HAR 데이터셋이라는 가정 하에, Attribute Decoder 앞까지 (차원 맞춰주기 + 정보 고도화)를 수행한 뒤, 동일한 하나의 Attribute Decoder에 넣어서 같은 표현으로 만든 뒤에, 이를 합쳐서(그냥 더한다거나, 평균을 낸다거나? 다른 좋은 방법이 있을지도.) 사용하는 방법.


- 텍스트 Guidance가 없더라도, 각 Label에 대한 간단한 Text를 만들거나, LLM을 통해 자동으로 만든다? 

### 논문의 뒷부분(Experiments와 그 뒤도) 자세히 다시 읽어보자.
- 생각보다 무슨 내용이 있거나 하진 않았다.