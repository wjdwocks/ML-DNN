## 해야할 것.
- 논문쓰기
    - Main Table쪽 Experiment쪽 작성목표
    - GENEActiv 2Teacher들 MTKD_RL 추가실험해보기. (14cls, 7cls 모두)
- VQShape 논문 더 잘 이해해보기.
    - Codebook에 대해서 더 자세히
    - 전체적인 Process에 대해서 더 자세히 공부.
    - 한번 코드를 돌려보고싶기는 함. (주말에 시간이 된다면)

## 25.9.22에 한 이야기들.
### 집중해서 봐야할 것.
- How) 어떻게 slot마다 다 다르게 알아서 맞춰서 가게 될까?
    - → K(Slot)에 대해서 Softmax를 해서 서로 경쟁적으로 Feature Embedding을 가져가기 때문에, 하나의 Time-series의 Feature Embedding에서, 각 Slot이 서로 다른 부분에 집중해서 Slot에 반영하게 됨.(Slot Attention)

- VQShape의 어느 부분에서 MultiModal을 넣을 때 모듈화할 수 있을지.??
    - 코드에서 Codebook이 어떻게 활용되고, 얘가 뭔지?
    - 3.1 Shape-level representation에서, Image(Text)를 어떤 Attribute tuple로 나눌 수 있을지, 어떻게 Time-series와 Alignment할 수 있을지? Cross Attention?
    - 그래서 어떻게 Objective Loss를 설계할 수 있을지?
    - Attribute를 어떻게 할건데? 

    - Code Book에 Text Modality를 포함할 수 있을까?
    - 그렇다면 어떻게 Attribute를 쪼갤 수 있을까?

### 내 생각
- 똑같이 Embedding을 통해서 Alignment할 순 없을까?
- 둘 이상의 모달리티(Sig + Txt or Image)를 통해서 Signal의 Interpretability를 더 늘릴 수 있을까? 를 VQShape에 적용할 수 있을만한 방법론까지.



## VQShape 더 공부.
### Codebook에 대한 더 깊은 고찰
- CodeBook은 그냥 (512, 8)의 파라미터라고 보면 됨.
- 학습되고, Prediction에 이용되는 512개의 Concept이고, 각 차원이 8임.
- 이 Codebook은 두 가지 Loss에 의해서 학습이됨 (학습 도중 업데이트)
    1. L_vq : loss = torch.mean((z_q.detach() - z)^2) + self.commit_loss * torch.mean((z_q - z.detach())^2)
        - Token의 z_e(shape 표현)이 z_q와 비슷해지도록 하는 항.
        - Code book의 z_q가 Token의 Shape 표현과 비슷해지도록 하는 항의 합.
        - mean을 하는 이유는 8차원 각각에 대해서 다 비슷해지도록 하기 위해서.
    2. L_s : s_loss = nn.functional.mse_loss(s_hat, s.detach())
        - s_hat : 치환된 Codebook Code z_q의 Shape Decoder 표현에 Offset, Scale을 적용한 것.
        - s : Start Time, Length로 원본 시계열에서 얻어온 Shape을 Interpolation한 것.

### 어째서 Slot 기반 논문들과 비슷한가?
- Codebook이라고 하는것은 결국 nn.parameter를 통해 (512, 8)로 구현이 됨.
- 이것은 512개의 Knowledge Slot이 있고 각각이 8차원으로 표현된다고 볼 수 있음.
- 지금까지 본 논문들의 Slot 업데이트 및 활용 방식 (CCT, mGLW 등)
    - Embedding Feature를 Slot이 Slot Attention하여 업데이트함. (GRU사용)
    - 최종적으로 Prediction을 수행하는 Model의 Embedding Feature를 Slot에 대한 표현으로 Cross Attention하여, Slot기반의 표현으로 변환한 후 예측에 사용(분류기 등)
- 내가 이해한 Codebook의 활용 방식
    - L_vq (z_q와 z_e가 서로 비슷해지기 위한 Loss)와, L_s (z_q를 Shape Decoder에 넣은 뒤, Attributes들로 복원해서 원본 시계열과 비교하는 Loss)를 통해 학습 중 업데이트됨.
    - 최종적으로 Prediction을 수행하기 위해 Input을 Attribute Decoder를 통해 (z_e, sigma, mu, t, l)로 분해함.
        1. Tokens 방식 : (z_q, sigma, mu, t, l)을 Classifier에 넣어서 최종 분류.
        2. Code Histogram 방식 : 얻어진 z_e를 가장 가까운 z_q로 치환한 후, 어떤 code(z_q)가 얼마나 선택되었는지의 빈도 vector 표현을 Classifier에 넣어서 최종 분류.
- 즉, 각 Code에 포함되어 있는 z_q들은 Task Prediction에 도움이되는 정보이자, 사람이 이해할 수 있게 표현될 수 있다는 점에서 비슷함.
- 최종 Prediction을 위해 Input Features를 Slot에 대한 표현(Code Histogram)으로 바꾼 뒤, 최종 예측을 수행하는 것이 비슷함.

### 각 Loss에 대해서 더 자세히 파고들기.
- L_vq
- L_s
- L_div
- L_x

### 만약 VQShape 이 아키텍처에 다른 Modality(Text or Img)를 넣는다면? 어떻게 구현해볼 수 있을지?
- 

### 논문의 뒷부분(Experiments와 그 뒤도) 자세히 다시 읽어보자.