## 할 일.
- Abstracted Shapes as Tokens- A Generalizable and Interpretable Model for Time-series Classification 논문 자세히 읽어보기. + 코드 살펴보기.
- Improved Knowledge Distillation Based on Global Latent Workspace with Multimodal Knowledge Fusion for Understanding Topological Guidance on Wearable Sensor Data 논문 자세히 읽어보고, 코드 살펴보기.

## mGLW논문 
### 방법론 위주로 논문을 읽어보자.
- III-B(1) : Preprocessing
    - 각 Teacher 1, Teacher 2의 마지막 Feature Vector를 사용해서 Slot을 업데이트해야하는데, 둘의 Shape이 다르면 안된다.
    - 또한 Student도 이 업데이트된 Slot을 이용해서 표현을 바꿀건데, 이것의 Shape이 다르면, 전체적으로 같은 Knowledge가 Alignment되는게 아니다.
    - 그래서, Teacher1, Teacher2, Student의 Feature Vector를 가장 작은 것의 크기로 맞춰준다.

- III-B(2) : Slot Attention을 통한 Slot 업데이트
    - Query : Slot (K x D), K는 Slot개수, D는 embedding shape
    - Key/Value : Token (L x D), L은 Length, D는 동일
        - 물론, 각 Query, Key, Value마다 특정 가중치(선형 layer)를 통해 더 고도화된 것으로 변환됨.
    - Slot Attention을 통해서 Query와 Key를 Attention하는데, Softmax의 축을 Slot 쪽에 걸어줌.
        - Q(ζ)·K(e).T → (K x D)·(D x L) → (K x L)
        - 즉, 각 Slot과 Token간의 score 형태로 나오게 됨.
        - 그런데, Softmax를 Slot 쪽에 걸어주기 때문에, 각 토큰마다, Slot의 점수 합 = 1이 된다.
        - 이것은 슬롯 간의 경쟁을 유도하고, 한 토큰에 대해 여러 Slot이 높은 Score를 가질 수 없게 함. → 각 토큰을 하나의 Slot에 특정시키기 위함.
        - Attention 자체는, Q와 K사이의 유사도 score를 만드는 것이 목적이고, 어느 축으로 Softmax를 하느냐에 따라 해석/역할이 달라진다.
            - Q축(Slot) : 각 Key가 어떤 Query에 배정될지 경쟁. → Slot Attention이 하는 것.
            - K축(Token) : 각 Query가 어떤 Key를 얼마나 볼지. → 대부분의 Attention이라고 하면 이거인데
    - 이렇게 한 번 Softmax된 점수들을 가지고, 다시 Token축에 대해서 정규화를 진행한다.
        - 이 때는 그냥 전체 합으로 나누어주는 방식을 사용. → A / sum(A)
        - 각 슬롯을 업데이트하기 위해서, 실제로 곱해질 가중치를 만드는 단계
        - 슬롯별 업데이트 크기 차이를 줄이고, 안정성 학습을 위함임.
    - 위와 같이 재정규화를 통해 얻어진 Weight와 Value를 곱해서 얻은 것을 u라고 하자.
        - 이 u는 실제로 Slot을 업데이트하기 위한 GRU의 입력으로 들어갈 것임.
        - 이 기존 State는 유지하고, Input을 T1으로부터 얻은 u1과, T2로부터 얻은 u2의 가중합으로 넣어준다.
        - 가중합 beta는 0.5를 사용한 듯 함.

- III-B(3) : Slot을 이용한 Prediction
    - 위의 업데이트된 Slot(ζ)과, Student의 Feature Embedding Es를 이용하여 Cross Attention 수행.
        - Q(Es)를 Query, K(ζ),V(ζ)를 Key/Value로 두고, Slot을 축으로 한 Softmax를 수행함.
        - Token마다, 각 Slot에 대해서 점수를 가지게 되고, 그 Slot의 점수의 합이 1임.
        - Weight는 Es(L x D), ζ(K x D)이기에, L x K의 Shape이 됨.
        - 이 가중치를 그대로, V(ζ)에 곱해서 (L x D)의 "Slot을 참고하여 문맥화된 Token표현"을 만든다.
    - 최종 Prediction
        - 위의 "Slot을 참고하여 문맥화된 Token"을 바로 Classifier에 넣는게 아니라, Residual Connection과 같은 것을 추가한다.
        - 기존의 Es (L x D)를 더해주는데, 둘의 Shape이 같기 때문에 그냥 단순히 더해준다.
        - 원소별 덧셈이므로 (L x D)형태 그대로.
        - 이렇게 나온 거를 그대로 쭉 진행함.
    - 즉, Student의 마지막 Feature Vector를 (기존 값 + Slot을 참고하여 문맥화된 표현)으로 바꿔줘서 마지막 FC layer에서 참고할 정보를 더 추가해준다는 느낌.

- 신기한 실험 (Fig.8)
    - loss설계 부분에서, KTS와 KIS들의 MSE를 줄이는 loss가 하나 들어갔었음.
    - 근데 이게, Student의 KTS가 Teacher들의 KIS1, KIS2와의 MSE를 줄이기 위함이고, 이로인해서 KD_loss비중을 정하는 coefficient의 최적화와 연관이 된다고 했었음.
    - 신기하게 Figure를 보면 실제로 coefficient가 점점 줄어들다가 epoch이 지날수록 다시 커짐. 
    - 그렇다는건 한쪽에 너무 치우치지 않고, 양쪽의 knowledge를 잘 흡수하고 있다는거니까 좋은것인듯.
    - <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/9월/25.9.19/gwt_exp.png" alt="results" width="700">


### 코드도 보면서, 그러한 방법론이 어떻게 구현되어있는지를 알아봐야함.
- gwt쪽을 제외하면 기존 코드와 거의 동일해서 gwt쪽을 주로 봄.
1. gwt model의 정의
    - Student나 Teacher를 제외하고도, Preprocessing, Attention, GRU 등을 수행하면서 학습해야 할 Embedding Layer나 Linear Layer 등등을 가지는 GWT Model이 있어야 함.
    ```python
    if "gwt" in args.student: # 이 GWT모델은 Concept Slot을 위한 모델이라고 생각하면 맞으려나 → Preprocessing, Slot 업데이트 및 Student와의 CA까지 다 이안에 있다.
        gwt_model = create_gwt_model(n_concepts=args.num_concepts,
                                        in_feature1=args.in_feature1,
                                        in_feature2=args.in_feature2,
                                        embedding_dim=args.in_feature2, # 왜 feature2? > 아마 img는 D차원이 16 x 16으로 고정, sig는 D차원이 125로 고정되기 때문에 더 작은 125가 항상 선택되도록 한듯.
                                            ## 여기서, 500window의 경우 256 vs 125이고, 1000window의 경우에도 256 vs 250이라, 무조건 feature2를 선택.
                                        num_heads=args.num_heads,
                                        num_iterations=args.num_iters,
                                        use_cuda=args.cuda,
                                        token_size1=args.token_size1,
                                        token_size2=args.token_size2,
                                            # 위의 embedding_dim은 window_length에만 영향을 받음(signal의 경우.)
                                            # 하지만, token_size는 WideResnet의 경우, width에 따라서만 달라짐.
                                            #                   - 1 : 16 → 32 → 64
                                            #                   - 3 : 48 → 96 → 192
                                        input_token_size=args.input_token_size)
        print('GWT generated.')
    else:
        gwt_model = None
    ```
    - 이 gwt model을 통해서, Proposed Method의 대부분의 일을 수행하게 된다.
    - Preprocessing, CSA, CA 등과 관련해서 Slot Update, KIS, KTS추출 및 Student Output 변환까지 모두 gwt를 통해서 이루어짐.
    ```python
        if self.have_teacher:
            if self.train_gwt:
                teacher_outputs2, _, _, feat_sig = self.teacher2(signals, True)
                teacher_outputs, _, _, feat_img = self.teacher(data, True)

                # print(feat_sig.shape) # [64, token_size, 125] # 여기서 Token_size는 Channel을 의미하는듯?
                # print(feat_img.shape) # [64, token_size, 16, 16]
                B, L, H, W = feat_img.shape
                feat_img = feat_img.reshape(B, L, -1)  # [64, token_size, 256]
                # print(f'feat_img.shape {feat_img.shape}')
                # print(f'feat_sig.shape {feat_sig.shape}')

                output = self.student.encode(signals) # [B, 3, 500] → [B, token_size, 125] # 얘도 위의 feat_img/sig 와 같은 형태로 변환.
                # print(f'feat_sig shape: {feat_sig.shape}')
                output, attn_feat_img, attn_feat_sig, attn_concept = self.gwt(output, feat_img, feat_sig) # 이 세개를 gwt에 넣어줌. 
                # 위에서 나온 output은 위의 output(attention3) + Slot에 대한 표현으로 변환된 att3(Residual Connection)로 얻어짐.
                # gwt는 Preprocess + Slot Attention + Cross Attention 모두를 다 수행해주는 모듈느낌.
                # output은 Decoder에 넣어줄거, attn_feat들은 KIS들, attn_concept은 KTS
                
                # attn_feat_img: [B, 5, 64]
                # attn_feat_sig: [B, 5, 64]
                output = self.student.decode(output) # 위의 3번째 feature + slot으로 바뀐 표현을 이후 네트워크에 흘려 얻은 logits임.
    ```
2. Student Model의 내부구조.
    - 지금까지 내가 사용했던 Student는 그냥 forward pass한번으로 입력을 받고, 바로 그에 상응하는 Prediction Probability를 출력함.
    - 하지만 논문이 제안하는 방법에 따르면 Student는 마지막 Stage의 Feature를 gwt_model을 통해서 (원본 표현 + Concept 기반 표현)으로 변환한 뒤 나머지 일을 수행해야 한다.
    - 그래서, 아레와 같이 Student 모델은 3번째 Stage의 Feature를 출력해주는 Encoder와, 변환된 Feature를 받아서 최종 확률분포
    ```python
    def forward(self, x, return_feat=False):
        x = self.conv1(x)  # [2, 16, 500] window_size가 500이라는 가정.
        attention1 = self.block1(x)  # [2, 16, 500]
        attention2 = self.block2(attention1)  # [2, 32, 250]
        attention3 = self.block3(attention2)  # [2, 64, 125]
        out = self.batch_norm(attention3)
        out = self.activation(out)
        out = self.avg_pool(out)
        out = out.view(-1, self.out_filters)

        logit = self.fc(out)
        if return_feat:
            return logit, attention1, attention2, attention3
        else:
            return logit

    def encode(self, x):
        x = self.conv1(x)  # [2, 16, 500]
        attention1 = self.block1(x)  # [2, 16, 500]
        attention2 = self.block2(attention1)  # [2, 32, 250]
        attention3 = self.block3(attention2)  # [2, 64, 125]

        return attention3

    def decode(self, x):
        out = self.batch_norm(x)
        out = self.activation(out)
        out = self.avg_pool(out)
        out = out.view(-1, self.out_filters)

        return self.fc(out)  # , attention1, attention2, attention3
    ```
3. gwt 모델의 내부 구조
    - 아레는 gwt모델의 생성자 부분인데, positional encoding을 하기 위한 pos_feat과, 사용할 각종 클래스의 객체를 생성하고, Preprocessing을 위한 차원 맞추기에 대한 코드가 있음.
    ```python
    class MultimodalGlobalWorkspace(nn.Module):
        def __init__(
                self,
                in_feature1=256,
                in_feature2=125,
                n_concepts=5,
                embedding_dim=768,
                num_heads=5,
                attention_dropout=0.1,
                num_iterations=3,
                mlp_hidden_size=100,
                token_size1=64,
                token_size2=64,
                input_token_size=64,
                *args,
                **kwargs,
        ):
            super().__init__()

            self.token_size1 = token_size1
            self.token_size2 = token_size2
            self.input_token_size = input_token_size
            self.token_size_min = token_size1 if token_size1 < token_size2 else token_size2     # token_size는 왠만해서 같음 WRN을 사용할 때에는.
                                                                                                # 그래서 보니 Heterogeneous Architecture에 대한 실험은 안보이는걸로 봐서 맞는듯
                                                                                                # 근데, 3 Stage의 구조만 유지할 수 있다면 Token_size는 아마 같을듯.
            self.embedding_dim = embedding_dim
            self.pos_feat1 = nn.Parameter(torch.zeros(1, 1, token_size1 * in_feature1), requires_grad=True)
            self.pos_feat2 = nn.Parameter(torch.zeros(1, 1, token_size2 * in_feature2), requires_grad=True)
            # 위의 nn.Parameter는 
            self.gwt = Multimodal_Slot_Attention(embedding_dim=embedding_dim,
                                                n_spatial_concepts=n_concepts,
                                                num_iterations=num_iterations,
                                                latent_dim=embedding_dim, # 125, 250중 하나 signal 모델의 D차원
                                                mlp_hidden_size=mlp_hidden_size,
                                                feature1_dim=in_feature1,
                                                feature2_dim=in_feature2)
            self.multimodal_concept_embedder = CrossAttentionEmbedding(
                dim=embedding_dim,
                num_heads=num_heads,
                attention_dropout=attention_dropout,
            )

            # ToDo: Empirical part here. Check here out thoroughly later.
            if token_size1 != token_size2: # 왜지? 둘이 다르면?? → 아마도 WideResnet에서만 실험을 했고, width는 1 or 3이기 때문에, 둘이 다르면 무조건 token_size는 1에 맞춰진다는 의미인듯.
                if token_size1 > token_size2:
                    self.proj_feat1 = nn.Sequential(
                        nn.Linear(token_size1, self.token_size_min),
                        nn.ReLU(),
                        nn.Linear(self.token_size_min, self.token_size_min)
                    )
                    self.ln_feat1 = nn.LayerNorm([self.token_size_min, in_feature1])
                    # No need for feat2
                    self.proj_feat2 = nn.Identity()
                    self.ln_feat2 = nn.Identity()
                else: # 이것도 마찬가지로, 둘이 같이 않다면, Architecture Difference실험 → 그래도 WRN은 동일.
                    self.proj_feat2 = nn.Sequential(
                        nn.Linear(token_size2, self.token_size_min),
                        nn.ReLU(),
                        nn.Linear(self.token_size_min, self.token_size_min)
                    )
                    self.ln_feat2 = nn.LayerNorm([self.token_size_min, in_feature2])
                    # No need for feat1
                    self.proj_feat1 = nn.Identity()
                    self.ln_feat1 = nn.Identity()
            else: # 둘이 같다면, 무조건 Student거(64) 따라가게 하기.
                if (self.token_size_min > self.input_token_size) or (self.input_token_size > self.token_size_min):
                    self.proj_feat1 = nn.Sequential(
                        nn.Linear(token_size1, self.input_token_size),
                        nn.ReLU(),
                        nn.Linear(self.input_token_size, self.input_token_size)
                    )
                    self.ln_feat1 = nn.LayerNorm([self.input_token_size, in_feature1])
                    self.proj_feat2 = nn.Sequential(
                        nn.Linear(token_size2, self.input_token_size),
                        nn.ReLU(),
                        nn.Linear(self.input_token_size, self.input_token_size)
                    )
                    self.ln_feat2 = nn.LayerNorm([self.input_token_size, in_feature2])
                else: # 여기는 이미 모두가 width가 1이라서 이미 맞춰져있는 경우.
                    self.proj_feat1 = nn.Identity()
                    self.ln_feat1 = nn.Identity()
                    self.proj_feat2 = nn.Identity()
                    self.ln_feat2 = nn.Identity()
    ```
    - 다음으로는, forward pass를 통해서 어떤 값을 받고, 어떤 것들을 반환하는지를 보자
    - gwt.forward()에서 받는 것은 각각 Student, Teacher1, Teacher2의 마지막 Stage의 Feature 이다.
    - 여기에서 각 Teacher의 Feature를 Positional Encoding → L(token_size)차원 맞추기 → gwt(Slot Attention Module)에 넣어 Knowledge Slot을 업데이트하고, 각 Teacher의 KIS도 같이 받아옴(나중에 loss계산할 때 쓸거임.)
    - 그리고, Student의 Feature와 Knowledge Slot을 CA모듈에 넣어줘서 Student의 Embedding(out)과 KTS를 얻어온다.
    - 여기서 논문에서 못본 것이 있는데, Cross Attention을 할 때 Multi-Head Attention을 사용한다는 것임.
        - Slot과 Student의 Embedding을 (Head, Head/D)로 나누어서 KTS를 각각 얻었기 때문에, 다시 Head차원으로 mean(1)을 해서 평균을 내줌.
    - 이렇게 gwt에서는 Student의 Decoder에 넣어줄 것과, loss 계산에서 사용할 KIS, KTS들을 다 받아온다.
    ```python
    def forward(self, x, feat_img, feat_sig):
        """

        """
        B, L, D1 = feat_img.shape # 왜 image의 Shape이 B, L, D1이지? → forward에 넣기 전에 Width, Height를 하나로 합쳤음. 즉 D1에는 (16 x 16)인 256이 한번에 들어간거 아마 flatten
        feat_img = feat_img + self.pos_feat1.view(-1, L, D1) # 아마 positional encoding
        feat_img = self.proj_feat1(feat_img.transpose(-1, -2)).transpose(-1, -2) # 여기서 L차원 동일하게 맞추기.
        feat_img = self.ln_feat1(feat_img)

        B, L, D2 = feat_sig.shape
        feat_sig = feat_sig + self.pos_feat2.view(-1, L, D2) # 아마 positional encoding
        feat_sig = self.proj_feat2(feat_sig.transpose(-1, -2)).transpose(-1, -2) # 여기서 L차원(token_size) 동일하게 맞춤.
        feat_sig = self.ln_feat2(feat_sig)
        concepts, attn1, attn2 = self.gwt(feat_img, feat_sig) # concepts은 업데이트된 knowledge slots, attn1, attn2는 각 Teacher 1, 2에 대한 KIS임. loss 계산할 때 쓰려고 가져온듯.
        out, concept_attn = self.multimodal_concept_embedder(x, concepts) # 얘는 CA모듈에서, student의 embedding x와 업데이트된 concpets을 넣어서 out(slot정보 흡수한 embedding)과, concept_attn(KTS)를 얻어옴.
        # 여기서도 KTS는 KIS와 loss계산하려고 가져온듯.
        concept_attn = concept_attn.mean(1)  # average over heads # 이게 (B, Head, L, K)의 형태로 얻어져서, Head차원에 대해서 평균을 내는거임.
        # x: [B, in_feature*in_feature * latent_dim]
        # attn: [B, in_feature*in_feature, n_concepts]
        # Todo: Skip connection here. Other operations can be defined.
        out = out + x # Residual Connection 해주는거. 
        # CA모듈에서 봤듯이 out또한, (B, L, D)로 x와 차원이 똑같이 나와서 그냥 더하면 더해짐.

        return out, attn1, attn2, concept_attn # out은 student의 stage3 뒷부분(Decoder)에 바로 넣을거, attn들은 KIS1, KIS2, concept_attn은 KTS
    ```
4. 그럼 gwt의 첫 번째인 Slot Attention
    - 우선 Multimodal_Slot_Attention 클래스로 와서 nn.Embedding을 통해서 (n_spatial_concepts, D)인 학습 가능한 파라미터 테이블을 생성한다 → 얘가 Knowledge Slot임.
    - 근데, 이 논문의 취지대로 Global Latent Workspace를 가져서 학습 전체 기간동안 공유되어야 하는데 여기서 nn.Embedding을 통해 [K, D]의 weight를 얻고, 전체 학습 기간동안 공유되며, GRU의 Hidden State 초기값으로 사용됨.
    - 그렇게 slot의 기존값(~~init.weight)와, Teacher1, Teacher2의 Feature Vector를 slot attention을 실제로 수행하는 모듈에 넣어서 concepts(업데이트된 slot), attn1(Teacher1의 KIS), attn2(Teacher2의 KIS)를 얻는다.
    ```python
    class Multimodal_Slot_Attention(nn.Module):
        def __init__(
                self,
                embedding_dim=768, # 125(500w) or 250(1000w)으로, signal 모델의 D차원에 맞게 나옴.
                latent_dim=768, # = embedding_dim
                n_spatial_concepts=10, # concept 개수
                num_iterations=3, # gru 반복횟수
                mlp_hidden_size=64,
                feature1_dim=256,
                feature2_dim=125,
                *args,
                **kwargs,
        ):
            super().__init__()

            self.embedding_dim = embedding_dim
            self.n_spatial_concepts = n_spatial_concepts
            self.spatial_concept_slots_init = nn.Embedding(self.n_spatial_concepts, latent_dim) # 각 Concept들도 D차원에 맞춰주기 위함인듯?
            # (n_spatial_concepts, D)인 학습 가능한 파라미터 테이블을 생성함.
            # 이 concept_slot은 이미 latent_dim으로 생성이 되어있고, 업데이트되고, Teacher들의 feature는 CSA모듈 들어가서 맞춰준 후, Slot Attention 할거임.
            # 또한 nn.Embedding은 내부에 .weights 로 숨겨진 내부 파라미터가 존재해서, 여기에서 concept_slot을 업데이트시키는 듯.
            
            nn.init.xavier_uniform_(self.spatial_concept_slots_init.weight)

            # Workspace
            self.multimodal_concept_slot_attention = MultimodalConceptQuerySlotAttention(num_iterations=num_iterations,
                                                                                        slot_size=latent_dim,
                                                                                        feature1_dim=feature1_dim,
                                                                                        feature2_dim=feature2_dim,
                                                                                        mlp_hidden_size=mlp_hidden_size)

        def forward(self, x1, x2, sigma=0):
            """
            x1: image feature embedding from teacher 1
            x2: signal feature embedding from teacher 2
            """
            # Step 1: Initialize concept params.
            mu = self.spatial_concept_slots_init.weight.expand(x1.size(0), -1, -1) # mu에 concept_slots의 weights를 할당함. 근데, x1.size(0) == batch_size만큼 복제해서 확장시킴.
            z = torch.randn_like(mu).type_as(x1)
            concept_slots_init = mu + z * sigma * mu.detach() # 여기서 concept_slots_init은 또다시 mu + ....의 표현이기 때문에 이 weights가 gradient로 흘러 들어올 수 있음.
            # 뒤의 mu에는 detach()되어있는건 이게 noise의 형태로 넣을거라 이 뒤의 mu에는 gradient를 흘리지 않기 위함.

            concepts, attn1, attn2 = self.multimodal_concept_slot_attention(x1, x2,
                                                                            concept_slots_init)  # [B, num_concepts, latent_dim]
            # 그래서 multimodal_concept_slot_attention()에 concept_slots_init을 넣어서 거기서 gradient 계산을 하게 되면, concept_slots의 weights에도 영향을 주게 됨.
            # 이 multimodal_concept_slot_attention에서는, 각 Teacher의 stage3 embedding 표현을 넣어주고, 기존 concept_slot을 넣어줘서 slot을 업데이트하는 CSA모듈의 역할.
            
            
            return concepts, attn1, attn2
    ```
    - 우리가 논문에서 봤을 때에는 L x D 차원 모두 같게 맞춰준다고 했는데 지금까지 보면 L(Token_Size)만 맞춰줬음.
    - 그래서 이 모듈에 들어와서 처음으로 하는 일은 D차원을 똑같이 맞춰주게 됨. 이 D는 T1, T2, Slot 모두 같아져야 함.
    - 그 다음에, 각 Teacher와 Slot을 Attention한 뒤, Slot 축에 Softmax를 걸어서 각 slot이 경쟁적으로 softmax를 하게 하고, 다시 Token축에 대해 정규화(그냥 다 더해서 나누기)를 해서 slot을 업데이트할 값(가중치)의 형태로 바꿔줌.
    - 그 가중치와 Value를 곱해서 각 Teacher마다의 GRU input값을 얻고, 여기서는 T1, T2에 대해 1:1비율로 더해서 GRU의 입력으로 넣어준다. → Slot Update 끝
    ```python
    class MultimodalConceptQuerySlotAttention(nn.Module):
        def __init__(
                self,
                num_iterations,
                slot_size, # 위에서 embedding_dim = latent_dim = 125(500w) or 250(1000w) 그거
                mlp_hidden_size,
                feature1_dim=256,
                feature2_dim=125,
                truncate='bi-level',
                epsilon=1e-8,
                drop_path=0.2,
        ):
            super().__init__()
            self.slot_size = slot_size # Slot_size이자, 각 Teacher의 D 차원 수.
            self.epsilon = epsilon
            self.truncate = truncate
            self.num_iterations = num_iterations # GRU 반복 횟수.

            self.norm_feature1 = nn.LayerNorm(feature1_dim)
            self.norm_feature2 = nn.LayerNorm(feature2_dim)
            self.norm_slots = nn.LayerNorm(slot_size)
            # self.norm_mlp = nn.LayerNorm(slot_size)

            # 밑의 얘네는 각 Key, Value, Query로 만들어주는 Linear면서 차원까지 맞춰주기 위함인듯.
            self.project_k1 = linear(feature1_dim, slot_size, bias=False)
            self.project_k2 = linear(feature2_dim, slot_size, bias=False)
            self.project_v1 = linear(feature1_dim, slot_size, bias=False)
            self.project_v2 = linear(feature2_dim, slot_size, bias=False)
            self.project_q = linear(slot_size, slot_size, bias=False)

            self.gru = gru_cell(slot_size, slot_size)
            # self.mlp = nn.Sequential(
            #     nn.Linear(slot_size, mlp_hidden_size),
            #     nn.ReLU(),
            #     nn.Linear(mlp_hidden_size, slot_size))

            # self.drop_path = DropPath(drop_path) if drop_path > 0 else nn.Identity()

        def forward(self, features1, features2, slots_init):
            """
            handl multimodal features.
            """
            # `feature1` has shape [batch_size, num_feature, inputs_size].
            features1 = self.norm_feature1(features1) # teacher 1은 img사용
            k1 = self.project_k1(features1)  # Shape: [B, num_features, slot_size]
            v1 = self.project_v1(features1)  # Shape: [B, num_features, slot_size]
            B, N1, D = v1.shape

            # `feature2` has shape [batch_size, num_feature, inputs_size].
            features2 = self.norm_feature2(features2) # teacher2는 sig사용
            k2 = self.project_k2(features2)  # Shape: [B, num_features, slot_size]
            v2 = self.project_v2(features2)  # Shape: [B, num_features, slot_size]
            _, N2, _ = v2.shape # 어차피 위에거랑 B, L, D 다 shape이 똑같음.

            slots = slots_init # 이전 Slot 가져옴.
            # Multiple rounds of attention.
            for i in range(self.num_iterations):
                if i == self.num_iterations - 1: # 마지막 iteration일 때에만 수행.
                    slots = slots.detach() + slots_init - slots_init.detach() # 마지막 Iteration에서는 더이상 Hidden State를 Gradient로 업데이트하지 않음.
                slots_prev = slots
                slots = self.norm_slots(slots)
                q = self.project_q(slots) # Query로 만듦.
                # Attention
                scale = D ** -0.5 # 이건 왜하는거지? → 값의 안정을 위해서, 논문 수식에도, Softmax할 때 이거로 나눠줌.
                # for feature1
                attn_logits1 = torch.einsum('bid,bjd->bij', q, k1) * scale # query x key1 해준 다음에, sqrt(D)로 나눠주고
                attn1 = F.softmax(attn_logits1, dim=1) # dim=1로 softmax → 이게 논문에서 봤던, 경쟁적인 slot attention을 위해 Slot축에 걸어준 것.
                # Weighted mean
                attn_sum1 = torch.sum(attn1, dim=-1, keepdim=True) + self.epsilon # 다시 slot을 업데이트하기 위해 각 slot마다 vector표현의 합 = 1로 바꿔주기 위함.
                attn_wm1 = attn1 / attn_sum1
                updates1 = torch.einsum('bij, bjd->bid', attn_wm1, v1) # 그 weight와 Value를 곱해줘서, GRU의 input으로 넣기 위한 각 Teacher의 값들.
                # for feature2
                attn_logits2 = torch.einsum('bid,bjd->bij', q, k2) * scale # 이 einsum이 뭔지 모르겠네
                attn2 = F.softmax(attn_logits2, dim=1)
                # Weighted mean
                attn_sum2 = torch.sum(attn2, dim=-1, keepdim=True) + self.epsilon
                attn_wm2 = attn2 / attn_sum2
                updates2 = torch.einsum('bij, bjd->bid', attn_wm2, v2)

                updates = (updates1.reshape(-1, D) + updates2.reshape(-1, D)) / 2. # 두 Teacher의 가중치는 1 : 1
                # 이게 뒤에 Loss계산할 때랑 헷갈렸었는데, Loss계산할 때에는 두 Teacher의 KIS와 Student의 KTS를 비교해서 KD_loss의 가중치를 결정하고, Slot Update에서는 그냥 1:1로 반영함.
                # Update slots
                slots = self.gru(
                    updates,
                    slots_prev.reshape(-1, D)
                )
                slots = slots.reshape(B, -1, D)  # 여기서, 각 Slot은 (10, 125)와 같이 (slot개수, embedding D)로 있던걸, (B, k, D)로 바꿔버리는거임. Batch만큼 복제.
                # Todo: additionallay introduced.
                # slots = slots + self.mlp(self.norm_mlp(slots))

            return slots, attn1, attn2
    ```
5. 업데이트된 Slot을 이용해서 Student표현 강화하기
    - 아레와 같이 Student의 Feature Embedding과, 업데이트된 Slot을 받아와서 각각을 Query, Key/Value로 변환한 후 Attention Weight를 계산.
    - 이 때, Head개수를 두고, dimension size를 나누어서 multi-head attention을 수행함.
    - 계산하는 것의 shape을 잘 따라가보면, Student Feature의 D차원과 Slot의 D차원을 Head개수로 나누고, 각각에 대해 Attention을 수행한 후 Head개수로 곱해줘서 Student의 Feature 표현은 차원이 다시 맞아짐.
    - 근데, KTS는 나누어진 Head개수 그대로 가지고 반환되어서 위에서 mean(1)을 통해 합쳐준거임.
    ```python
    class CrossAttentionEmbedding(nn.Module): # CA모듈, num_heads=8인것으로 보아, Multi-head Attention을 기본으로 함.
        def __init__(
                self, dim, num_heads=8, attention_dropout=0.1
        ):
            super().__init__()
            self.num_heads = num_heads
            head_dim = dim // self.num_heads # 125/250 차원인데, head개수 5개로 나누어서 각각 head_dim은 25/50으로 나뉘는듯.
            self.scale = head_dim ** -0.5

            self.q = nn.Linear(dim, dim, bias=False)
            self.kv = nn.Linear(dim, dim * 2, bias=False) # 한번에 key + value가 flatten된 것 처럼 만든듯?
            self.attn_drop = nn.Dropout(attention_dropout)

        def forward(self, x, y): # x는 student의 embedding, y는 concept
            B, Nx, C = x.shape
            By, Ny, Cy = y.shape

            assert C == Cy, "Feature size of x and y must be the same" # 틀리면 에러나야함. C == Cy == dim이기도 함.

            q = self.q(x).reshape(B, Nx, 1, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # student의 embedding이 query
            # (B, N, 1, Head, C//Head) → (1, B, Head, N, C//Head)가 됨. (permute 후)
            
            kv = ( # 여기서 첫번째거는 Key, 두번째거는 Value가 되도록 한듯, Concept이 Key/Value
                self.kv(y)
                .reshape(By, Ny, 2, self.num_heads, C // self.num_heads)
                .permute(2, 0, 3, 1, 4)
            )

            q = q[0]
            k, v = kv[0], kv[1]

            attn = (q @ k.transpose(-2, -1)) * self.scale # 또 scale이 그대로인거보면 여기가 Cross Attention으로 가중치 계산하는 부분. 아마도 KTS(softmax 전)
            # Transpose(-2, -1)하면, q : (B, Head, Nq, C//Head), k : (B, Head, C//Head, Nk)으로 마지막 두 축만 바뀜.
            # 이거를 @로 행렬곱을 해주면, (B, Head, Nq, Nk)가 됨. Nq는 Student의 token size, Nk는 Slot의 개수.
            attn = attn.softmax(dim=-1) # slot개수 축으로 softmax → 각 토큰마다 Slot가중치의 합이 1
            attn = self.attn_drop(attn) # dropout은 왜한거지?

            x = (attn @ v).transpose(1, 2).reshape(B, Nx, C) # 여기서 Cross Attention을 통해 student의 embedding을 Concept Slot에 관한 표현으로 변환.
            # (B, Head, Nq, Nk) @ (B, Head, Nk, C//Head) → (B, Head, Nq, C//Head).transpose(1,2) → (B, Nq, Head, C//Head).reshape(B, Nq, C)를하면 Head x C//Head가 되어서 C가 됨?? 그게 무슨의미인거지?

            return x, attn # 그 Slot정보를 흡수한 embedding과, KTS 반환
            # 그래서, x는 Cross Attention을 완성한 embedding이고, (B, L, D)의 형태 그대로라서, 기존 x와 그냥 더해짐.
            # 하지만 attn(KTS)는 (B, Head, L, K)라서, Head개수에 대해서 평균을 내줘야하는데, 이거는 이 함수에서 나온 뒤에 해줌.
    ```
6. loss 계산하기
    - 일단 각 Teacher의 KD_loss 비중을 정하기 위한 KTS와 KIS들의 mse를 얻어놓고, 이를 통해서 각 KD_loss의 비중을 정함 = coef_dynamic
    - loss_SL은 Student의 CrossEntropy loss
    - 그리고 loss_align이라는게 있는데, 이거는 KIS1 vs KTS와 KIS2 vs KTS가 서로 비슷해지도록 하는 항임. → 즉, Student의 KTS가 각 Teacher의 KIS의 중앙쪽(사이)에 올 수 있도록 하는 loss로, 이를 통해 두 Teacher의 KD_loss비중을 더 효율적이게 할 수 있다고 함.
    - 마지막으로 논문에 spkd loss도 소개가 되어있는데 하필 없는쪽 코드를 봐서 여기엔 없지만, 전에 봤던것들이랑 아에 똑같아서 생략.
    ```python
    if self.train_gwt:
        # Todo: multimodal loss
        # print(f'attn_concept.shape {attn_concept.shape}')
        # print(f'attn_feat_img.shape {attn_feat_img.shape}')
        # print(f'attn_feat_sig.shape {attn_feat_sig.shape}')
        loss_align_st1 = F.mse_loss(attn_concept, torch.transpose(attn_feat_img, -2, -1)) # Student의 KTS vs T1의 KIS 를 mse로 비교.
        loss_align_st2 = F.mse_loss(attn_concept, torch.transpose(attn_feat_sig, -2, -1)) # Student의 KTS vs T2의 KIS 를 mse로 비교
        loss_align = loss_align_st1 + loss_align_st2 # alpha_DC를 최적으로 해주기 위함이라고 함. → Student의 KTS가 각 Teacher 모두와 가까워지도록.
        # Todo: dynamic adaptation using multimidal alignment distance
        dist_st1 = loss_align_st1.detach()
        dist_st2 = loss_align_st2.detach()
        coef_dynamic = dist_st1 / (dist_st1 + dist_st2) # 각 값들은 위의 MSE로 구해지고, detach()했기 때문에, coef_dynamic은 gradient가 흘러가지 않음.
        # print(f'coeff_dist: {coef_dynamic}')
        loss = (1 - lambda_) * loss_SL + coef_dynamic * lambda_ * T * T * loss_KD + (
                1 - coef_dynamic) * lambda_ * T * T * loss_KD2
        loss += self.lam_align * loss_align  # multimodal alignment loss
        # 논문에서는 gamma_s * L_s 가 있는데 이거는 SPKD loss이고, 여기 코드에는 spkd미적용. 다른 곳엔 있음.
    else:
        loss = (1 - lambda_) * loss_SL + 0.3 * lambda_ * T * T * loss_KD + 0.7 * lambda_ * T * T * loss_KD2
    ```



## VQShape논문
### Introduction
1. 문제인식
    - 최근의 많은 Time-series관련 모델들은 Transformer 계열, 딥러닝 기반 모델이지만, 내부 표현(Latent Representation)이 사람에게 직관적인 해석 가능성을 보여주지 못한다.
        - time-series forecasting과 같은 task에서 이러한 모델들은 좋은 성능을 보이지만, 왜 그렇게 예측했는지를 알 수 없음.
        - Classificaion과 같은 task에서도 왜 이러한 shape이 특정 class를 의미하는지를 알 수 없음.
    - 최근의 모델들은, 여러 domain에 걸쳐서 작동되기가 어렵다.
        - 서로 다른 domain, sampling rate, window length, scale, noise level 등 변수가 너무 다양해서, 특정 dataset/domain에서는 잘 작동하지만, 다른 곳에서는 성능이 떨어지는 경우가 많다.
        - 특히, 해석 가능한 model 혹은, domain specialist model(도메인 특화 모델) 등은 한 데이터셋에 최적화되어 있어서 도메인 간에 전이가 잘 이루어지지 않음.
2. 기존의 방식
    - Shapelet 방식
        - 시계열의 일부분(Subsequence) 중에서 특정 클래스 분류에 유용한 모양(shape)을 나타내는 부분(shapelet)을 찾아서 그것을 특징(feature)로 사용.
            - 우선, 후보 Shapelet들을 Sliding window 방식 또는 무작위 샘플링 방식으로 여러개를 추출한다.
            - 각 후보 shapelet에 대해서, class discriminablity와 같은 지표로 각 클래스마다 "좋은 Shapelet"만 k개 남김.
            - prediction 단계에서, 샘플 time-series와 각 shapelet을 비교한 것을 feature로 이용하여, 최종 prediction에 활용.
        - 장점
            - 해석 가능하다.
            - Global Pattern보다는, Local Pattern을 잘 포착해서, class 구분에 유용하다.
        - 단점
            - shapelet의 길이를 미리 정의해야 해서, 유연성이 부족하다. ex) 같은 행동이더라도, 사람에 따라 length나 amplitude가 다를 수 있는데, shapelet은 그것이 같은 행동이라고 인지하지 못할 수 있음.
            - 도메인 간의 일반화 능력이 떨어짐. (sampling ratio, noise level 등이 다르다면, 그거에 맞춘 shapelet들이 다시 필요함.)
            - computation cost가 매우 높다. (후보 subsequence가 많을 때 + 샘플 시계열과 shapelet과의 distance 계산)
    - VQ-VAE 방식
        - 학습하는 것 : Encoder, Discreted CodeBook, Decoder
        - 학습 방식
            1. time-series를 여러 개의 patch로 쪼갠다.
            2. 각 patch들을 encoder에 넣어서 각 patch마다 vector의 형태로 얻는다.
            3. 각 패치의 vector와 codebook 내의 code(vector)들의 거리를 계산해서 가장 가까운 code로 각 patch를 대체함.
            4. Decoder는 patch단위(혹은 token sequence단위)로 복원을 수행하고, 전체 time-series를 조립한 후 loss 계산.
        - 장점
            - label(GT)없이도, 강력하게 표현학습이 가능하다. → Ground Truth없이 자기 자신과의 비교만 수행함.
        - 단점
            - 해석 가능하지 못함. 각 token이 뭔지 모름.
            - scale, spped, offset 변화에 대한 불변성이 부족함. → 이것 또한, 고정 길이의 patch를 사용하기 때문에, 사람마다 개인차가 존재하는데 이것을 구분 못함.
3. 우리가 하려는 VQShape에 대한 개요.
    - Shapelet 방식과 VQ-VAE 방식을 잘 결합해서 사용할것이다.
        - Shapelet의 time-series의 일부분이 클래스 구분에 핵심이라는 관점 채택
        - VQ-VAE의 벡터 양자화와 코드북을 통한 dictionary 구성과, 재구성 학습을 통해 Label없이도 내부 표현을 학습할 수 있는 것 채택.
    - Time-series의 Subsequence를 (Shape(Abstract) + Attributes)로 분해하고, Shape만 VQ로 이산화하여 Shape codebook을 만듦. 

### 방법론에 대한 완전한 이해. (input ~ prediction 까지 end to end로 이해해보자.)
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/9월/25.9.5/VQShape.png" alt="results" width="700">
- 1. Instance Noramlization
    - 하는 일 : 한 시계열 내의 오프셋/스케일 편차를 줄여 patch embedding이 모양(형상)에 집중되게 하는 전처리.
    - 출력 모양 : (1, 512). → 하나당 512 길이를 가진다고 가정.
- 2. Patching
    - 하는 일 : 길이 512의 시계열을 K=64개의 비중첩 고정 길이의 패치(d_patch, 각 패치 길이)로 분할.
    - 출력 모양 : (1, 64, 8).
- 3. Linear Encoding
    - 하는 일 : 각 패치를 임베딩 차원으로 linear projection함. (learnable한 linear encoder를 이용.)
    - 출력 모양 : (1, 64, 512).
- 4. Positional Embedding
    - 하는 일 : 토큰의 순서 정보를 더한다.
    - 출력 모양 : (1, 64, 512). → 각 patch마다 순서 정보만 더할 뿐 출력 형태는 바뀌지 않음.
- 5. Time-series Encoder (Transformer 기반)
    - 하는 일 : self-attention으로 패치들 간 정보를 섞어 K개의 '문맥화된' 잠재 embedding을 생성. 각 k번째(1, k, 512) 값은 k번째 패치뿐 아니라, 전체 패치 정보가 들어있다.
    - 출력 모양 : (1, 64, 512). → 각 패치들이 서로 간의 정보를 반영한 형태로 변환될 뿐 출력 형태는 같다. 최종 : ĥ_k (0 <= k < 64)
- --------------------------------------여기까지는 ViT와 매우 유사함.----------------------------------
- 6. Attribute Decoder
    - 하는 일 : 64개의 ĥ_k에서 속성 튜플 τ̂_k = (z_k, μ_k, σ_k, t_k, l_k)을 "예측" 하는 모듈. 
        - 즉, 각각 다른 MLP 헤드가 5개 존재하고, 각각을 "예측"하기 위한 MLP에 입력 (1, 64, 512)을 넣어서 출력으로 얻는다.
        - 아래의 각 속성을 출력하는 layer들은 모델과 같이 학습이 진행되고, 각 속성들을 조합해서, 원본 시계열을 복원할 수도 있고, 이를 통해 부분 구간/전체 구간 재구성 손실을 구성함.
        - z_k : 양자화 후의 z_k로, "추상화된 모양" 자체를 담는 잠재 벡터임. 코드북 Z의 한 코드를 의미하며, Shape Decoder S(뒤에 나옴.)가 이 벡터만 보고 offset, scale, length가 제거된 순수한 파형을 복원할 수 있어야 함. 이후 σ·μ를 다시 입혀서 실제 구현을 재현하는 방식. (왜 재현해야 하는가? Reconstruction loss를 사용하나?). 또한, 양자화 자체는 VQ-VAE 방식을 사용하고, 이 때 code book을 8차원 벡터로 구성했기 때문에 8차원의 값을 사용했다는 듯.
        - μ_k : offset(평균)을 의미하는 scalar 값. (512 → h → 1)로 Hidden layer가 하나 존재하는 MLP로 구성.
        - σ_k : scale(표준편차)를 의미하는 scalar 값. softplus를 사용해서 양수로 출력되게 함.
        - t_k : 시작 위치를 의미하는 scalar 값.
        - l_k : 잘라낼 구간의 길이를 의미하는 scalar 값. 
    - 출력 형태 : (1, 64, 12) 12 = 8+1+1+1+1
- ---------------------------------------이렇게 얻어진 Attributes들을 어떻게 사용하는지-------------------------
- 7. Latent Space Operation 부분
    - 표준 모양을 얻기. (L_vq)
        - Attribute Decoder를 통해 얻은 각 Patch의 8차원 Shape latent z_k를 Codebook의 가장 가까운 코드 e_j로 치환함.
        - 이 때 L_vq : z_k와 선택된 코드 e_j가 더 가까워지도록 하는 벡터 양자화 손실.
        - 선택된 코드 e_j를 Shape Decoder에 통과시켜 표준 모양 s_k를 얻는다. 
    - Offset/Scale 적용 (L_s)
        - 표준 모양 s_k는 아직 "정규화된 모양" 그 자체임.
        - Attribute Decoder가 예측한 Offset(μ_k), Scale(σ_k)을 적용해서 s′_k = μ_k + σ_k·s_k 로 s_k를 보정한다. (s'_k 얻음.)
        - 이제, Attribute Decoder가 예측한 시작 구간(t_k)과, 구간 길이(l_k)를 이용해서 원본 time-series의 subsequence를 가져온다.
        - 가져온 subsequence는, Shape Decoder로 얻은 z'_k의 length에 맞게 interpolation된다. → 그렇지 않으면, 서로 loss를 계산할 수 없고, z'_k를 subsequence에 맞추기에는, codebook code의 일반화 능력(일관성)이 사라짐.
        - 모델이 patch를 보고 예측한 "이 patch는 원본 time-series의 t, l에 대한 subsequence이고, 대체적인 codebook code의 표현으로는 z일 것이야. 그리고, 그것을 Shape Decoder에 넣은 뒤 μ, σ의 보정을 거치면 그 subsequence와 비슷하게 되겠지?" 라는 학습을 수행하게 된다. → L_s는 z'_k와 interpolation된 subsequence를 비교한 loss.
    - 위치, 길이의 다양성을 유도하는 L_div
        - (t, l)은 원래라면, positional encoding과 patch length를 알고 있기 때문에, 그냥 알 수 있고, 이를 Ground Truth로 해서 Supervised Learning을 시킬 수 있다.
        - 하지만, 이것보다 Self-supervised learning의 방식을 채택하는 것이 z_k, offset, scale, shpae decoder 등 모두를 학습하는데에 더 큰 도움이 되기 때문에 t, l도 예측하도록 시키는 것임.
        - 하지만, 이렇게 했을 때 반복 패턴이 많거나, 짧은 길이에 대해서 반복적으로 모델이 예측을 하는 현상이 있어서, 이것을 막기 위해 L_div를 넣음.
        - L_div는 각 패치마다 예측된 t, l이 서로 겹치는 부분이 적어지도록 하는 Loss이다. 이를 통해서 각 patch들이 시계열 전체의 넓은 범위를 커버할 수 있게 한다.
- 8. 마지막 재구성 손실을 얻기위한 복원.
    - Latent Space Operation 전의 tau에는 Attribute Decoder를 통해 예측된 z​,μ,σ​,t​,l 이 존재했는데, 여기서 z만 codebook의 가장 가까운 code로 치환한다.
    - 그렇게 얻어진 새로운 tau를 Attribute Encoder → Time-series Decoder 를 통과시켜 원본 시계열을 복원한다.
    - 이렇게 복원된 Time-series와, 원본 Time-series를 비교해서 마지막 L_x를 얻는다.
    - 이렇게 하면, 이 최종 L_x를 통해 Attribute Encoder, Time-series Decoder뿐 아니라, codebook code의 표현력까지도 강화가 될듯?

### 그럼 이걸 어떻게 Classification에 사용함?
- 데이터셋에 대해서 Codebook, Linear Encoder, Time-series Encoder, Attribute Decoder 등을 미리 학습함. 
    - 위의 과정을 통해서 학습을 해서, 이 데이터셋에 대해 최적의 Codebook, Time-series Encoder, Attribute Decoder 등의 것들이 만들어짐.
- 이렇게 학습된 것들을 가지고, 예측할 Time-sereis X를 통과시킨다.
    - 이렇게 쭉 아키텍처를 순차적으로 지나, Attribute Decoder까지 통과하면 τ = (z​,μ​,σ​,t​,l​)까지 얻는다.
    - 여기서 z는 학습때와 똑같이 codebook의 code들과 거리를 비교해서 가장 가까운 code e와 치환됨.
    - 학습 때였으면 이렇게 치환된 τ'을 복원한 후 reconstruction loss를 구했을 텐데, 예측때에는 딱 이 τ'을 만드는 것 까지함.
- 최종 예측
    - 최종 예측 때에는, 이렇게 얻어진 τ'을 Feature로 하여, Classifier에 넣어서 최종 예측을 수행한다.
    - 즉, 예측 때 각 Time-series를 Patch 단위로 분해하고, 각 Patch들을 codebook의 코드로 치환하는데, 그 치환된 code들이 어떤 class에 부합한지 분류하는 느낌으로 예측을 수행한다.? 
    - 예측할 Time-series X를 Patch 단위로 분해하고, 각 Patch마다 codebook내의 code로 치환하는데, 그 codebook의 code들을 attribute로 모양을 만들고, 조합했을 때 "이 code들의 조합은 이 class를 의미하는구나" 라는 느낌으로 학습하게 됨.
    - 이런 점에서, 각 codebook내의 code들은 특정 interpretable한 특성을 가진다고 볼 수 있고, 모든 codebook들을 Shape Decoder를 통하게 하면, 이거를 직접 사람이 눈으로도 확인할 수 있어서 interpretable하다는 것이다.

### github 코드를 보며, 어떤 부분이 어떠한지까지도 알 수 있으면 좋을텐데, 이건 좀 걸릴듯
1. 일단 model을 찾은듯
    - 일단 보면, 주요한 class 객체가 몇개 보임.
    - PatchEncoder, PatchDecoder, AttributeEncoder, AttributeDecoder, ShapeDecoder가 있음.
    - AttributeEncoder/Decoder, ShapeDecoder는 논문에 나와있는 이름 그대로라서 무엇을 하는 class인지 확실한데 PatchEncoder, PatchDecoder가 살짝 다름.
    - 하지만, Architecture를 보면, Patching → Linear Encoding + Positional Encoding → Time-series Encoder인것과, Time-series Decoder가 있는거로 봐서 얘네가 PatchEncoder/Decoder라고 생각하고 코드를 봐야할듯.
    ```python
    class VQShape(nn.Module):
        def __init__(
                self, 
                dim_embedding: int = 256, # Embedding dimension of Transformers
                patch_size: int = 8, # Patch size of PatchTST backbone
                num_patch: int = 64, # Number of patches of PatchTST backbone
                num_enc_head: int = 6, # Number of heads in Transformer encoder
                num_enc_layer: int = 6, # Number of layers in Transformer encoder
                num_tokenizer_head: int = 6, # Number of heads in Transformer tokenizer
                num_tokenizer_layer: int = 6, # Number of layers in Transformer tokenizer
                num_dec_head: int = 6, # Number of heads in Transformer decoder
                num_dec_layer: int = 6, # Number of layers in Transformer decoder
                num_token: int = 32, # Number of shape tokens (output of tokenizer)
                len_s: int = 256, # Unified length of shapes
                len_input: int = 512, # Unified length of input time series
                s_smooth_factor: int = 11, # Smoothing factor for moving average
                num_code: int = 512, # Codebook size # 코드북에 넣을 코드 개수인듯
                dim_code: int = 8, # Shape code dimension # 이게 아마 codebook 내의 각 code의 dimension이고, abstracted shape이 8차원이었어서 이것도 똑같은듯?
                codebook_type: str = "standard", # Type of codebook
                lambda_commit: float = 1., # Commitment loss coefficient
                lambda_entropy: float = 1., # Entropy loss coefficient of the codebook
                entropy_gamma: float = 1., # Entropy gamma of the codebook
                mask_ratio: float = 0.25 # Mask ratio for pretraining
            ):
            super().__init__()
            
            self.len_s = len_s
            self.s_smooth_factor = s_smooth_factor
            self.num_code = num_code
            self.codebook_type = codebook_type
            self.min_shape_len = 1/64
            self.entropy_gamma = entropy_gamma

            self.num_patch = num_patch
            self.patch_size = patch_size  
            self.mask_ratio = mask_ratio
            self.num_token = num_token

            self.encoder = PatchEncoder(
                dim_embedding=dim_embedding,
                patch_size=patch_size,
                num_patch=num_patch,
                num_head=num_enc_head,
                num_layer=num_enc_layer
            )

            if codebook_type == "standard":
                self.codebook = EuclCodebook(
                    num_code, 
                    dim_code, 
                    commit_loss=lambda_commit,
                    entropy_loss=lambda_entropy,
                    entropy_gamma=entropy_gamma
                )
            elif codebook_type == "vqtorch": # 이거를 쓰는거 같은데
                from vqtorch.nn import VectorQuant
                self.codebook = VectorQuant(
                    feature_size=dim_code,
                    num_codes=num_code,
                    beta=0.95,
                    kmeans_init=False,
                    affine_lr=10,
                    sync_nu=0.2,
                    replace_freq=40,
                    dim=-1
                )
            else:
                raise NotImplementedError(f"Invalid codebook type [{codebook_type}].")
            
            self.decoder = PatchDecoder(
                dim_embedding=dim_embedding,
                patch_size=int(len_input / num_token),
                num_head=num_dec_head,
                num_layer=num_dec_layer
            )

            self.tokenizer = Tokenizer(
                dim_embedding=dim_embedding,
                num_token=num_token,
                num_head=num_tokenizer_head,
                num_layer=num_tokenizer_layer
            )

            self.attr_encoder = AttributeEncoder(dim_code=dim_code, dim_embedding=dim_embedding)
            self.attr_decoder = AttributeDecoder(dim_code=dim_code, dim_embedding=dim_embedding)
            self.shape_decoder = ShapeDecoder(dim_code, len_s, out_kernel_size=s_smooth_factor)

        def forward(self, x, *, mode='pretrain', num_input_patch=-1, mask=None, finetune=False):
            '''
            x: shape (batch_size, time_steps), time series data
            mode: mode of the forward pass
            num_input_patch: number of patches of the input time series (!! set if x is a partial time series, e.g. forecasting)
            mask: mask that indicates the missing values in the input time series (for imputation)
            finetune: whether to compute loss and update parameters for downstream tasks
            '''
            if mode == 'pretrain':
                return self.pretrain(x)
            elif mode == 'evaluate':
                return self.evaluate(x)
            elif mode == 'tokenize':
                return self.tokenize(x)
            elif mode == 'forecast':
                return self.forecast(x, num_input_patch, finetune)
            elif mode == 'imputation':
                return self.imputation(x, mask, finetune)
            else:
                raise NotImplementedError(f"VQShape: Invalid mode [{mode}]")
    ```
    - 여기서 나는 pretrain()부분을 먼저 봐야할듯 함.
    - pretrain의 코드와 논문을 대입해서 보면, x_mean, x_std를 통해 instance Norm을 이미 한 상태로 patchencoder의 patch_and_embed함수를 호출한다.
    - 그러고 나온 
    ```python
    def pretrain(self, x: torch.Tensor):
        self.x_mean = x.mean(dim=-1, keepdims=True)
        self.x_std = (x.var(dim=-1, keepdims=True) + 1e-5).sqrt()
        # Patch and embed the ts data
        # self.encoder가 PatchEncoder 클래스 객체
        x_embed = self.encoder.patch_and_embed((x - self.x_mean)/self.x_std) # 이거로 Instance Norm을 한 상태로 encoder에 넣어버림.
        batch_size, num_patch, patch_dim = x_embed.shape

        ## 여기까지가 Time-series Encoder(Transformer)를 지난 시점.
        ## x_embed = (B, num_patch=64, patch_dim=256)
        
        # Mask a subset of the patches
        # 아마도 Self-Supervised Learning을 하기 위해서 Masking을 하는 부분.
        num_masked = int(self.mask_ratio * num_patch) # mask ratio(25%)만큼 랜덤하게 패치를 골라 masking함. 64 / 4 = 16개 마스크
        rand_indices = torch.rand(batch_size, num_patch, device=x.device).argsort(dim=-1) # index를 랜덤으로 섞어버림.
        masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:] # 그래서 섞인 index의 앞의 16개는 masking, 뒤의 48개는 unmasking.
        x_unmasked = x_embed[torch.arange(batch_size, device=x.device).unsqueeze(1), unmasked_indices] # 여기부터 다시보자.....

        # Encode unmasked patches
        x_unmasked = self.encoder.transformer(x_unmasked) # 이게 되는거여? 아마 self.transformer 만든데에 바로 넣어버린다 이런거같긴함.

        return self._forward(x, x_unmasked, None, compute_loss=True)

    def _forward(
            self, 
            x: torch.Tensor, 
            x_embed: torch.Tensor, 
            tokenizer_attn_mask: torch.Tensor, 
            compute_loss: bool = False
        ):
        '''
        x: shape (batch_size, time_steps), time series data
        x_embed: shape (batch_size, num_patch, dim_embedding), embedded patches of the input time series
        tokenizer_attn_mask: shape (batch_size, num_token), mask that indicates the missing values in the input patches
        compute_loss: whether to compute loss
        '''

        # Tokenize
        h_shape = self.tokenizer(x_embed, tokenizer_attn_mask)

        # Latent space operations to quantize and decode the tokens
        z_e, tl_sample, mu_hat, sigma_hat = self.attr_decoder(h_shape)
        t_hat, l_hat = tl_sample[...,[0]], tl_sample[...,[1]]

        t_hat = t_hat * (1 - self.min_shape_len)
        l_hat = l_hat * (1 - t_hat) + self.min_shape_len
        if self.codebook_type == 'standard':
            z_q, z_idx, z_loss = self.codebook(z_e)
        else:
            z_q, vq_dict = self.codebook(z_e)
            z_loss = vq_dict['loss']
            z_idx = vq_dict['q']
        h_hat = self.attr_encoder(z_q, t_hat, l_hat, mu_hat, sigma_hat)

        # Reconstruct time-series
        x_hat, _ = self.decoder(h_hat)
        x_hat = x_hat * self.x_std + self.x_mean

        # Decode tokens into shapes
        s_hat_norm, _, _ = self.shape_decoder(z_q)
        s_hat = s_hat_norm * sigma_hat + mu_hat
        s_hat = s_hat * self.x_std.unsqueeze(1) + self.x_mean.unsqueeze(1)

        output_dict = {
            'x_true': x,
            'x_pred': x_hat,
            's_true': None,
            's_pred': s_hat,
            'code': z_q,
            'code_idx': z_idx,
            't_pred': t_hat,
            'l_pred': l_hat,
            'mu_pred': mu_hat,
            'sigma_pred': sigma_hat
        }

        # Compute loss
        if compute_loss:
            # Reconstruction loss
            x_loss = nn.functional.mse_loss(x_hat, x)
            s = extract_subsequence(x, t_hat, l_hat, self.len_s, smooth=self.s_smooth_factor)
            s_loss = nn.functional.mse_loss(s_hat, s.detach())
            output_dict['s_true'] = s

            # Disentanglement loss
            log_l = l_hat.log() / (torch.ones_like(l_hat) * self.min_shape_len).log()
            dist_loss = eucl_sim_loss(torch.cat([torch.cos(torch.pi * t_hat) * log_l, torch.sin(torch.pi * t_hat) * log_l], dim=-1), 0.2)

            loss_dict = {
                'ts_loss': x_loss.unsqueeze(0),
                'vq_loss': z_loss.unsqueeze(0),
                'shape_loss': s_loss.unsqueeze(0),
                'dist_loss': dist_loss.unsqueeze(0)
            }
            return output_dict, loss_dict
        else:
            return output_dict
    ```