## 할 일.
- Abstracted Shapes as Tokens- A Generalizable and Interpretable Model for Time-series Classification 논문 자세히 읽어보기. + 코드 살펴보기.
- Improved Knowledge Distillation Based on Global Latent Workspace with Multimodal Knowledge Fusion for Understanding Topological Guidance on Wearable Sensor Data 논문 자세히 읽어보고, 코드 살펴보기.

## mGLW논문 
### 방법론 위주로 논문을 읽어보자.
- III-B(1) : Preprocessing
    - 각 Teacher 1, Teacher 2의 마지막 Feature Vector를 사용해서 Slot을 업데이트해야하는데, 둘의 Shape이 다르면 안된다.
    - 또한 Student도 이 업데이트된 Slot을 이용해서 표현을 바꿀건데, 이것의 Shape이 다르면, 전체적으로 같은 Knowledge가 Alignment되는게 아니다.
    - 그래서, Teacher1, Teacher2, Student의 Feature Vector를 가장 작은 것의 크기로 맞춰준다.

- III-B(2) : Slot Attention을 통한 Slot 업데이트
    - Query : Slot (K x D), K는 Slot개수, D는 embedding shape
    - Key/Value : Token (L x D), L은 Length, D는 동일
        - 물론, 각 Query, Key, Value마다 특정 가중치(선형 layer)를 통해 더 고도화된 것으로 변환됨.
    - Slot Attention을 통해서 Query와 Key를 Attention하는데, Softmax의 축을 Slot 쪽에 걸어줌.
        - Q(ζ)·K(e).T → (K x D)·(D x L) → (K x L)
        - 즉, 각 Slot과 Token간의 score 형태로 나오게 됨.
        - 그런데, Softmax를 Slot 쪽에 걸어주기 때문에, 각 토큰마다, Slot의 점수 합 = 1이 된다.
        - 이것은 슬롯 간의 경쟁을 유도하고, 한 토큰에 대해 여러 Slot이 높은 Score를 가질 수 없게 함. → 각 토큰을 하나의 Slot에 특정시키기 위함.
        - Attention 자체는, Q와 K사이의 유사도 score를 만드는 것이 목적이고, 어느 축으로 Softmax를 하느냐에 따라 해석/역할이 달라진다.
            - Q축(Slot) : 각 Key가 어떤 Query에 배정될지 경쟁. → Slot Attention이 하는 것.
            - K축(Token) : 각 Query가 어떤 Key를 얼마나 볼지. → 대부분의 Attention이라고 하면 이거인데
    - 이렇게 한 번 Softmax된 점수들을 가지고, 다시 Token축에 대해서 정규화를 진행한다.
        - 이 때는 그냥 전체 합으로 나누어주는 방식을 사용. → A / sum(A)
        - 각 슬롯을 업데이트하기 위해서, 실제로 곱해질 가중치를 만드는 단계
        - 슬롯별 업데이트 크기 차이를 줄이고, 안정성 학습을 위함임.
    - 위와 같이 재정규화를 통해 얻어진 Weight와 Value를 곱해서 얻은 것을 u라고 하자.
        - 이 u는 실제로 Slot을 업데이트하기 위한 GRU의 입력으로 들어갈 것임.
        - 이 기존 State는 유지하고, Input을 T1으로부터 얻은 u1과, T2로부터 얻은 u2의 가중합으로 넣어준다.
        - 가중합 beta는 0.5를 사용한 듯 함.

- III-B(3) : Slot을 이용한 Prediction
    - 위의 업데이트된 Slot(ζ)과, Student의 Feature Embedding Es를 이용하여 Cross Attention 수행.
        - Q(Es)를 Query, K(ζ),V(ζ)를 Key/Value로 두고, Slot을 축으로 한 Softmax를 수행함.
        - Token마다, 각 Slot에 대해서 점수를 가지게 되고, 그 Slot의 점수의 합이 1임.
        - Weight는 Es(L x D), ζ(K x D)이기에, L x K의 Shape이 됨.
        - 이 가중치를 그대로, V(ζ)에 곱해서 (L x D)의 "Slot을 참고하여 문맥화된 Token표현"을 만든다.
    - 최종 Prediction
        - 위의 "Slot을 참고하여 문맥화된 Token"을 바로 Classifier에 넣는게 아니라, Residual Connection과 같은 것을 추가한다.
        - 기존의 Es (L x D)를 더해주는데, 둘의 Shape이 같기 때문에 그냥 단순히 더해준다.
        - 원소별 덧셈이므로 (L x D)형태 그대로.
        - 이렇게 나온 거를 그대로 쭉 진행함.
    - 즉, Student의 마지막 Feature Vector를 (기존 값 + Slot을 참고하여 문맥화된 표현)으로 바꿔줘서 마지막 FC layer에서 참고할 정보를 더 추가해준다는 느낌.


### 코드도 보면서, 그러한 방법론이 어떻게 구현되어있는지를 알아봐야함.

## VQShape논문
### 방법론에 대한 완전한 이해. (input ~ prediction 까지 end to end로 이해해보자.)
- 



### github 코드를 보며, 어떤 부분이 어떠한지까지도 알 수 있으면 좋을텐데, 이건 좀 걸릴듯.