## 할 일.
- Abstracted Shapes as Tokens- A Generalizable and Interpretable Model for Time-series Classification 논문 자세히 읽어보기. + 코드 살펴보기.
- Improved Knowledge Distillation Based on Global Latent Workspace with Multimodal Knowledge Fusion for Understanding Topological Guidance on Wearable Sensor Data 논문 자세히 읽어보고, 코드 살펴보기.

## mGLW논문 
### 방법론 위주로 논문을 읽어보자.
- III-B(1) : Preprocessing
    - 각 Teacher 1, Teacher 2의 마지막 Feature Vector를 사용해서 Slot을 업데이트해야하는데, 둘의 Shape이 다르면 안된다.
    - 또한 Student도 이 업데이트된 Slot을 이용해서 표현을 바꿀건데, 이것의 Shape이 다르면, 전체적으로 같은 Knowledge가 Alignment되는게 아니다.
    - 그래서, Teacher1, Teacher2, Student의 Feature Vector를 가장 작은 것의 크기로 맞춰준다.

- III-B(2) : Slot Attention을 통한 Slot 업데이트
    - Query : Slot (K x D), K는 Slot개수, D는 embedding shape
    - Key/Value : Token (L x D), L은 Length, D는 동일
        - 물론, 각 Query, Key, Value마다 특정 가중치(선형 layer)를 통해 더 고도화된 것으로 변환됨.
    - Slot Attention을 통해서 Query와 Key를 Attention하는데, Softmax의 축을 Slot 쪽에 걸어줌.
        - Q(ζ)·K(e).T → (K x D)·(D x L) → (K x L)
        - 즉, 각 Slot과 Token간의 score 형태로 나오게 됨.
        - 그런데, Softmax를 Slot 쪽에 걸어주기 때문에, 각 토큰마다, Slot의 점수 합 = 1이 된다.
        - 이것은 슬롯 간의 경쟁을 유도하고, 한 토큰에 대해 여러 Slot이 높은 Score를 가질 수 없게 함. → 각 토큰을 하나의 Slot에 특정시키기 위함.
        - Attention 자체는, Q와 K사이의 유사도 score를 만드는 것이 목적이고, 어느 축으로 Softmax를 하느냐에 따라 해석/역할이 달라진다.
            - Q축(Slot) : 각 Key가 어떤 Query에 배정될지 경쟁. → Slot Attention이 하는 것.
            - K축(Token) : 각 Query가 어떤 Key를 얼마나 볼지. → 대부분의 Attention이라고 하면 이거인데
    - 이렇게 한 번 Softmax된 점수들을 가지고, 다시 Token축에 대해서 정규화를 진행한다.
        - 이 때는 그냥 전체 합으로 나누어주는 방식을 사용. → A / sum(A)
        - 각 슬롯을 업데이트하기 위해서, 실제로 곱해질 가중치를 만드는 단계
        - 슬롯별 업데이트 크기 차이를 줄이고, 안정성 학습을 위함임.
    - 위와 같이 재정규화를 통해 얻어진 Weight와 Value를 곱해서 얻은 것을 u라고 하자.
        - 이 u는 실제로 Slot을 업데이트하기 위한 GRU의 입력으로 들어갈 것임.
        - 이 기존 State는 유지하고, Input을 T1으로부터 얻은 u1과, T2로부터 얻은 u2의 가중합으로 넣어준다.
        - 가중합 beta는 0.5를 사용한 듯 함.

- III-B(3) : Slot을 이용한 Prediction
    - 위의 업데이트된 Slot(ζ)과, Student의 Feature Embedding Es를 이용하여 Cross Attention 수행.
        - Q(Es)를 Query, K(ζ),V(ζ)를 Key/Value로 두고, Slot을 축으로 한 Softmax를 수행함.
        - Token마다, 각 Slot에 대해서 점수를 가지게 되고, 그 Slot의 점수의 합이 1임.
        - Weight는 Es(L x D), ζ(K x D)이기에, L x K의 Shape이 됨.
        - 이 가중치를 그대로, V(ζ)에 곱해서 (L x D)의 "Slot을 참고하여 문맥화된 Token표현"을 만든다.
    - 최종 Prediction
        - 위의 "Slot을 참고하여 문맥화된 Token"을 바로 Classifier에 넣는게 아니라, Residual Connection과 같은 것을 추가한다.
        - 기존의 Es (L x D)를 더해주는데, 둘의 Shape이 같기 때문에 그냥 단순히 더해준다.
        - 원소별 덧셈이므로 (L x D)형태 그대로.
        - 이렇게 나온 거를 그대로 쭉 진행함.
    - 즉, Student의 마지막 Feature Vector를 (기존 값 + Slot을 참고하여 문맥화된 표현)으로 바꿔줘서 마지막 FC layer에서 참고할 정보를 더 추가해준다는 느낌.


### 코드도 보면서, 그러한 방법론이 어떻게 구현되어있는지를 알아봐야함.
- 내일 와서 train_merge_7cls의 loss부분부터 보면 됨. 어떻게 loss가 구현되어있나?
- 내일 그거 보고, VQShape 코드도 봐야함.

## VQShape논문
### Introduction
1. 문제인식
    - 최근의 많은 Time-series관련 모델들은 Transformer 계열, 딥러닝 기반 모델이지만, 내부 표현(Latent Representation)이 사람에게 직관적인 해석 가능성을 보여주지 못한다.
        - time-series forecasting과 같은 task에서 이러한 모델들은 좋은 성능을 보이지만, 왜 그렇게 예측했는지를 알 수 없음.
        - Classificaion과 같은 task에서도 왜 이러한 shape이 특정 class를 의미하는지를 알 수 없음.
    - 최근의 모델들은, 여러 domain에 걸쳐서 작동되기가 어렵다.
        - 서로 다른 domain, sampling rate, window length, scale, noise level 등 변수가 너무 다양해서, 특정 dataset/domain에서는 잘 작동하지만, 다른 곳에서는 성능이 떨어지는 경우가 많다.
        - 특히, 해석 가능한 model 혹은, domain specialist model(도메인 특화 모델) 등은 한 데이터셋에 최적화되어 있어서 도메인 간에 전이가 잘 이루어지지 않음.
2. 기존의 방식
    - Shapelet 방식
        - 시계열의 일부분(Subsequence) 중에서 특정 클래스 분류에 유용한 모양(shape)을 나타내는 부분(shapelet)을 찾아서 그것을 특징(feature)로 사용.
            - 우선, 후보 Shapelet들을 Sliding window 방식 또는 무작위 샘플링 방식으로 여러개를 추출한다.
            - 각 후보 shapelet에 대해서, class discriminablity와 같은 지표로 각 클래스마다 "좋은 Shapelet"만 k개 남김.
            - prediction 단계에서, 샘플 time-series와 각 shapelet을 비교한 것을 feature로 이용하여, 최종 prediction에 활용.
        - 장점
            - 해석 가능하다.
            - Global Pattern보다는, Local Pattern을 잘 포착해서, class 구분에 유용하다.
        - 단점
            - shapelet의 길이를 미리 정의해야 해서, 유연성이 부족하다. ex) 같은 행동이더라도, 사람에 따라 length나 amplitude가 다를 수 있는데, shapelet은 그것이 같은 행동이라고 인지하지 못할 수 있음.
            - 도메인 간의 일반화 능력이 떨어짐. (sampling ratio, noise level 등이 다르다면, 그거에 맞춘 shapelet들이 다시 필요함.)
            - computation cost가 매우 높다. (후보 subsequence가 많을 때 + 샘플 시계열과 shapelet과의 distance 계산)
    - VQ-VAE 방식
        - 학습하는 것 : Encoder, Discreted CodeBook, Decoder
        - 학습 방식
            1. time-series를 여러 개의 patch로 쪼갠다.
            2. 각 patch들을 encoder에 넣어서 각 patch마다 vector의 형태로 얻는다.
            3. 각 패치의 vector와 codebook 내의 code(vector)들의 거리를 계산해서 가장 가까운 code로 각 patch를 대체함.
            4. Decoder는 patch단위(혹은 token sequence단위)로 복원을 수행하고, 전체 time-series를 조립한 후 loss 계산.
        - 장점
            - label(GT)없이도, 강력하게 표현학습이 가능하다. → Ground Truth없이 자기 자신과의 비교만 수행함.
        - 단점
            - 해석 가능하지 못함. 각 token이 뭔지 모름.
            - scale, spped, offset 변화에 대한 불변성이 부족함. → 이것 또한, 고정 길이의 patch를 사용하기 때문에, 사람마다 개인차가 존재하는데 이것을 구분 못함.
3. 우리가 하려는 VQShape에 대한 개요.
    - Shapelet 방식과 VQ-VAE 방식을 잘 결합해서 사용할것이다.
        - Shapelet의 time-series의 일부분이 클래스 구분에 핵심이라는 관점 채택
        - VQ-VAE의 벡터 양자화와 코드북을 통한 dictionary 구성과, 재구성 학습을 통해 Label없이도 내부 표현을 학습할 수 있는 것 채택.
    - Time-series의 Subsequence를 (Shape(Abstract) + Attributes)로 분해하고, Shape만 VQ로 이산화하여 Shape codebook을 만듦. 

### 방법론에 대한 완전한 이해. (input ~ prediction 까지 end to end로 이해해보자.)
- 1. Instance Noramlization
    - 하는 일 : 한 시계열 내의 오프셋/스케일 편차를 줄여 patch embedding이 모양(형상)에 집중되게 하는 전처리.
    - 출력 모양 : (1, 512). → 하나당 512 길이를 가진다고 가정.
- 2. Patching
    - 하는 일 : 길이 512의 시계열을 K=64개의 비중첩 고정 길이의 패치(d_patch, 각 패치 길이)로 분할.
    - 출력 모양 : (1, 64, 8).
- 3. Linear Encoding
    - 하는 일 : 각 패치를 임베딩 차원으로 linear projection함. (learnable한 linear encoder를 이용.)
    - 출력 모양 : (1, 64, 512).
- 4. Positional Embedding
    - 하는 일 : 토큰의 순서 정보를 더한다.
    - 출력 모양 : (1, 64, 512). → 각 patch마다 순서 정보만 더할 뿐 출력 형태는 바뀌지 않음.
- 5. Time-series Encoder (Transformer 기반)
    - 하는 일 : self-attention으로 패치들 간 정보를 섞어 K개의 '문맥화된' 잠재 embedding을 생성. 각 k번째(1, k, 512) 값은 k번째 패치뿐 아니라, 전체 패치 정보가 들어있다.
    - 출력 모양 : (1, 64, 512). → 각 패치들이 서로 간의 정보를 반영한 형태로 변환될 뿐 출력 형태는 같다. 최종 : ĥ_k (0 <= k < 64)
- --------------------------------------여기까지는 ViT와 매우 유사함.----------------------------------
- 6. Attribute Decoder
    - 하는 일 : 64개의 ĥ_k에서 속성 튜플 τ̂_k = (z_k, μ_k, σ_k, t_k, l_k)을 "예측" 하는 모듈. 
        - 즉, 각각 다른 MLP 헤드가 5개 존재하고, 각각을 "예측"하기 위한 MLP에 입력 (1, 64, 512)을 넣어서 출력으로 얻는다.
        - 아래의 각 속성을 출력하는 layer들은 모델과 같이 학습이 진행되고, 각 속성들을 조합해서, 원본 시계열을 복원할 수도 있고, 이를 통해 부분 구간/전체 구간 재구성 손실을 구성함.
        - z_k : 양자화 후의 z_k로, "추상화된 모양" 자체를 담는 잠재 벡터임. 코드북 Z의 한 코드를 의미하며, Shape Decoder S(뒤에 나옴.)가 이 벡터만 보고 offset, scale, length가 제거된 순수한 파형을 복원할 수 있어야 함. 이후 σ·μ를 다시 입혀서 실제 구현을 재현하는 방식. (왜 재현해야 하는가? Reconstruction loss를 사용하나?). 또한, 양자화 자체는 VQ-VAE 방식을 사용하고, 이 때 code book을 8차원 벡터로 구성했기 때문에 8차원의 값을 사용했다는 듯.
        - μ_k : offset(평균)을 의미하는 scalar 값. (512 → h → 1)로 Hidden layer가 하나 존재하는 MLP로 구성.
        - σ_k : scale(표준편차)를 의미하는 scalar 값. softplus를 사용해서 양수로 출력되게 함.
        - t_k : 시작 위치를 의미하는 scalar 값.
        - l_k : 잘라낼 구간의 길이를 의미하는 scalar 값. 
    - 출력 형태 : (1, 64, 12) 12 = 8+1+1+1+1
- ---------------------------------------이렇게 얻어진 Attributes들을 어떻게 사용하는지-------------------------
- 7. Latent Space Operation 부분
    - 표준 모양을 얻기. (L_vq)
        - Attribute Decoder를 통해 얻은 각 Patch의 8차원 Shape latent z_k를 Codebook의 가장 가까운 코드 e_j로 치환함.
        - 이 때 L_vq : z_k와 선택된 코드 e_j가 더 가까워지도록 하는 벡터 양자화 손실.
        - 선택된 코드 e_j를 Shape Decoder에 통과시켜 표준 모양 s_k를 얻는다. 
    - Offset/Scale 적용 (L_s)
        - 표준 모양 s_k는 아직 "정규화된 모양" 그 자체임.
        - Attribute Decoder가 예측한 Offset(μ_k), Scale(σ_k)을 적용해서 s′_k = μ_k + σ_k·s_k 로 s_k를 보정한다. (s'_k 얻음.)
        - 이제, Attribute Decoder가 예측한 시작 구간(t_k)과, 구간 길이(l_k)를 이용해서 원본 time-series의 subsequence를 가져온다.
        - 가져온 subsequence는, Shape Decoder로 얻은 z'_k의 length에 맞게 interpolation된다. → 그렇지 않으면, 서로 loss를 계산할 수 없고, z'_k를 subsequence에 맞추기에는, codebook code의 일반화 능력(일관성)이 사라짐.
        - 모델이 patch를 보고 예측한 "이 patch는 원본 time-series의 t, l에 대한 subsequence이고, 대체적인 codebook code의 표현으로는 z일 것이야. 그리고, 그것을 Shape Decoder에 넣은 뒤 μ, σ의 보정을 거치면 그 subsequence와 비슷하게 되겠지?" 라는 학습을 수행하게 된다. → L_s는 z'_k와 interpolation된 subsequence를 비교한 loss.
    - 위치, 길이의 다양성을 유도하는 L_div
        - (t, l)은 원래라면, positional encoding과 patch length를 알고 있기 때문에, 그냥 알 수 있고, 이를 Ground Truth로 해서 Supervised Learning을 시킬 수 있다.
        - 하지만, 이것보다 Self-supervised learning의 방식을 채택하는 것이 z_k, offset, scale, shpae decoder 등 모두를 학습하는데에 더 큰 도움이 되기 때문에 t, l도 예측하도록 시키는 것임.
        - 하지만, 이렇게 했을 때 반복 패턴이 많거나, 짧은 길이에 대해서 반복적으로 모델이 예측을 하는 현상이 있어서, 이것을 막기 위해 L_div를 넣음.
        - L_div는 각 패치마다 예측된 t, l이 서로 겹치는 부분이 적어지도록 하는 Loss이다. 이를 통해서 각 patch들이 시계열 전체의 넓은 범위를 커버할 수 있게 한다.
- 8. 마지막 재구성 손실을 얻기위한 복원.
    - Latent Space Operation 전의 tau에는 Attribute Decoder를 통해 예측된 z​,μ,σ​,t​,l 이 존재했는데, 여기서 z만 codebook의 가장 가까운 code로 치환한다.
    - 그렇게 얻어진 새로운 tau를 Attribute Encoder → Time-series Decoder 를 통과시켜 원본 시계열을 복원한다.
    - 이렇게 복원된 Time-series와, 원본 Time-series를 비교해서 마지막 L_x를 얻는다.
    - 이렇게 하면, 이 최종 L_x를 통해 Attribute Encoder, Time-series Decoder뿐 아니라, codebook code의 표현력까지도 강화가 될듯?

### 그럼 이걸 어떻게 Classification에 사용함?
- 데이터셋에 대해서 Codebook, Linear Encoder, Time-series Encoder, Attribute Decoder 등을 미리 학습함. 
    - 위의 과정을 통해서 학습을 해서, 이 데이터셋에 대해 최적의 Codebook, Time-series Encoder, Attribute Decoder 등의 것들이 만들어짐.
- 이렇게 학습된 것들을 가지고, 예측할 Time-sereis X를 통과시킨다.
    - 이렇게 쭉 아키텍처를 순차적으로 지나, Attribute Decoder까지 통과하면 τ = (z​,μ​,σ​,t​,l​)까지 얻는다.
    - 여기서 z는 학습때와 똑같이 codebook의 code들과 거리를 비교해서 가장 가까운 code e와 치환됨.
    - 학습 때였으면 이렇게 치환된 τ'을 복원한 후 reconstruction loss를 구했을 텐데, 예측때에는 딱 이 τ'을 만드는 것 까지함.
- 최종 예측
    - 최종 예측 때에는, 이렇게 얻어진 τ'을 Feature로 하여, Classifier에 넣어서 최종 예측을 수행한다.
    - 즉, 예측 때 각 Time-series를 Patch 단위로 분해하고, 각 Patch들을 codebook의 코드로 치환하는데, 그 치환된 code들이 어떤 class에 부합한지 분류하는 느낌으로 예측을 수행한다.? 
    - 예측할 Time-series X를 Patch 단위로 분해하고, 각 Patch마다 codebook내의 code로 치환하는데, 그 codebook의 code들을 attribute로 모양을 만들고, 조합했을 때 "이 code들의 조합은 이 class를 의미하는구나" 라는 느낌으로 학습하게 됨.
    - 이런 점에서, 각 codebook내의 code들은 특정 interpretable한 특성을 가진다고 볼 수 있고, 모든 codebook들을 Shape Decoder를 통하게 하면, 이거를 직접 사람이 눈으로도 확인할 수 있어서 interpretable하다는 것이다.

### github 코드를 보며, 어떤 부분이 어떠한지까지도 알 수 있으면 좋을텐데, 이건 좀 걸릴듯
- 