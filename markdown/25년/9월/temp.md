## Introduction
1. Human activity recognition based on wearable sensor data has been widely applied in domains such as healthcare, medical monitoring, smart homes, and abnormal detection for security. To meet these demands, numerous studies have explored deep learning approaches to enhance the accuracy and robustness of wearable sensor–based activity recognition. However, there are still challenges: raw wearable sensor data have inherently limited information, show variability between individuals in activity patterns, involve difficulty in handling sensor noise, and exhibit high sensitivity to segmentation and sampling strategies.

2. To address these limitations, recent studies have explored transforming raw time-series data from wearable sensors into image-based representations, which have achieved promising results. In particular, persistence images (PI) and gramian angular fields (GAF) provide rich contextual information and capture high-dimensional structures that cannot be obtained from raw data alone. PI encodes topological features that remain invariant under transformations such as stretching, bending, or rotation, thereby reflecting global and intrinsic properties of the data. These features can be extracted through topological data analysis (TDA), which preserves the essential shape characteristics of the time-series. In contrast, GAF represents pairwise similarities between time points by mapping them into polar coordinates, enabling the visualization of temporal correlations and periodic patterns. These image representations effectively capture more complex and informative patterns within time-series. Moreover, they can serve as useful inputs for feature extraction when directly applied to deep neural networks. However, generating IRs requires additional computational time and memory, which can lead to degraded performance on resource-constrained wearable devices.

3. To further address these limitations, knowledge distillation (KD) has been adopted as an effective strategy for building lightweight models. In this paradigm, a smaller student model learns from the guidance of a larger teacher model, thereby distilling its knowledge in a compressed form. Beyond the conventional single-teacher setting, multiple teachers can be employed to provide complementary supervision, producing a more robust student. Moreover, KD can be extended to multimodal teachers, where diverse image-based models transfer knowledge to a time-series student model. Such approaches not only enhance the generalization capability of the student but also improve its robustness against noise. In addition, the supplementary features derived from IRs convey information that raw time-series data cannot capture, thereby enhancing the interpretability of the student through the distillation process. Previous works have examined KD using IRs alongside raw signals, but little attention has been paid to the potential of combining multiple, heterogeneous IRs within a unified distillation framework.

4. In this paper, we investigate knowledge distillation with image representations to develop more effective lightweight models for human activity recognition using wearable sensor data. First, we generate PI and GAF representations and train them from scratch to examine the role and individual performance of each modality. Second, we evaluate various KD strategies that combine IRs with raw time-series data, assessing the effectiveness of each IR-driven distillation setting. Finally, we explore a three-modality setting where PI, GAF, and time-series data are jointly incorporated into the KD process. In addition, we examine both cases with and without feature-based distillation, providing insights into the compatibility of each IR with different KD strategies, as well as the interplay when multiple IRs are used together. Through these experiments, we aim to answer whether greater informational richness consistently leads to better outcomes, offering practical guidance for designing efficient and high-performing systems for wearable sensor analysis. We further demonstrate the performance of distilled lightweight models in terms of both inference time and accuracy, and compare models trained with time-series only, single IR + time-series, and multiple IRs + time-series configurations.

## Background
Topological Data Analysis (TDA) is a mathematical framework designed to extract "complex" and "informative" topological features from data \cite{}. "these complementary information을 통해서 여러 분야에서 성능을 올리기 위한 연구들이 진행되고 있다." 

To extract PI, one-dimensional time-series are projected into an $n$-dimensional point cloud using a sliding window method \cite{}. Persistent homology is then computed on this point cloud. Points within a distance $\epsilon$ are connected to form a simplicial complex. As the radius $\epsilon$ increases, the complex grows and topological features—such as connected components, loops, and voids—appear and disappear, with their lifetimes defined by birth and death times. This multiscale process is referred to as a filtration \cite{}. The resulting birth–death pairs are summarized in a persistence diagram (PD), which remains stable under small perturbations of the input \cite{}. However, since PDs are unordered and vary in size, they are not straightforward to use directly in machine learning or deep learning models.

To generate a fixed-size representation, PDs are converted into persistence images (PIs). The construction of PIs begins with a persistence surface, created by placing a weighted Gaussian at each point in the PD and summing them. This surface is then discretized on a regular grid, where the value of each grid cell is calculated to form a pixel matrix, namely the PI. Higher pixel values correspond to stronger persistence.

Through this procedure, PIs can provide richer information that complements the original time-series data. However, the extraction of PIs requires considerable computational time and memory, and is typically carried out on CPUs. This creates difficulties for deploying deep learning models and for use on small devices. Therefore, in this study, TDA is applied only on the teacher side to supply additional knowledge beneficial for distillation, so that the student does not require TDA during either training or inference. Figure \ref{figure:PD_PI} illustrates an example of time-series data along with its corresponding PD and PI.