## 이번주 하려고 하는 것.
<li> PAMAP2 데이터셋에 대해서 2Teacher 학습 진행  </li>
<li> 각각의 alpha값(0.3, 0.5, 0.7)에 대해서 wrn163, wrn281에서만 수행을 할 예정. </li>
<li> VLM이랑 LLM이 무엇인지 찍먹해보자. </li>

## PAMAP2에서 각각 alpha 값 (0.3, 0.5, 0.7) + lambda = 0.99 에 대해서 값 비교.
- wrn163, wrn281에 대해서만 3trial 평균으로 성능을 비교.

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.4.25/PAMAP_result.png" alt="results" width="700">

## VLM과 LLM이 무엇인지 찍먹해보기.
- https://v.daum.net/v/20250409080013693 - VLM과 LLM에 대한 내용을 설명해주는 뉴스기사인듯
- https://www.ibm.com/kr-ko/think/topics/vision-language-models - VLM에 대해서 자세히(?) 설명해주는 블로그같은거
- [https://aiflower.tistory.com/37 - ](https://aiflower.tistory.com/19) - What matters when building vision-language models? 라는 논문 설명해주는 블로그.


## 첫 번째 뉴스기사
- 이거로 LLM과 VLM이 뭘 하는 얘인지, 기초적으로 배경지식을 얻는것이 목표
- 구글에서 2017년에 트랜스포머라는 LM(Language Model)을 개발함.
- 처음에는 잘 안됐고, 실험적이었는데, 구조를 튜닝하면서 성능이 점점 좋아지고, 혁신적으로 좋아짐.
- 이전까지의 언어 모델은 문장을 한 단어씩 읽어서 그에 대한 한계가 존재했는데, 트랜스포머는 문장의 모든 단어를 한번에 읽어서 빠르게 동작하고, 단어들 사이의 연관성 및 문맥 파악이 잘됨. → 복잡하고 긴 문장에 대한 학습 가능.
- 이후로 트랜스포머 기반의 LLM이 계속 발전해온게 GPT랑 Gemini, LLaMA 임. 각각 수십억 ~ 수조 개 까지의 파라미터를 가짐.
---
- LLM의 기반 모델로 사용되던 트랜스포머가 그 기능을 이미지 이해와 생성 분야로 확장하게 됨. → 이것이 Vision-Language Model(VLM) 이 되었다.
- VLM은 이미지와 텍스트를 동시에 처리하는 멀티-모달(Multi-Modal) 작업을 수행함.
- 입출력의 관점에서 LLM은 Text → Text 이고, VLM은 Text + Image → Text가 된다.
- GPT에서는 이러한 멀티모달 입력 (Text + Image + Audio)이 되는게 GPT-4o 부터였다고 함.

## 두 번째 VLM이 뭔지 설명해주는 블로그
- VLM은 컴퓨팅 비전과 자연어 처리 기능을 결합한 인공지능 모델임.
- LLM과 비전 모델(시각적 알고리즘)을 결합한 것.

### VLM 모델의 요소
1. 언어 인코더 (Language Encoder?)
- 단어와 구문 간의 의미적 연관성과 문맥적 연관성을 포착하여 AI 모델이 처리할 수 있는 텍스트 임베딩으로 변환하는 역할.
- 대부분의 VLM은 언어 Encoder로 트랜스포머 모델 아키텍처를 사용한다.
- 트랜스포머 아키텍처
<ul>
<li> Encoder는 Input Sequence를 Embedding이라는 숫자 표현으로 변환하여 Input Sequence에서 Token의 의미와 위치(Embedding 후에 Positional Encoding)를 확인함. </li>
<li> ㄴ> Self-Attention을 통해 위치 정보를 고려하여 Input Sequence에서 가장 중요한 Token에 Attention Score를 높게 부여함. </li>
<li> Decoder는 Self-Attention과 Encoder 출력을 활용한 Cross-Attention을 통해, 가장 높은 확률을 가진 Output Sequence를 생성한다. </li>
<li> Decoder에서의 Self-Attention은 지금까지 생성한 단어들 간의 관계를 파악하는데 사용. Cross-Attention은 Encoder Output을 참고하여 문맥을 보완하는데 사용. </li>
</ul>

2. Vision Encoder
- Image or Video Input에서 색상, 모양, 텍스처와 같은 중요한 시각적 속성을 추출하여 머신러닝 모델이 처리할 수 있는 여러 개의 Embedding Vector로 변환한다.
- 이전에는 Conv Net을 사용해서 저러한 Feature들을 추출했지만, 요즘에는 Vision Transformer(ViT)를 사용한다.
- Vision Transformer에서는 이미지를 고정 크기의 Patch로 나눈 후, 각 Patch를 Flatten → Embedding 하여 Language Transformer에서 Token을 다루듯 Sequence로 처리한다.
- 그 다음 ViT는 이러한 Patch Embedding들에 대해 Self-Attention을 수행하여, 입력 이미지의 Transformer 기반 표현을 생성한다.
- 즉, 이미지 입력을 Patch → Embedding 하면, 그 이후로는 기존 Transformer와 동일한 구조(블록 조합이 다를 수 있다고 함.)을 거쳐서 결과를 출력하기에 멀티 모달이 가능해진 다는 것.

### VLM 모델의 학습은 어떻게 되나?
- VLM 모델은 Vision Encoder와 Language Encoder의 정보를 정렬하고, 융합하여 이미지와 텍스트의 상관관계를 학습하게 되고, 두 가지 Modality를 함께 판단할 수 있도록 학습이 진행된다.
<li> 다음과 같은 접근 방식을 사용함.</li>

#### 대조 학습
- 두 Encoder의 Image와 Text Embedding을 Shared Embedding Space에 매핑한다.
- VLM은 Image-Text Pair 데이터 세트를 학습하여 일치하는 쌍의 Embedding Distance는 최소화하고, 일치하지 않는 쌍의 Embedding Distance는 최대화 되도록 훈련된다.
- 일반적으로 대조 언어-이미지 학습(CLIP)을 사용함 인터넷에서 가져온 4억 개의 이미지-캡션 쌍을 사용해서 훈련된다고 한다.

#### 마스킹
- VLM이 Text 또는 Image에서 무작위로 가려진 부분을 예측하는 방식으로 학습하는 기법임.
- 예를 들어 마스킹되지 않은 이미지를 보고, 텍스트 캡션의 가려진 부분을 예측하는 방식으로 학습됨.
- 반대로, 마스킹되지 않은 캡션을 보고, 이미지의 가려진 픽셀을 재구성 하는 방식으로 학습이 된다.

#### 생성형 모델 학습
- 새로운 데이터를 생성하는 방법을 학습하는 것.
- 텍스트-이미지 생성은 입력 텍스트에서 이미지를 생성.
- 이미지-텍스트 생성은 입력 이미지에서 캡션, 이미지 설명 or 요약같은 텍스트 생성.

#### 사전 학습된 모델
- VLM을 처음부터 학습시키는 것은 비용이 많이 들어서 말이 안됨.
- 사전 학습된 모델을 활용해서 VLM을 구축하는게 맞다.
- 사전 학습된 LLM과 Vision Encoder를 사용할 수 있으며, 이미지의 시각적 표현을 LLM의 입력 공간에 투영하는 매핑 네트워크 레이어가 추가된다.
- VLM을 위한 고품질 학습 데이터를 수집하는것이 번거로우나, 기존 데이터셋을 사용해서 보다 구체적인 Down Stream 작업을 위한 사전학습, 최적화 및 미세 조정이 가능하다.

### VLM 사용 사례
- 캡션 및 요약
- 이미지 생성
- 이미지 검색 및 조회
- 이미지 분할
- 객체 감지
- 시각적 질의 응답


### VLM이 더 발전해야하는 부분
#### 편향
- VLM이 학습 데이터에 포함된 현실 세계의 편향이나 pretrained된 모델의 편향을 학습하게 되는 경우.
- 다양한 데이터를 이용하여 이러한 편향을 줄여야 함.

#### 비용 및 복잡성
- Vision Model과 LLM은 이미 충분히 각각 매우 비용이 크고, 복잡한 모델임.
- 이러한 것을 병합하려고 하면 그 비용과 복잡성이 더욱 증가할 것이다.

#### 일반화
- VLM은 이전에 본 적 없는 새로운 데이터에 적응하고 정확하게 예측하는 일반화 능력에 한계를 보일 수 있다.
- 각 데이터가 정형화 되어 있거나, 도메인이 제한적일 수 있어서
- Transformer 구조에서 추론에 사용하는 Cross-Attention은 결국 학습된 Attention Weight를 기반으로 동작하기 때문에.

#### AI 할루시네이션
- VLM이 Text를 생성할 때 다음 단어가 나올 확률이 가장 높은 문장을 생성하기 때문에 실제로 이미지에 없는 정보일지라도, 자연스러운 문장을 출력하게 됨.
- 즉, 고양이가 비행기 위에 누워있는 이미지를 보고(특이한 상황) → 고양이가 창가에 누워있음 (자연스러운 문장) 이렇게 출력이 되는 현상이 발생.
- 또한 VLM은 이미지를 Patch(조각) 단위로 나눈 뒤, Attention을 통해 전체 의미를 조합하는데, 이미지의 일부가 불분명하거나, 해당 사물에 대한 훈련이 부족한 경우, 모델은 추측 기반의 묘사를 하게 된다. → 생성형 AI 과제때 생각해보면 프롬프트를 아무리 자세하게 작성해도 갑자기 이상한게 튀어나오거나 하는게 이런 이유인듯 하다.

---
## What matters when building vision-language models?
- VLM의 구조 설계, 학습 방식, 데이터 처리 등에서 실제로 중요한 요소가 무엇인지 실증적으로 분석한 리뷰 논문이라고 함.
- 선행해서 봐야할 것.
<li> Vision Transformer 와 Patch Embedding 개념</li>
<li> BERT / GPT 기반의 Text Encoder 개념</li>
<li> Cross-Attention 과 Multimodal Fusion 방식</li>
<li> CLIP, ALIGN 등 대표 VLM의 구성 구조.</li>

## Transformer 공부
[Transformer](https://github.com/wjdwocks/ML-DNN/blob/main/markdown/my_study/Transformer.md)

## Vision Transformer 공부
[ViT](https://github.com/wjdwocks/ML-DNN/blob/main/markdown/my_study/VisionTransformer.md)