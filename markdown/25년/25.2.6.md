### 해야 할 것
<li> 2 Teacher (GAF + Sig) 에 대해서 14cls에 대해서 코드를 만들어야 함. (서버에서 돌아가도록 공부해서 만들기.) (완) </li>
<li> 14cls에 대해서는 161-161, 163-161, 281-161, 283-161 조합에 대해 모두 해봐야함. (완) </li>
<li> alpha값 (KD1과 KD2 간의 비중을 0.1, 0.3, 0.5, 0.7, 0.9에 대해서 비교해보기 (3번씩 계산 후 평균 + 표준편차)) (하는중 하루이틀정도 걸릴듯) </li>
<li> 이제는 7cls에 대해서도 GAF를 만들고, GAF Teacher를 만들고, 지금까지 했던 것들을 모두 똑같이 해야 함. (완) </li>
<li> 7cls-500w, 7cls-1000w 에 대해서 각각 GAF만들고, GAF Teacher 학습하고(vanila), 1 Teacher(GAF) KD도 결과를 내보는 것이 목표. (완) </li>
<li> TPKD 논문 9 ~ 11p 부분을 보고, 14cls와 7cls의 결과를 확인하며 비교해보자. (오케이) </li>
<li> PAMAP2 데이터셋에 대해서도 GAF로 변환하고, 위와같은 작업을 수행해야 함. </li>

### 지금까지 결과 추가 (2-Teacher, 14cls)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 69.7864 </li>
<li> 70.6720 </li>
<li> 70.3247 </li>
<li> 평균 : 70.2610 </li>
<li> 표준편차 : 0.3643 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.1511 </li>
<li> 70.3421 </li>
<li> 70.5504 </li>
<li> 평균 : 70.3479 </li>
<li> 표준편차 : 0.1631 </li>
<br>
</ul>

<li> T1 : WRN28-1(GAF), T2 : WRN28-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 69.2134 </li>
<li> 70.3073 </li>
<li> 69.7517 </li>
<li> 평균 : 69.7575 </li>
<li> 표준편차 : 0.4466 </li>
<br>
</ul>

<li> T1 : WRN28-3(GAF), T2 : WRN28-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.5504 </li>
<li> 70.3247 </li>
<li> 69.5781 </li>
<li> 평균 : 70.1511 </li>
<li> 표준편차 : 0.4155 </li>
<br>
</ul>

### 지금까지 결과 추가 (Teacher Network, 7cls, 500w)
<li> Single : WRN161 </li>
<ul>
<li> 84.5852 </li>
<li> 84.8465 </li>
<li> 85.0425 </li>
<li> 평균 : 84.8247 </li>
<li> 표준편차 : 0.1873 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4017 </li>
<li> 85.6956 </li>
<li> 86.1855 </li>
<li> 평균 : 85.7609 </li>
<li> 표준편차 : 0.3233 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 85.4344 </li>
<li> 84.7159 </li>
<li> 85.0098 </li>
<li> 평균 : 85.0534 </li>
<li> 표준편차 : 0.2949 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.8060 </li>
<li> 87.1326 </li>
<li> 86.0222 </li>
<li> 평균 : 86.6536 </li>
<li> 표준편차 : 0.4660 </li>
<br>
</ul>

### 지금까지 결과 추가 (Teacher Network, 7cls, 1000w)
<li> Single : WRN161 </li>
<ul>
<li> 84.2913 </li>
<li> 84.9118 </li>
<li> 84.6179 </li>
<li> 평균 : 84.6070 </li>
<li> 표준편차 : 0.2534 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4997 </li>
<li> 86.4468 </li>
<li> 86.4468 </li>
<li> 평균 : 86.1311 </li>
<li> 표준편차 : 0.4465 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 84.8465 </li>
<li> 84.8792 </li>
<li> 85.1078 </li>
<li> 평균 : 84.9445 </li>
<li> 표준편차 : 0.1162 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.4794 </li>
<li> 86.6101 </li>
<li> 85.9569 </li>
<li> 평균 :  </li>
<li> 표준편차 :  </li>
<br>
</ul>

### 지금까지 결과 추가 (1 Teacher KD, 7cls, 500w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.3860 </li>
<li> 89.6473 </li>
<li> 89.7453 </li>
<li> 평균 : 89.5929 </li>
<li> 표준편차 : 0.1516 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.3534 </li>
<li> 88.7002 </li>
<li> 89.0268 </li>
<li> 평균 : 89.0268 </li>
<li> 표준편차 : 0.2667 </li>
<br>
</ul>

### 지금까지 결과 추가 (1 Teacher KD, 7cls, 1000w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.8106 </li>
<li> 90.3005 </li>
<li> 90.1045 </li>
<li> 평균 : 90.0719 </li>
<li> 표준편차 : 0.2013 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.9412 </li>
<li> 89.8106 </li>
<li> 89.1574 </li>
<li> 평균 : 89.6364 </li>
<li> 표준편차 : 0.3429 </li>
<br>
</ul>


### 지금까지 결과 추가 (2 Teacher KD, 7cls, 500w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 91.0516 </li>
<li> 89.5820 </li>
<li> 89.9412 </li>
<li> 평균 : 90.1916 </li>
<li> 표준편차 : 0.6255 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.6924 </li>
<li> 90.4637 </li>
<li> 90.3658 </li>
<li> 평균 : 90.5073 </li>
<li> 표준편차 : 0.1369 </li>
<br>
</ul>


### 지금까지 결과 추가 (2 Teacher KD, 7cls, 1000w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 90.0392 </li>
<li> 89.0268 </li>
<li> 89.3860 </li>
<li> 평균 : 89.4840 </li>
<li> 표준편차 : 0.4191 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.0065 </li>
<li> 90.4311 </li>
<li> 89.7779 </li>
<li> 평균 : 90.0718 </li>
<li> 표준편차 : 0.2706 </li>
<br>
</ul>

### 500w, 14cls에 대한 실험 결과
| Experiment (alpha = 0.3, 500w, 14cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| WRN16-1 (GAF 단일 네트워크, 500w, 14cls)                                                      | 63.0491  | 63.3270  | 62.9797  | <u>**63.1186**</u> | 0.1501  |
| WRN16-3 (GAF 단일 네트워크, 500w, 14cls)                                                      | 64.0389  | 63.9521  | 63.9173  | <u>**63.9694**</u> | 0.0511  |
| WRN28-1 (GAF 단일 네트워크, 500w, 14cls)                                                      | 63.2922  | 64.0042  | 63.3096  | <u>**63.5353**</u> | 0.3316  |
| WRN28-3 (GAF 단일 네트워크, 500w, 14cls)                                                      | 64.1604  | 64.8029  | 65.3933  | <u>**64.7855**</u> | 0.5035  |
| T: WRN16-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.9703  | 67.6680  | 67.0429  | <u>**67.8937**</u> | 0.8029  |
| T: WRN16-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.2063  | 67.7201  | 68.3278  | <u>**68.0847**</u> | 0.2626  |
| T: WRN28-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.0082  | 66.4004  | 67.0603  | <u>**66.8230**</u> | 0.2996  |
| T: WRN28-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.5464  | 66.4872  | 67.7027  | <u>**67.2454**</u> | 0.5399  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.7864  | 70.6720  | 70.3247  | <u>**70.2610**</u> | 0.3643  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.1511  | 70.3421  | 70.5504  | <u>**70.3479**</u> | 0.1631  |
| T1: WRN28-1(GAF), T2: WRN28-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.2134  | 70.3073  | 69.7517  | <u>**69.7575**</u> | 0.4466  |
| T1: WRN28-3(GAF), T2: WRN28-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.5504  | 70.3247  | 69.5781  | <u>**70.1511**</u> | 0.4155  |

### 500w, 7cls에 대한 실험 결과
| Experiment (alpha = 0.3, 500w, 7cls)                                                         | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 500w)                                                 | 84.5852  | 84.8465  | 85.0425  | <u>**84.8247**</u> | 0.1873  |
| Single: WRN163 (Teacher Network, 7cls, 500w)                                                 | 85.4017  | 85.6956  | 86.1855  | <u>**85.7609**</u> | 0.3233  |
| Single: WRN281 (Teacher Network, 7cls, 500w)                                                 | 85.4344  | 84.7159  | 85.0098  | <u>**85.0534**</u> | 0.2949  |
| Single: WRN283 (Teacher Network, 7cls, 500w)                                                 | 86.8060  | 87.1326  | 86.0222  | <u>**86.6536**</u> | 0.4660  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.8106  | 90.3005  | 90.1045  | <u>**90.0719**</u> | 0.2013  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.9412  | 89.8106  | 89.1574  | <u>**89.6364**</u> | 0.3429  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0392  | 89.0268  | 89.3860  | <u>**89.4840**</u> | 0.4191  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0065  | 90.4311  | 89.7779  | <u>**90.0718**</u> | 0.2706  |

### 1000w, 7cls에 대한 실험 결과
| Experiment (alpha = 0.3, 1000w, 7cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 1000w)                                                | 84.2913  | 84.9118  | 84.6179  | <u>**84.6070**</u> | 0.2534  |
| Single: WRN163 (Teacher Network, 7cls, 1000w)                                                | 85.4997  | 86.4468  | 86.4468  | <u>**86.1311**</u> | 0.4465  |
| Single: WRN281 (Teacher Network, 7cls, 1000w)                                                | 84.8465  | 84.8792  | 85.1078  | <u>**84.9445**</u> | 0.1162  |
| Single: WRN283 (Teacher Network, 7cls, 1000w)                                                | 86.4794  | 86.6101  | 85.9569  | <u>**86.3488**</u> | 0.2822  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3860  | 89.6473  | 89.7453  | <u>**89.5929**</u> | 0.1516  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3534  | 88.7002  | 89.0268  | <u>**89.0268**</u> | 0.2667  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 91.0516  | 89.5820  | 89.9412  | <u>**90.1916**</u> | 0.6255  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 90.6924  | 90.4637  | 90.3658  | <u>**90.5073**</u> | 0.1369  |

### 추가실험
<li> GENE_Active 14cls, 500w 데이터에 대해 (T : WRN163, S : WRN161)과 (T : WRN281, S : WRN161)에 대해서 추가 실험을 진행함 (높은 것을 최대한 뽑아보기 위해서.) </li>
<li> T : WRN163, S : WRN 161 - 68.1195, 67.1992, 67.9285, 68.6578, 67.1644, 67.5638 </li>
<li> T : WRN281, S : WRN 161 - 67.0255, 67.1297, 67.4423, 67.5291, 66.2441 </li>


### alpha(두 Teacher간 KD_loss의 비중)을 바꿔가며 실험한 결과
<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>asdasdasd
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>asdasd
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>


# Transformer 공부 Attention is All You Need 논문
## 1️⃣ Transformer가 등장한 이유
### ✅ 기존 RNN, LSTM의 한계점
1. **긴 문장을 다룰 때 정보 손실 문제**
   - RNN, LSTM은 **입력을 순차적으로 처리**하므로 **긴 문장에서 앞쪽 정보가 뒤쪽에 잘 전달되지 않음**  
   - (Long-Term Dependency 문제 발생)
   
2. **병렬 연산 불가능 → 학습 속도 느림**
   - RNN/LSTM은 순차적으로 학습되므로, **GPU 병렬 연산이 어려움 → 학습 속도가 느림**
   
3. **Gradient Vanishing 문제**
   - 깊은 네트워크에서 **초반 입력의 Gradient가 사라지는 문제 발생 → 학습 어려움**

---

### ✅ Transformer는 어떻게 해결했나?
1. **Attention Mechanism (Self-Attention) 활용**
   - 모든 단어가 **다른 단어와 얼마나 연관이 있는지 동시에 계산**  
   - → **멀리 떨어진 단어들 간의 관계도 효과적으로 학습 가능**
   
2. **순차 처리(X), 병렬 연산(O) 가능**
   - 모든 단어를 **동시에 처리 가능 → GPU 가속 최적화**
   
3. **Residual Connection 사용 → Gradient Vanishing 방지**
   - 각 Layer에서 **원본 입력을 더해줌** → 정보 손실 방지

---

## 2️⃣ Transformer의 주요 구성 요소
1. **Positional Encoding** → Input Embedding 시 단어 순서 정보를 반영
2. **Multi-Head Self-Attention** → 단어 간의 연관성을 여러 Head를 통해 다양한 관점에서 학습
3. **Feed Forward Network (FFN)** → 비선형 변환을 통해 표현력 증가 + 깊이 있는 학습 가능
4. **Residual Connection + Layer Normalization** → 안정적인 학습

---

# 3️⃣ Transformer의 Encoder 동작 과정 (입력 → Context 벡터 생성)

### ✅ 예제: 입력 문장
> **"I am a teacher" (영어) → "나는 선생님이다" (한글)로 번역한다고 가정**

### 🟢 Step 1: Input Embedding + Positional Encoding
- 각 단어를 **Embedding 벡터(고차원 표현)로 변환**
- Transformer는 단어의 순서를 알 수 없기 때문에 **Positional Encoding 추가**
- 최종 입력 벡터 = `Embedding Matrix + Positional Encoding`

---

### 🟢 Step 2: Multi-Head Self-Attention (자기 자신에게 Attention)
> **"I am a teacher" 문장에서 단어들 간의 연관성을 학습**

✅ **Query, Key, Value 행렬 생성**
- Self-Attention을 수행하여 **각 단어가 문장 내 다른 단어와 얼마나 관련 있는지 계산**

✅ **Scaled Dot-Product Attention 수행**
- Attention Score 계산: Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V
- Softmax를 적용하여 **모든 단어의 중요도를 확률적으로 정규화하고, 각 중요도에 따라 가중치를 부여**
- **모든 단어의 Value를 가중합하여 새로운 벡터를 생성**

✅ **Multi-Head Attention (h개로 분할하여 다른 의미 학습)**
- 각 Head는 **서로 다른 문맥 정보를 학습** (CNN의 필터처럼)
- 여러 개의 Attention 결과를 Concat하여 최종 Attention 벡터 생성

---

### 🟢 Step 3: Feed Forward Network (FFN)
> **Self-Attention을 거친 벡터를 더욱 정제된 표현으로 변환**

✅ **비선형 변환을 추가하여 표현력을 증가**
- FFN(X) = max(0, X * W1 + b1) * W2 + b2
- CNN과 DNN의 활성화 함수처럼, FFN에서도 비선형성을 추가하여 모델의 표현력을 증가시킴
- 더 풍부한 특징을 학습할 수 있도록 변환

✅ **Residual Connection과 Layer Normalization을 수행하여 정보 손실 방지**

---

# 4️⃣ Transformer의 Decoder 동작 과정 (Context 벡터 → 문장 생성)

### ✅ Decoder의 입력
- 훈련할 때는 **정답 문장의 앞부분**이 Decoder의 입력이 됨
- 예측할 때는 **Decoder가 예측한 단어를 입력으로 사용**
- 예) `"<SOS> 나는"` → `"나는 선생님"` → `"나는 선생님이다"`

---

### 🟢 Step 1: Decoder Input Embedding + Positional Encoding
- Decoder도 입력을 **Embedding 벡터로 변환 후, Positional Encoding을 추가**
- `"<SOS> 나는"` → Embedding → Positional Encoding 추가

---

### 🟢 Step 2: Masked Multi-Head Self-Attention
> **Decoder가 "나는"을 예측할 때, "선생님"을 보지 못하도록 Masking 적용(Training 과정에서)**

✅ **Masked Self-Attention이란?**
- 현재까지 생성된 단어까지만 고려하도록 Future Token을 가리는 Mask 적용
- Softmax 연산 시, 미래 단어는 **-∞ (무한대로 낮은 값)** 처리하여 고려되지 않도록 함

---

### 🟢 Step 3: Encoder-Decoder Attention
> **Encoder에서 생성한 Context 벡터를 참고하여, 다음 단어를 예측**

✅ **Decoder의 Query, Encoder의 Key, Value 사용**
- Query: Decoder에서 Self-Attention을 수행한 후 벡터
- Key & Value: Encoder의 최종 출력 (입력 문장의 Context 벡터) # **Encoder의 Value를 사용한다는것은 꼭 기억해야 함.(Value는 Encoder의 정보임)**
- Decoder의 Query와 Encoder의 Key를 Attention 하여 나온 결과와 Encoder의 Value를 가중합한 결과를 FFN에 통과시킨 뒤 가장 가까운 단어를 찾으면 그것이 Predict 단어가 된다.

✅ **Encoder에서 얻은 Context 정보를 활용하여 더욱 정확한 단어 예측**
- `"나는"`이 등장했을 때, `"선생님"`이 나올 확률이 높아지도록 조정됨

---

### 🟢 Step 4: Feed Forward Network (FFN)
> **Attention을 수행한 벡터를 더욱 정제된 표현으로 변환**

✅ **Encoder와 동일한 FFN 적용**
- ReLU 활성화 함수 + 두 개의 Linear Layer 사용
- 최종적으로 문맥 정보를 더욱 잘 반영한 벡터를 생성

---

### 🟢 Step 5: Softmax를 통해 단어 예측
> **Decoder의 최종 출력을 Softmax에 통과시켜 가장 적절한 단어를 예측**

✅ **Softmax를 통해 확률 분포 계산**
- P(선생님 | 나는) = exp(z_선생님) / sum(exp(z_i))
- 가장 확률이 높은 단어 `"선생님"`을 선택

✅ **이전까지 생성된 단어를 입력으로 넣고 반복하여 최종 문장 생성**
- `"나는 선생님"` → `"나는 선생님이다"`

---

# Transformer 전체 과정 요약
1. **Encoder**
 - 입력 문장을 **벡터로 변환**
 - Self-Attention을 수행하여 단어 간 관계 학습
 - FFN을 통해 더욱 정제된 벡터 생성

2. **Decoder**
 - 이전까지 생성된 단어를 입력으로 사용
 - Masked Self-Attention 수행
 - Encoder-Decoder Attention을 통해 입력 문장 정보 반영
 - FFN을 통해 최적의 벡터 생성 후 Softmax를 통해 단어 예측

---

# ✅ 결론
Transformer는 **문장을 효과적으로 표현(Encoding)하고, 이를 기반으로 새로운 문장을 자연스럽게 생성(Decoding)하는 모델**이다.
- **Self-Attention** → 문맥을 반영한 벡터 생성
- **Multi-Head Attention** → 다양한 의미를 학습
- **Residual Connection & FFN** → 정보 손실 방지 & 표현력 증가

🚀 **즉, "입력 문장을 벡터로 변환 → 변환된 벡터를 바탕으로 새로운 문장을 생성"하는 것이 Transformer의 핵심 원리!** 🚀


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>