### 해야 할 것
<li> 2 Teacher (GAF + Sig) 에 대해서 14cls에 대해서 코드를 만들어야 함. (서버에서 돌아가도록 공부해서 만들기.) (완) </li>
<li> 14cls에 대해서는 161-161, 163-161, 281-161, 283-161 조합에 대해 모두 해봐야함. (완) </li>
<li> alpha값 (KD1과 KD2 간의 비중을 0.1, 0.3, 0.5, 0.7, 0.9에 대해서 비교해보기 (3번씩 계산 후 평균 + 표준편차)) (하는중 하루이틀정도 걸릴듯) </li>
<li> 이제는 7cls에 대해서도 GAF를 만들고, GAF Teacher를 만들고, 지금까지 했던 것들을 모두 똑같이 해야 함. (완) </li>
<li> 7cls-500w, 7cls-1000w 에 대해서 각각 GAF만들고, GAF Teacher 학습하고(vanila), 1 Teacher(GAF) KD도 결과를 내보는 것이 목표. (완) </li>
<li> TPKD 논문 9 ~ 11p 부분을 보고, 14cls와 7cls의 결과를 확인하며 비교해보자. (오케이) </li>
<li> PAMAP2 데이터셋에 대해서도 GAF로 변환하고, 위와같은 작업을 수행해야 함. </li>


### 물어볼 것
1. PAMAP2데이터셋에서 30hz와 100hz 데이터의 차이. (둘 다 해봐야하나?) (30hz가 논문에서 말한 33.3hz를 말하는것이라고 함.)
2. Annealing할 때 Student에 어떤 얘를 넣어줄지. (Teacher2에 넣은 그대로 넣는다.)

### 다음주에 할 것.
1. Annealing을 적용한 GENE 14cls, GENE 7cls 결과 내기.
2. Annealing + TPKD(SP Loss를 적용한) GENE 14cls, 7cls 결과내기
3. PAMAP 최대한 진도 빼기 (여기는 엄청오래걸릴듯.)


### 지금까지 결과 추가 (2-Teacher, 14cls)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 69.7864 </li>
<li> 70.6720 </li>
<li> 70.3247 </li>
<li> 평균 : 70.2610 </li>
<li> 표준편차 : 0.3643 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.1511 </li>
<li> 70.3421 </li>
<li> 70.5504 </li>
<li> 평균 : 70.3479 </li>
<li> 표준편차 : 0.1631 </li>
<br>
</ul>

<li> T1 : WRN28-1(GAF), T2 : WRN28-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 69.2134 </li>
<li> 70.3073 </li>
<li> 69.7517 </li>
<li> 평균 : 69.7575 </li>
<li> 표준편차 : 0.4466 </li>
<br>
</ul>

<li> T1 : WRN28-3(GAF), T2 : WRN28-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.5504 </li>
<li> 70.3247 </li>
<li> 69.5781 </li>
<li> 평균 : 70.1511 </li>
<li> 표준편차 : 0.4155 </li>
<br>
</ul>

### 지금까지 결과 추가 (Teacher Network, 7cls, 500w)
<li> Single : WRN161 </li>
<ul>
<li> 84.5852 </li>
<li> 84.8465 </li>
<li> 85.0425 </li>
<li> 평균 : 84.8247 </li>
<li> 표준편차 : 0.1873 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4017 </li>
<li> 85.6956 </li>
<li> 86.1855 </li>
<li> 평균 : 85.7609 </li>
<li> 표준편차 : 0.3233 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 85.4344 </li>
<li> 84.7159 </li>
<li> 85.0098 </li>
<li> 평균 : 85.0534 </li>
<li> 표준편차 : 0.2949 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.8060 </li>
<li> 87.1326 </li>
<li> 86.0222 </li>
<li> 평균 : 86.6536 </li>
<li> 표준편차 : 0.4660 </li>
<br>
</ul>

### 지금까지 결과 추가 (Teacher Network, 7cls, 1000w)
<li> Single : WRN161 </li>
<ul>
<li> 84.2913 </li>
<li> 84.9118 </li>
<li> 84.6179 </li>
<li> 평균 : 84.6070 </li>
<li> 표준편차 : 0.2534 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4997 </li>
<li> 86.4468 </li>
<li> 86.4468 </li>
<li> 평균 : 86.1311 </li>
<li> 표준편차 : 0.4465 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 84.8465 </li>
<li> 84.8792 </li>
<li> 85.1078 </li>
<li> 평균 : 84.9445 </li>
<li> 표준편차 : 0.1162 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.4794 </li>
<li> 86.6101 </li>
<li> 85.9569 </li>
<li> 평균 : 86.3488 </li>
<li> 표준편차 : 0.2822 </li>
<br>
</ul>

### 지금까지 결과 추가 (1 Teacher KD, 7cls, 500w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.3860 </li>
<li> 89.6473 </li>
<li> 89.7453 </li>
<li> 평균 : 89.5929 </li>
<li> 표준편차 : 0.1516 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.3534 </li>
<li> 88.7002 </li>
<li> 89.0268 </li>
<li> 평균 : 89.0268 </li>
<li> 표준편차 : 0.2667 </li>
<br>
</ul>

### 지금까지 결과 추가 (1 Teacher KD, 7cls, 1000w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.8106 </li>
<li> 90.3005 </li>
<li> 90.1045 </li>
<li> 평균 : 90.0719 </li>
<li> 표준편차 : 0.2013 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.9412 </li>
<li> 89.8106 </li>
<li> 89.1574 </li>
<li> 평균 : 89.6364 </li>
<li> 표준편차 : 0.3429 </li>
<br>
</ul>


### 지금까지 결과 추가 (2 Teacher KD, 7cls, 500w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 91.0516 </li>
<li> 89.5820 </li>
<li> 89.9412 </li>
<li> 평균 : 90.1916 </li>
<li> 표준편차 : 0.6255 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.6924 </li>
<li> 90.4637 </li>
<li> 90.3658 </li>
<li> 평균 : 90.5073 </li>
<li> 표준편차 : 0.1369 </li>
<br>
</ul>


### 지금까지 결과 추가 (2 Teacher KD, 7cls, 1000w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 90.0392 </li>
<li> 89.0268 </li>
<li> 89.3860 </li>
<li> 평균 : 89.4840 </li>
<li> 표준편차 : 0.4191 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.0065 </li>
<li> 90.4311 </li>
<li> 89.7779 </li>
<li> 평균 : 90.0718 </li>
<li> 표준편차 : 0.2706 </li>
<br>
</ul>

### 500w, 14cls에 대한 실험 결과
| Experiment (alpha = 0.3, 500w, 14cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| WRN16-1 (GAF 단일 네트워크, 500w, 14cls)                                                      | 63.0491  | 63.3270  | 62.9797  | <u>**63.1186**</u> | 0.1501  |
| WRN16-3 (GAF 단일 네트워크, 500w, 14cls)                                                      | 64.0389  | 63.9521  | 63.9173  | <u>**63.9694**</u> | 0.0511  |
| WRN28-1 (GAF 단일 네트워크, 500w, 14cls)                                                      | 63.2922  | 64.0042  | 63.3096  | <u>**63.5353**</u> | 0.3316  |
| WRN28-3 (GAF 단일 네트워크, 500w, 14cls)                                                      | 64.1604  | 64.8029  | 65.3933  | <u>**64.7855**</u> | 0.5035  |
| T: WRN16-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.9703  | 67.6680  | 67.0429  | <u>**67.8937**</u> | 0.8029  |
| T: WRN16-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.2063  | 67.7201  | 68.3278  | <u>**68.0847**</u> | 0.2626  |
| T: WRN28-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.0082  | 66.4004  | 67.0603  | <u>**66.8230**</u> | 0.2996  |
| T: WRN28-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.5464  | 66.4872  | 67.7027  | <u>**67.2454**</u> | 0.5399  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.7864  | 70.6720  | 70.3247  | <u>**70.2610**</u> | 0.3643  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.1511  | 70.3421  | 70.5504  | <u>**70.3479**</u> | 0.1631  |
| T1: WRN28-1(GAF), T2: WRN28-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.2134  | 70.3073  | 69.7517  | <u>**69.7575**</u> | 0.4466  |
| T1: WRN28-3(GAF), T2: WRN28-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.5504  | 70.3247  | 69.5781  | <u>**70.1511**</u> | 0.4155  |

### 500w, 7cls에 대한 실험 결과
| Experiment (alpha = 0.3, 500w, 7cls)                                                         | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 500w)                                                 | 84.5852  | 84.8465  | 85.0425  | <u>**84.8247**</u> | 0.1873  |
| Single: WRN163 (Teacher Network, 7cls, 500w)                                                 | 85.4017  | 85.6956  | 86.1855  | <u>**85.7609**</u> | 0.3233  |
| Single: WRN281 (Teacher Network, 7cls, 500w)                                                 | 85.4344  | 84.7159  | 85.0098  | <u>**85.0534**</u> | 0.2949  |
| Single: WRN283 (Teacher Network, 7cls, 500w)                                                 | 86.8060  | 87.1326  | 86.0222  | <u>**86.6536**</u> | 0.4660  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.8106  | 90.3005  | 90.1045  | <u>**90.0719**</u> | 0.2013  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.9412  | 89.8106  | 89.1574  | <u>**89.6364**</u> | 0.3429  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0392  | 89.0268  | 89.3860  | <u>**89.4840**</u> | 0.4191  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0065  | 90.4311  | 89.7779  | <u>**90.0718**</u> | 0.2706  |

### 1000w, 7cls에 대한 실험 결과
| Experiment (alpha = 0.3, 1000w, 7cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 1000w)                                                | 84.2913  | 84.9118  | 84.6179  | <u>**84.6070**</u> | 0.2534  |
| Single: WRN163 (Teacher Network, 7cls, 1000w)                                                | 85.4997  | 86.4468  | 86.4468  | <u>**86.1311**</u> | 0.4465  |
| Single: WRN281 (Teacher Network, 7cls, 1000w)                                                | 84.8465  | 84.8792  | 85.1078  | <u>**84.9445**</u> | 0.1162  |
| Single: WRN283 (Teacher Network, 7cls, 1000w)                                                | 86.4794  | 86.6101  | 85.9569  | <u>**86.3488**</u> | 0.2822  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3860  | 89.6473  | 89.7453  | <u>**89.5929**</u> | 0.1516  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3534  | 88.7002  | 89.0268  | <u>**89.0268**</u> | 0.2667  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 91.0516  | 89.5820  | 89.9412  | <u>**90.1916**</u> | 0.6255  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 90.6924  | 90.4637  | 90.3658  | <u>**90.5073**</u> | 0.1369  |

## Alpha값 바꿔가며 추가 실험 결과
### Alpha = 0.1

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|--------|--------|--------|--------|--------|
| T1: WRN161(GAF), T2: WRN1611(Sig), S: WRN1611 (14cls) | 69.1439 | 69.9080 | 70.0642 | **69.7054** | 0.4021 |
| T1: WRN163(GAF), T2: WRN1631(Sig), S: WRN1611 (14cls) | 70.8283 | 70.9151 | 70.9325 | **70.8920** | 0.0456 |
| T1: WRN281(GAF), T2: WRN2811(Sig), S: WRN1611 (14cls) | 69.7691 | 69.0050 | 69.3523 | **69.3755** | 0.3124 |
| T1: WRN283(GAF), T2: WRN2831(Sig), S: WRN1611 (14cls) | 69.4218 | 69.1439 | 69.8906 | **69.4854** | 0.3081 |

---

### Alpha = 0.3

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|--------|--------|--------|--------|--------|
| T1: WRN161(GAF), T2: WRN1611(Sig), S: WRN1611 (14cls) | 69.7864 | 70.6720 | 70.3247 | **70.2610** | 0.3643 |
| T1: WRN163(GAF), T2: WRN1631(Sig), S: WRN1611 (14cls) | 70.1511 | 70.3421 | 70.5504 | **70.3479** | 0.1631 |
| T1: WRN281(GAF), T2: WRN2811(Sig), S: WRN1611 (14cls) | 69.2134 | 70.3073 | 69.7517 | **69.7575** | 0.4466 |
| T1: WRN283(GAF), T2: WRN2831(Sig), S: WRN1611 (14cls) | 70.5504 | 70.3247 | 69.5781 | **70.1511** | 0.4155 |

---

### Alpha = 0.5

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|--------|--------|--------|--------|--------|
| T1: WRN161(GAF), T2: WRN1611(Sig), S: WRN1611 (14cls) | 70.7588 | 70.4115 | 70.9498 | **70.7067** | 0.2228 |
| T1: WRN163(GAF), T2: WRN1631(Sig), S: WRN1611 (14cls) | 70.2205 | 70.3768 | 69.5781 | **70.0585** | 0.3456 |
| T1: WRN281(GAF), T2: WRN2811(Sig), S: WRN1611 (14cls) | 69.4565 | 70.2900 | 68.9877 | **69.5781** | 0.5386 |
| T1: WRN283(GAF), T2: WRN2831(Sig), S: WRN1611 (14cls) | 70.0990 | 69.8038 | 70.8804 | **70.2611** | 0.4542 |

---

### Alpha = 0.7

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|--------|--------|--------|--------|--------|
| T1: WRN161(GAF), T2: WRN1611(Sig), S: WRN1611 (14cls) | 69.2308 | 69.4391 | 70.2032 | **69.6244** | 0.4180 |
| T1: WRN163(GAF), T2: WRN1631(Sig), S: WRN1611 (14cls) | 69.5954 | 68.8661 | 69.8559 | **69.4391** | 0.4189 |
| T1: WRN281(GAF), T2: WRN2811(Sig), S: WRN1611 (14cls) | 68.9356 | 67.7548 | 69.1439 | **68.6114** | 0.6117 |
| T1: WRN283(GAF), T2: WRN2831(Sig), S: WRN1611 (14cls) | 68.7098 | 69.1092 | 69.5954 | **69.1381** | 0.3621 |

---

### Alpha = 0.9

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|--------|--------|--------|--------|--------|
| T1: WRN161(GAF), T2: WRN1611(Sig), S: WRN1611 (14cls) | 69.0224 | 68.9529 | 68.8488 | **68.9414** | 0.0713 |
| T1: WRN163(GAF), T2: WRN1631(Sig), S: WRN1611 (14cls) | 68.5362 | 68.7446 | 68.8835 | **68.7214** | 0.1427 |
| T1: WRN281(GAF), T2: WRN2811(Sig), S: WRN1611 (14cls) | 67.3554 | 68.7446 | 67.7895 | **67.9632** | 0.5803 |
| T1: WRN283(GAF), T2: WRN2831(Sig), S: WRN1611 (14cls) | 67.6854 | 68.3799 | 68.1195 | **68.0616** | 0.2865 |






### 추가실험
<li> GENE_Active 14cls, 500w 데이터에 대해 (T : WRN163, S : WRN161)과 (T : WRN281, S : WRN161)에 대해서 추가 실험을 진행함 (높은 것을 최대한 뽑아보기 위해서.) </li>
<li> T : WRN163, S : WRN 161 - 68.1195, 67.1992, 67.9285, 68.6578, 67.1644, 67.5638 </li>
<li> T : WRN281, S : WRN 161 - 67.0255, 67.1297, 67.4423, 67.5291, 66.2441 </li>


### alpha(두 Teacher간 KD_loss의 비중)을 바꿔가며 실험한 결과
<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li> 69.1439 </li>
<li> 69.9080 </li>
<li> 70.0642 </li>
<li> mean : 69.7054 </li>
<li> std : 0.4021 </li>
</ul>

<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li> 70.8283 </li>
<li> 70.9151 </li>
<li> 70.9325 </li>
<li> mean : 70.8920 </li>
<li> std : 0.0456 </li>
</ul>

<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li> 69.7691 </li>
<li> 69.0050 </li>
<li> 69.3523 </li>
<li> mean : 69.3755 </li>
<li> std : 0.3124 </li>
</ul>

<li> alpha : 0.1 (T1(image)가 10%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li> 69.4218 </li>
<li> 69.1439 </li>
<li> 69.8906 </li>
<li> mean : 69.4854 </li>
<li> std : 0.3081 </li>
</ul>

<li> alpha : 0.3 (T1(image)가 30%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li> 69.7864 </li>
<li> 70.6720 </li>
<li> 70.3247 </li>
<li> mean : 70.2610 </li>
<li> std : 0.3643 </li>
</ul>

<li> alpha : 0.3 (T1(image)가 30%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li> 70.1511 </li>
<li> 70.3421 </li>
<li> 70.5504 </li>
<li> mean : 70.3479 </li>
<li> std : 0.1631 </li>
</ul>

<li> alpha : 0.3 (T1(image)가 30%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li> 69.2134 </li>
<li> 70.3073 </li>
<li> 69.7517 </li>
<li> mean : 69.7575 </li>
<li> std : 0.4466 </li>
</ul>

<li> alpha : 0.3 (T1(image)가 30%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li> 70.5504 </li>
<li> 70.3247 </li>
<li> 69.5781 </li>
<li> mean : 70.1511 </li>
<li> std : 0.4155 </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li> 70.7588 </li>
<li> 70.4115 </li>
<li> 70.9498 </li>
<li> mean : 70.7067 </li>
<li> std : 0.2228 </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li> 70.2205 </li>
<li> 70.3768 </li>
<li> 69.5781 </li>
<li> mean : 70.0585 </li>
<li> std : 0.3456 </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li> 69.4565 </li>
<li> 70.2900 </li>
<li> 68.9877 </li>
<li> mean : 69.5781 </li>
<li> std : 0.5386 </li>
</ul>

<li> alpha : 0.5 (T1(image)가 50%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li> 70.0990 </li>
<li> 69.8038 </li>
<li> 70.8804 </li>
<li> mean : 70.2611 </li>
<li> std : 0.4542 </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li> 69.2308 </li>
<li> 69.4391 </li>
<li> 70.2032 </li>
<li> mean : 69.6244 </li>
<li> std : 0.4180 </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li> 69.5954 </li>
<li> 68.8661 </li>
<li> 69.8559 </li>
<li> mean : 69.4391 </li>
<li> std : 0.4189 </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li> 68.9356 </li>
<li> 67.7548 </li>
<li> 69.1439 </li>
<li> mean : 68.6114 </li>
<li> std : 0.6117 </li>
</ul>

<li> alpha : 0.7 (T1(image)가 70%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li> 68.7098 </li>
<li> 69.1092 </li>
<li> 69.5954 </li>
<li> mean : 69.1381 </li>
<li> std : 0.3621 </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li> 69.0224 </li>
<li> 68.9529 </li>
<li> 68.8488 </li>
<li> mean : 68.9414 </li>
<li> std : 0.0713 </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li> 68.5362 </li>
<li> 68.7446 </li>
<li> 68.8835 </li>
<li> mean : 68.7214 </li>
<li> std : 0.1427 </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li> 67.3554 </li>
<li> 68.7446 </li>
<li> 67.7895 </li>
<li> mean : 67.9632 </li>
<li> std : 0.5803 </li>
</ul>

<li> alpha : 0.9 (T1(image)가 90%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li> 67.6854 </li>
<li> 68.3799 </li>
<li> 68.1195 </li>
<li> mean : 68.0616 </li>
<li> std : 0.2865 </li>
</ul>


## PAMAP2 데이터 분석
### 데이터 형태
1. 83319개의 데이터와 각 데이터는 40개의 Feature를 가지고 있다.
2. 83319개의 데이터는 0 ~ 11에 포함하는 라벨을 각각 가지고 있다.
3. 데이터셋에서 9명의 사람으로부터 83319개의 데이터를 얻은 것이다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP.png" alt="PAMAP shape" width="500">

### 학습할 때 데이터의 사용
1. 9명의 사람은 0 ~ 8의 id로 구분되고, 학습을 할 때 그 중 한명을 Test_id로 지정하여 남은 8명은 Train set으로 학습한다.
2. 이렇기 때문에, 같은 사람의 데이터를 학습하고, 테스트하게 되는 불상사는 일어나지 않음.
3. 같은 사람에서 나온 데이터를 학습하고, 테스트때 사용한다면, Acc가 높게 나올 확률이 높다. (GENE_Active때 겪어본 문제.)

### 데이터의 사용
1. .data 파일의 형식으로 저장되어있다. (GENE_Active랑 다르다.)
2. 이렇게 .data 파일로 되어있는 경우 .npy파일과 다르게 pickle.load를 통하여 데이터를 얻어왔다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP_pickle.png" alt="PAMAP Pickle load" width="500">
3. 근데 cls_id = [24, 1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17] 인데, 원래 PAMAP2에는 24번 label이 없다. 24번은 뭘까
4. 0, 9, 10, 11을 묶어서 24로 놓은건가?
5. 근데 데이터를 로드해와서 np.unique(labels)를 찍어보니, 0 ~ 11로 정렬하게 나와있다. (아마 Encoding된 Label인듯.)
6. 뭐 어쨌든 데이터를 로드해왔다고 치고. (이해가 안감.) 데이터를 어떤식으로 불러와서 load하는지 보자.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP2_data_load.png" alt="PAMAP Data Load" width="500">
7. 데이터를 window크기 100씩으로 불러오는데, 시작 지점은 22씩만 늘어난다. (나머지 78개의 sequence는 계속 겹치면서 data를 로드하고 있다.)
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP2_step_size.png" alt="PAMAP Step Size" width="500">
8. 그 다음, 데이터를 만들고 싶은 크기로 x_train, y_train, x_test, y_test를 np.zeros로 만들어주고, 하나씩 넣어준다.
9. 왜하는가? > (idx, window_length, channel)의 형태를 (idx, batch_size, window_length, channel)의 형태로 바꿔주기 위함임.




# Transformer 공부 Attention is All You Need 논문
## 1️⃣ Transformer가 등장한 이유
### ✅ 기존 RNN, LSTM의 한계점
1. **긴 문장을 다룰 때 정보 손실 문제**
   - RNN, LSTM은 **입력을 순차적으로 처리**하므로 **긴 문장에서 앞쪽 정보가 뒤쪽에 잘 전달되지 않음**  
   - (Long-Term Dependency 문제 발생)
   
2. **병렬 연산 불가능 → 학습 속도 느림**
   - RNN/LSTM은 순차적으로 학습되므로, **GPU 병렬 연산이 어려움 → 학습 속도가 느림**
   
3. **Gradient Vanishing 문제**
   - 깊은 네트워크에서 **초반 입력의 Gradient가 사라지는 문제 발생 → 학습 어려움**

---

### ✅ Transformer는 어떻게 해결했나?
1. **Attention Mechanism (Self-Attention) 활용**
   - 모든 단어가 **다른 단어와 얼마나 연관이 있는지 동시에 계산**  
   - → **멀리 떨어진 단어들 간의 관계도 효과적으로 학습 가능**
   
2. **순차 처리(X), 병렬 연산(O) 가능**
   - 모든 단어를 **동시에 처리 가능 → GPU 가속 최적화**
   
3. **Residual Connection 사용 → Gradient Vanishing 방지**
   - 각 Layer에서 **원본 입력을 더해줌** → 정보 손실 방지

---

## 2️⃣ Transformer의 주요 구성 요소
1. **Positional Encoding** → Input Embedding 시 단어 순서 정보를 반영
2. **Multi-Head Self-Attention** → 단어 간의 연관성을 여러 Head를 통해 다양한 관점에서 학습
3. **Feed Forward Network (FFN)** → 비선형 변환을 통해 표현력 증가 + 깊이 있는 학습 가능
4. **Residual Connection + Layer Normalization** → 안정적인 학습

---

# 3️⃣ Transformer의 Encoder 동작 과정 (입력 → Context 벡터 생성)

### ✅ 예제: 입력 문장
> **"I am a teacher" (영어) → "나는 선생님이다" (한글)로 번역한다고 가정**

### 🟢 Step 1: Input Embedding + Positional Encoding
- 각 단어를 **Embedding 벡터(고차원 표현)로 변환**
- Transformer는 단어의 순서를 알 수 없기 때문에 **Positional Encoding 추가**
- 최종 입력 벡터 = `Embedding Matrix + Positional Encoding`

---

### 🟢 Step 2: Multi-Head Self-Attention (자기 자신에게 Attention)
> **"I am a teacher" 문장에서 단어들 간의 연관성을 학습**

✅ **Query, Key, Value 행렬 생성**
- Self-Attention을 수행하여 **각 단어가 문장 내 다른 단어와 얼마나 관련 있는지 계산**

✅ **Scaled Dot-Product Attention 수행**
- Attention Score 계산: Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V
- Softmax를 적용하여 **모든 단어의 중요도를 확률적으로 정규화하고, 각 중요도에 따라 가중치를 부여**
- **모든 단어의 Value를 가중합하여 새로운 벡터를 생성**

✅ **Multi-Head Attention (h개로 분할하여 다른 의미 학습)**
- 각 Head는 **서로 다른 문맥 정보를 학습** (CNN의 필터처럼)
- 여러 개의 Attention 결과를 Concat하여 최종 Attention 벡터 생성

---

### 🟢 Step 3: Feed Forward Network (FFN)
> **Self-Attention을 거친 벡터를 더욱 정제된 표현으로 변환**

✅ **비선형 변환을 추가하여 표현력을 증가**
- FFN(X) = max(0, X * W1 + b1) * W2 + b2
- CNN과 DNN의 활성화 함수처럼, FFN에서도 비선형성을 추가하여 모델의 표현력을 증가시킴
- 더 풍부한 특징을 학습할 수 있도록 변환

✅ **Residual Connection과 Layer Normalization을 수행하여 정보 손실 방지**

---

# 4️⃣ Transformer의 Decoder 동작 과정 (Context 벡터 → 문장 생성)

### ✅ Decoder의 입력
- 훈련할 때는 **정답 문장의 앞부분**이 Decoder의 입력이 됨
- 예측할 때는 **Decoder가 예측한 단어를 입력으로 사용**
- 예) `"<SOS> 나는"` → `"나는 선생님"` → `"나는 선생님이다"`

---

### 🟢 Step 1: Decoder Input Embedding + Positional Encoding
- Decoder도 입력을 **Embedding 벡터로 변환 후, Positional Encoding을 추가**
- `"<SOS> 나는"` → Embedding → Positional Encoding 추가

---

### 🟢 Step 2: Masked Multi-Head Self-Attention
> **Decoder가 "나는"을 예측할 때, "선생님"을 보지 못하도록 Masking 적용(Training 과정에서)**

✅ **Masked Self-Attention이란?**
- 현재까지 생성된 단어까지만 고려하도록 Future Token을 가리는 Mask 적용
- Softmax 연산 시, 미래 단어는 **-∞ (무한대로 낮은 값)** 처리하여 고려되지 않도록 함

---

### 🟢 Step 3: Encoder-Decoder Attention
> **Encoder에서 생성한 Context 벡터를 참고하여, 다음 단어를 예측**

✅ **Decoder의 Query, Encoder의 Key, Value 사용**
- Query: Decoder에서 Self-Attention을 수행한 후 벡터
- Key & Value: Encoder의 최종 출력 (입력 문장의 Context 벡터) # **Encoder의 Value를 사용한다는것은 꼭 기억해야 함.(Value는 Encoder의 정보임)**
- Decoder의 Query와 Encoder의 Key를 Attention 하여 나온 결과와 Encoder의 Value를 가중합한 결과를 FFN에 통과시킨 뒤 가장 가까운 단어를 찾으면 그것이 Predict 단어가 된다.

✅ **Encoder에서 얻은 Context 정보를 활용하여 더욱 정확한 단어 예측**
- `"나는"`이 등장했을 때, `"선생님"`이 나올 확률이 높아지도록 조정됨

---

### 🟢 Step 4: Feed Forward Network (FFN)
> **Attention을 수행한 벡터를 더욱 정제된 표현으로 변환**

✅ **Encoder와 동일한 FFN 적용**
- ReLU 활성화 함수 + 두 개의 Linear Layer 사용
- 최종적으로 문맥 정보를 더욱 잘 반영한 벡터를 생성

---

### 🟢 Step 5: Softmax를 통해 단어 예측
> **Decoder의 최종 출력을 Softmax에 통과시켜 가장 적절한 단어를 예측**

✅ **Softmax를 통해 확률 분포 계산**
- P(선생님 | 나는) = exp(z_선생님) / sum(exp(z_i))
- 가장 확률이 높은 단어 `"선생님"`을 선택

✅ **이전까지 생성된 단어를 입력으로 넣고 반복하여 최종 문장 생성**
- `"나는 선생님"` → `"나는 선생님이다"`

---

# Transformer 전체 과정 요약
1. **Encoder**
 - 입력 문장을 **벡터로 변환**
 - Self-Attention을 수행하여 단어 간 관계 학습
 - FFN을 통해 더욱 정제된 벡터 생성

2. **Decoder**
 - 이전까지 생성된 단어를 입력으로 사용
 - Masked Self-Attention 수행
 - Encoder-Decoder Attention을 통해 입력 문장 정보 반영
 - FFN을 통해 최적의 벡터 생성 후 Softmax를 통해 단어 예측

---

# ✅ 결론
Transformer는 **문장을 효과적으로 표현(Encoding)하고, 이를 기반으로 새로운 문장을 자연스럽게 생성(Decoding)하는 모델**이다.
- **Self-Attention** → 문맥을 반영한 벡터 생성
- **Multi-Head Attention** → 다양한 의미를 학습
- **Residual Connection & FFN** → 정보 손실 방지 & 표현력 증가

🚀 **즉, "입력 문장을 벡터로 변환 → 변환된 벡터를 바탕으로 새로운 문장을 생성"하는 것이 Transformer의 핵심 원리!** 🚀


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>