### 해야 할 것
<li> 2 Teacher (GAF + Sig) 에 대해서 14cls에 대해서 코드를 만들어야 함. (서버에서 돌아가도록 공부해서 만들기.) </li>
<li> 14cls에 대해서는 161-161, 163-161, 281-161, 283-161 조합에 대해 모두 해봐야함. </li>
<li> alpha값 (KD1과 KD2 간의 비중을 0.1, 0.3, 0.5, 0.7, 0.9에 대해서 비교해보기 (3번씩 계산 후 평균 + 표준편차)) </li>
<li> 이제는 7cls에 대해서도 GAF를 만들고, GAF Teacher를 만들고, 지금까지 했던 것들을 모두 똑같이 해야 함. </li>
<li> 7cls-500w, 7cls-1000w 에 대해서 각각 GAF만들고, GAF Teacher 학습하고(vanila), 1 Teacher(GAF) KD도 결과를 내보는 것이 목표. </li>
<li> TPKD 논문 9 ~ 11p 부분을 보고, 14cls와 7cls의 결과를 확인하며 비교해보자. </li>

### 지금까지 결과 추가 (2-Teacher, 14cls)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 69.7864 </li>
<li> 70.6720 </li>
<li> 70.3247 </li>
<li> 평균 : 70.2610 </li>
<li> 표준편차 : 0.3643 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.1511 </li>
<li> 70.3421 </li>
<li> 70.5504 </li>
<li> 평균 : 70.3479 </li>
<li> 표준편차 : 0.1631 </li>
<br>
</ul>

<li> T1 : WRN28-1(GAF), T2 : WRN28-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 69.2134 </li>
<li> 70.3073 </li>
<li> 69.7517 </li>
<li> 평균 : 69.7575 </li>
<li> 표준편차 : 0.4466 </li>
<br>
</ul>

<li> T1 : WRN28-3(GAF), T2 : WRN28-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.5504 </li>
<li> 70.3247 </li>
<li> 69.5781 </li>
<li> 평균 : 70.1511 </li>
<li> 표준편차 : 0.4155 </li>
<br>
</ul>

### 지금까지 결과 추가 (Teacher Network, 7cls, 500w)
<li> Single : WRN161 </li>
<ul>
<li> 84.5852 </li>
<li> 84.8465 </li>
<li> 85.0425 </li>
<li> 평균 : 84.8247 </li>
<li> 표준편차 : 0.1873 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4017 </li>
<li> 85.6956 </li>
<li> 86.1855 </li>
<li> 평균 : 85.7609 </li>
<li> 표준편차 : 0.3233 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 85.4344 </li>
<li> 84.7159 </li>
<li> 85.0098 </li>
<li> 평균 : 85.0534 </li>
<li> 표준편차 : 0.2949 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.8060 </li>
<li> 87.1326 </li>
<li> 86.0222 </li>
<li> 평균 : 86.6536 </li>
<li> 표준편차 : 0.4660 </li>
<br>
</ul>

### 지금까지 결과 추가 (Teacher Network, 7cls, 1000w)
<li> Single : WRN161 </li>
<ul>
<li> 84.2913 </li>
<li> 84.9118 </li>
<li> 84.6179 </li>
<li> 평균 : 84.6070 </li>
<li> 표준편차 : 0.2534 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4997 </li>
<li> 86.4468 </li>
<li> 86.4468 </li>
<li> 평균 : 86.1311 </li>
<li> 표준편차 : 0.4465 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 84.8465 </li>
<li> 84.8792 </li>
<li> 85.1078 </li>
<li> 평균 : 84.9445 </li>
<li> 표준편차 : 0.1162 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.4794 </li>
<li> 86.6101 </li>
<li> 85.9569 </li>
<li> 평균 :  </li>
<li> 표준편차 :  </li>
<br>
</ul>

### 지금까지 결과 추가 (1 Teacher KD, 7cls, 500w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.3860 </li>
<li> 89.6473 </li>
<li> 89.7453 </li>
<li> 평균 : 89.5929 </li>
<li> 표준편차 : 0.1516 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.3534 </li>
<li> 88.7002 </li>
<li> 89.0268 </li>
<li> 평균 : 89.0268 </li>
<li> 표준편차 : 0.2667 </li>
<br>
</ul>

### 지금까지 결과 추가 (1 Teacher KD, 7cls, 1000w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.8106 </li>
<li> 90.3005 </li>
<li> 90.1045 </li>
<li> 평균 : 90.0719 </li>
<li> 표준편차 : 0.2013 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.9412 </li>
<li> 89.8106 </li>
<li> 89.1574 </li>
<li> 평균 : 89.6364 </li>
<li> 표준편차 : 0.3429 </li>
<br>
</ul>


### 지금까지 결과 추가 (2 Teacher KD, 7cls, 500w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 91.0516 </li>
<li> 89.5820 </li>
<li> 89.9412 </li>
<li> 평균 : 90.1916 </li>
<li> 표준편차 : 0.6255 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.6924 </li>
<li> 90.4637 </li>
<li> 90.3658 </li>
<li> 평균 : 90.5073 </li>
<li> 표준편차 : 0.1369 </li>
<br>
</ul>


### 지금까지 결과 추가 (2 Teacher KD, 7cls, 1000w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 90.0392 </li>
<li> 89.0268 </li>
<li> 89.3860 </li>
<li> 평균 : 89.4840 </li>
<li> 표준편차 : 0.4191 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.0065 </li>
<li> 90.4311 </li>
<li> 89.7779 </li>
<li> 평균 : 90.0718 </li>
<li> 표준편차 : 0.2706 </li>
<br>
</ul>

### 500w, 14cls에 대한 실험 결과
| Experiment (alpha = 0.3, 500w, 14cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| WRN16-1 (GAF 단일 네트워크, 500w, 14cls)                                                      | 63.0491  | 63.3270  | 62.9797  | <u>**63.1186**</u> | 0.1501  |
| WRN16-3 (GAF 단일 네트워크, 500w, 14cls)                                                      | 64.0389  | 63.9521  | 63.9173  | <u>**63.9694**</u> | 0.0511  |
| WRN28-1 (GAF 단일 네트워크, 500w, 14cls)                                                      | 63.2922  | 64.0042  | 63.3096  | <u>**63.5353**</u> | 0.3316  |
| WRN28-3 (GAF 단일 네트워크, 500w, 14cls)                                                      | 64.1604  | 64.8029  | 65.3933  | <u>**64.7855**</u> | 0.5035  |
| T: WRN16-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.9703  | 67.6680  | 67.0429  | <u>**67.8937**</u> | 0.8029  |
| T: WRN16-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.2063  | 67.7201  | 68.3278  | <u>**68.0847**</u> | 0.2626  |
| T: WRN28-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.0082  | 66.4004  | 67.0603  | <u>**66.8230**</u> | 0.2996  |
| T: WRN28-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.5464  | 66.4872  | 67.7027  | <u>**67.2454**</u> | 0.5399  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.7864  | 70.6720  | 70.3247  | <u>**70.2610**</u> | 0.3643  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.1511  | 70.3421  | 70.5504  | <u>**70.3479**</u> | 0.1631  |
| T1: WRN28-1(GAF), T2: WRN28-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.2134  | 70.3073  | 69.7517  | <u>**69.7575**</u> | 0.4466  |
| T1: WRN28-3(GAF), T2: WRN28-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.5504  | 70.3247  | 69.5781  | <u>**70.1511**</u> | 0.4155  |

### 500w, 7cls에 대한 실험 결과
| Experiment (alpha = 0.3, 500w, 7cls)                                                         | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 500w)                                                 | 84.5852  | 84.8465  | 85.0425  | <u>**84.8247**</u> | 0.1873  |
| Single: WRN163 (Teacher Network, 7cls, 500w)                                                 | 85.4017  | 85.6956  | 86.1855  | <u>**85.7609**</u> | 0.3233  |
| Single: WRN281 (Teacher Network, 7cls, 500w)                                                 | 85.4344  | 84.7159  | 85.0098  | <u>**85.0534**</u> | 0.2949  |
| Single: WRN283 (Teacher Network, 7cls, 500w)                                                 | 86.8060  | 87.1326  | 86.0222  | <u>**86.6536**</u> | 0.4660  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.8106  | 90.3005  | 90.1045  | <u>**90.0719**</u> | 0.2013  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.9412  | 89.8106  | 89.1574  | <u>**89.6364**</u> | 0.3429  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0392  | 89.0268  | 89.3860  | <u>**89.4840**</u> | 0.4191  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0065  | 90.4311  | 89.7779  | <u>**90.0718**</u> | 0.2706  |

### 1000w, 7cls에 대한 실험 결과
| Experiment (alpha = 0.3, 1000w, 7cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 1000w)                                                | 84.2913  | 84.9118  | 84.6179  | <u>**84.6070**</u> | 0.2534  |
| Single: WRN163 (Teacher Network, 7cls, 1000w)                                                | 85.4997  | 86.4468  | 86.4468  | <u>**86.1311**</u> | 0.4465  |
| Single: WRN281 (Teacher Network, 7cls, 1000w)                                                | 84.8465  | 84.8792  | 85.1078  | <u>**84.9445**</u> | 0.1162  |
| Single: WRN283 (Teacher Network, 7cls, 1000w)                                                | 86.4794  | 86.6101  | 85.9569  | <u>**86.3488**</u> | 0.2822  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3860  | 89.6473  | 89.7453  | <u>**89.5929**</u> | 0.1516  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3534  | 88.7002  | 89.0268  | <u>**89.0268**</u> | 0.2667  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 91.0516  | 89.5820  | 89.9412  | <u>**90.1916**</u> | 0.6255  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 90.6924  | 90.4637  | 90.3658  | <u>**90.5073**</u> | 0.1369  |

### 추가실험
<li> GENE_Active 14cls, 500w 데이터에 대해 (T : WRN163, S : WRN161)과 (T : WRN281, S : WRN161)에 대해서 추가 실험을 진행함 (높은 것을 최대한 뽑아보기 위해서.) </li>
<li> T : WRN163, S : WRN 161 - 68.1195, 67.1992, 67.9285, 68.6578, 67.1644, 67.5638 </li>
<li> T : WRN281, S : WRN 161 - 67.0255, 67.1297, 67.4423, 67.5291, 66.2441 </li>


### lambda(KD_loss의 비중)을 바꿔가며 실험한 결과
<li> lambda : 0.1 (KD_loss가 10%), T : WRN161, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.1 (KD_loss가 10%), T : WRN163, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.1 (KD_loss가 10%), T : WRN281, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.1 (KD_loss가 10%), T : WRN283, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.3 (KD_loss가 30%), T : WRN161, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.3 (KD_loss가 30%), T : WRN163, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.3 (KD_loss가 30%), T : WRN281, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.3 (KD_loss가 30%), T : WRN283, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.5 (KD_loss가 50%), T : WRN161, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.5 (KD_loss가 50%), T : WRN163, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.5 (KD_loss가 50%), T : WRN281, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.5 (KD_loss가 50%), T : WRN283, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.9 (KD_loss가 90%), T : WRN161, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.9 (KD_loss가 90%), T : WRN163, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.9 (KD_loss가 90%), T : WRN281, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> lambda : 0.9 (KD_loss가 90%), T : WRN283, S : WRN161 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

### Self-Attention
<li>  </li>


### Cross-Attention
<li>  </li>


### 트랜스포머 논문 리뷰 공부 (Attention is All You Need)
<li> GPT는 Transformer의 Decoder 아키텍처를 활용함. </li>
<li> Transforemr 역사를 보면 RNN에서부터 번역이나 Time Series 데이터를 처리하기 위해 발전해 왔지만, 결국에는 입력 Sequence 전체에서 정보를 수집하도록 발전해왔다. </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/Transformer역사.png" alt="트랜스포머 역사" width="500">
<li> 트랜스포머는 Attention 매커니즘 하나만으로 다양한 자연어 처리 Task를 잘 수행할 수 있다. </li>
<li> 트랜스포머는 RNN이나 LSTM을 이용하여 시계열 데이터(시간 순서가 중요한)를 처리하는 것이 아닌 Positional Encoding을 사용한다. </li>
<li> Encoder와 Decoder로 구성된다. </li>
<li> Attention 과정을 여러 레이어에서 반복한다. </li>

### Transformer Encoder에서의 처음 순서(초반 과정)
<li> 1. Input Embedding </li>
<ul>
<li> 입력 문장을 학습에 사용할 수 있도록 Matrix의 형태로 임베딩을 수행한다. 각 단어 개수의 차원(행), embedding_dim(열)의 크기가 된다. </li>
<li> RNN이나 LSTM을 이용하지 않기 때문에, 각 문장 내의 단어(Sequence)의 순서 정보가 필요하기 때문에 Positional Encoding을 수행함. </li>
<li> Positional Encoding은 위치 정보를 입력 embedding matrix와 element wise 덧셈을 하여 구현한다. </li>
<li> 이제 Embedding Matrix에는 해석할 문장에 대한 정보와 각 단어의 순서 정보가 포함되게 된다. </li>
</ul>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/Input_embedding.png" alt="입력 임베딩" width="500">
<li> 2. Attention을 진행한다. </li>
<ul>
<li> Encoder 부분에서의 Attention은 Self-Attention으로서, 문장 내의 각 단어들이 서로 얼마나의 연관성을 지니는지 파악한다. </li>
<li> 왜?) 문장 내에서 여러 단어와 연관성(Dependency)이 높다면 중요한 단어라고 여기고, 영향력을 높여야 하기 때문. </li>
<li> 그렇게 단어 간의 연관성을 알아낸 뒤, 각 단어를 더 나은 표현(Representation)으로 변환해야 한다.(문맥을 반영한 새로운 input embedding matrix로) </li>
<li> '더 나은 표현'을 위해서 FeedForward Layer(FFN)을 통해서 *비선형성* 을 추가하고, 더욱 정제된 표현으로 만들어주는 과정을 거친다. </li>
<li> 또한 이 과정들을 Encoder에서는 문장의 길이가 길어지더라도 원래의 정보를 잃지 않도록 Residual Learning(원본 입력과 결과를 layer마다 더해줌)으로 진행되고, 이를 통해 여러 Layer를 지나더라도, 낮은 layer에서의 정보를 잃지 않고 유지할 수 있다. </li>
<li> Transformer의 Encoder에서는 Layer마다 두 번의 Residual Connection을 적용함. (1. Self-Attention 연산 후에 Multi-Head Attention의 출력과 원본 입력을 더해준다.) (2. Feed Forward Network 연산 후 FFN의 출력과 원본 입력을 더해준다.) </li>
<li> 이 때 중요한 것은 각 Layer마다 학습하는 Parameter가 서로 다르다. </li>
</ul>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/encoder.png" alt="Encoder Part" width="500">

### Transformer의 Decoder 동작 원리
<li> Decoder의 각 Layer에서는 마지막 Encoder Layer의 출력이 모든 Decoder Layer에 입력된다. </li>
<li>  </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/decoder_input.png" alt="Decoder_input" width="500">

### Multihead Attention
<li> Query : Attention Score를 계산하고 싶은 주체단어 </li>
<li> Key : Attention Score를 계산하고 싶은 대상 (Query는 Key와 자신의 Attention Score를 알고 싶어함.) </li>
<li> Value : 얻어진 Attention Score를 이용해서 가중치를 곱해줄 대상. 곱해진 가중치 결과가 Attention Value이다. </li>
<li> Multihead Attention은 하나의 Key, Value, Query에 대해서 h개의 서로 다른 Attention Concept을 학습하기 위해 사용되는 방식이다. </li>
<li> 즉, convolution layer에서 각 필터마다 다른 것을 학습하는 것 처럼, 여기서도 각 헤드마다 다른 것을 학습하기 위해서, Key, Value, Query를 헤드마다 서로 다른 가중치로 변형하여 학습을 하게 된다. </li>
<li> 그러면 h개의 각 헤드에서 Scaled Dot-Product Attention을 수행하여 Attention Value를 얻게 된다. </li>
<li> 그 다음, 각 Head의 결과를 단순히 연결(Concat)하여 더 풍부한 표현을 의미하도록 한다. </li>
<li> 최종적으로 Multihead Attention을 한 뒤에 나머지 Layer(FFN, Add & Norm)와 차원을 맞춰주기 위해서 Linear 변환을 수행한다. </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/Attention.png" alt="Attention" width="500">

# 🚀 Transformer 모델 정리: 한눈에 이해하는 Transformer

## 1️⃣ Transformer가 등장한 이유
### ✅ 기존 RNN, LSTM의 한계점
1. **긴 문장을 다룰 때 정보 손실 문제**
   - RNN, LSTM은 **입력을 순차적으로 처리**하므로 **긴 문장에서 앞쪽 정보가 뒤쪽에 잘 전달되지 않음**  
   - (Long-Term Dependency 문제 발생)
   
2. **병렬 연산 불가능 → 학습 속도 느림**
   - RNN/LSTM은 순차적으로 학습되므로, **GPU 병렬 연산이 어려움 → 학습 속도가 느림**
   
3. **Gradient Vanishing 문제**
   - 깊은 네트워크에서 **초반 입력의 Gradient가 사라지는 문제 발생 → 학습 어려움**

---

### ✅ Transformer는 어떻게 해결했나?
1. **Attention Mechanism (Self-Attention) 활용**
   - 모든 단어가 **다른 단어와 얼마나 연관이 있는지 동시에 계산**  
   - → **멀리 떨어진 단어들 간의 관계도 효과적으로 학습 가능**
   
2. **순차 처리(X), 병렬 연산(O) 가능**
   - 모든 단어를 **동시에 처리 가능 → GPU 가속 최적화**
   
3. **Residual Connection 사용 → Gradient Vanishing 방지**
   - 각 Layer에서 **원본 입력을 더해줌** → 정보 손실 방지

---

## 2️⃣ Transformer의 주요 구성 요소
1. **Positional Encoding** → 단어 순서 정보를 반영
2. **Multi-Head Self-Attention** → 단어 간의 연관성 학습
3. **Feed Forward Network (FFN)** → 비선형 변환을 통해 표현력 증가
4. **Residual Connection + Layer Normalization** → 안정적인 학습

---

# 🟢 3️⃣ Transformer의 Encoder 동작 과정 (입력 → Context 벡터 생성)

### ✅ 예제: 입력 문장
> **"I am a teacher" (영어) → "나는 선생님이다" (한글)로 번역한다고 가정**

### 🟢 Step 1: Input Embedding + Positional Encoding
- 각 단어를 **Embedding 벡터(고차원 표현)로 변환**
- Transformer는 단어의 순서를 알 수 없기 때문에 **Positional Encoding 추가**
- 최종 입력 벡터 = `Embedding Matrix + Positional Encoding`

---

### 🟢 Step 2: Multi-Head Self-Attention (자기 자신에게 Attention)
> **"I am a teacher" 문장에서 단어들 간의 연관성을 학습**

✅ **Query, Key, Value 행렬 생성**
- Self-Attention을 수행하여 **각 단어가 문장 내 다른 단어와 얼마나 관련 있는지 계산**

✅ **Scaled Dot-Product Attention 수행**
- Attention Score 계산: Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V

- Softmax를 적용하여 **가장 중요한 단어에 가중치를 부여**
- **가장 중요한 단어의 Value를 반영하여 새로운 벡터 생성**

✅ **Multi-Head Attention (h개로 분할하여 다른 의미 학습)**
- 각 Head는 **서로 다른 문맥 정보를 학습** (CNN의 필터처럼)
- 여러 개의 Attention 결과를 Concat하여 최종 Attention 벡터 생성

---

### 🟢 Step 3: Feed Forward Network (FFN)
> **Self-Attention을 거친 벡터를 더욱 정제된 표현으로 변환**

✅ **비선형 변환을 추가하여 표현력을 증가**
- FFN(X) = max(0, X * W1 + b1) * W2 + b2
- ReLU 활성화 함수를 적용하여 비선형성을 부여
- 더 풍부한 특징을 학습할 수 있도록 변환

✅ **Residual Connection과 Layer Normalization을 수행하여 정보 손실 방지**

---

# 🟢 4️⃣ Transformer의 Decoder 동작 과정 (Context 벡터 → 문장 생성)

### ✅ Decoder의 입력
- 훈련할 때는 **정답 문장의 앞부분**이 Decoder의 입력이 됨
- 예측할 때는 **Decoder가 예측한 단어를 입력으로 사용**
- 예) `"<SOS> 나는"` → `"나는 선생님"` → `"나는 선생님이다"`

---

### 🟢 Step 1: Decoder Input Embedding + Positional Encoding
- Decoder도 입력을 **Embedding 벡터로 변환 후, Positional Encoding을 추가**
- `"<SOS> 나는"` → Embedding → Positional Encoding 추가

---

### 🟢 Step 2: Masked Multi-Head Self-Attention
> **Decoder가 "나는"을 예측할 때, "선생님"을 보지 못하도록 Masking 적용**

✅ **Masked Self-Attention이란?**
- 현재까지 생성된 단어까지만 고려하도록 Future Token을 가리는 Mask 적용
- Softmax 연산 시, 미래 단어는 **-∞ (무한대로 낮은 값)** 처리하여 고려되지 않도록 함

---

### 🟢 Step 3: Encoder-Decoder Attention
> **Encoder에서 생성한 Context 벡터를 참고하여, 다음 단어를 예측**

✅ **Decoder의 Query, Encoder의 Key, Value 사용**
- Query: Decoder에서 Self-Attention을 수행한 후 벡터
- Key & Value: Encoder의 최종 출력 (입력 문장의 Context 벡터)

✅ **Encoder에서 얻은 Context 정보를 활용하여 더욱 정확한 단어 예측**
- `"나는"`이 등장했을 때, `"선생님"`이 나올 확률이 높아지도록 조정됨

---

### 🟢 Step 4: Feed Forward Network (FFN)
> **Attention을 수행한 벡터를 더욱 정제된 표현으로 변환**

✅ **Encoder와 동일한 FFN 적용**
- ReLU 활성화 함수 + 두 개의 Linear Layer 사용
- 최종적으로 문맥 정보를 더욱 잘 반영한 벡터를 생성

---

### 🟢 Step 5: Softmax를 통해 단어 예측
> **Decoder의 최종 출력을 Softmax에 통과시켜 가장 적절한 단어를 예측**

✅ **Softmax를 통해 확률 분포 계산**
- P(선생님 | 나는) = exp(z_선생님) / sum(exp(z_i))
- 가장 확률이 높은 단어 `"선생님"`을 선택

✅ **이전까지 생성된 단어를 입력으로 넣고 반복하여 최종 문장 생성**
- `"나는 선생님"` → `"나는 선생님이다"`

---

# 🚀 Transformer 전체 과정 요약
1. **Encoder**
 - 입력 문장을 **벡터로 변환**
 - Self-Attention을 수행하여 단어 간 관계 학습
 - FFN을 통해 더욱 정제된 벡터 생성

2. **Decoder**
 - 이전까지 생성된 단어를 입력으로 사용
 - Masked Self-Attention 수행
 - Encoder-Decoder Attention을 통해 입력 문장 정보 반영
 - FFN을 통해 최적의 벡터 생성 후 Softmax를 통해 단어 예측

---

# ✅ 결론
Transformer는 **문장을 효과적으로 표현(Encoding)하고, 이를 기반으로 새로운 문장을 자연스럽게 생성(Decoding)하는 모델**이다.
- **Self-Attention** → 문맥을 반영한 벡터 생성
- **Multi-Head Attention** → 다양한 의미를 학습
- **Residual Connection & FFN** → 정보 손실 방지 & 표현력 증가

🚀 **즉, "입력 문장을 벡터로 변환 → 변환된 벡터를 바탕으로 새로운 문장을 생성"하는 것이 Transformer의 핵심 원리!** 🚀


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>