### í•´ì•¼ í•  ê²ƒ
<li> 2 Teacher (GAF + Sig) ì— ëŒ€í•´ì„œ 14clsì— ëŒ€í•´ì„œ ì½”ë“œë¥¼ ë§Œë“¤ì–´ì•¼ í•¨. (ì„œë²„ì—ì„œ ëŒì•„ê°€ë„ë¡ ê³µë¶€í•´ì„œ ë§Œë“¤ê¸°.) (ì™„) </li>
<li> 14clsì— ëŒ€í•´ì„œëŠ” 161-161, 163-161, 281-161, 283-161 ì¡°í•©ì— ëŒ€í•´ ëª¨ë‘ í•´ë´ì•¼í•¨. (ì™„) </li>
<li> alphaê°’ (KD1ê³¼ KD2 ê°„ì˜ ë¹„ì¤‘ì„ 0.1, 0.3, 0.5, 0.7, 0.9ì— ëŒ€í•´ì„œ ë¹„êµí•´ë³´ê¸° (3ë²ˆì”© ê³„ì‚° í›„ í‰ê·  + í‘œì¤€í¸ì°¨)) (í•˜ëŠ”ì¤‘ í•˜ë£¨ì´í‹€ì •ë„ ê±¸ë¦´ë“¯) </li>
<li> ì´ì œëŠ” 7clsì— ëŒ€í•´ì„œë„ GAFë¥¼ ë§Œë“¤ê³ , GAF Teacherë¥¼ ë§Œë“¤ê³ , ì§€ê¸ˆê¹Œì§€ í–ˆë˜ ê²ƒë“¤ì„ ëª¨ë‘ ë˜‘ê°™ì´ í•´ì•¼ í•¨. (ì™„) </li>
<li> 7cls-500w, 7cls-1000w ì— ëŒ€í•´ì„œ ê°ê° GAFë§Œë“¤ê³ , GAF Teacher í•™ìŠµí•˜ê³ (vanila), 1 Teacher(GAF) KDë„ ê²°ê³¼ë¥¼ ë‚´ë³´ëŠ” ê²ƒì´ ëª©í‘œ. (ì™„) </li>
<li> TPKD ë…¼ë¬¸ 9 ~ 11p ë¶€ë¶„ì„ ë³´ê³ , 14clsì™€ 7clsì˜ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë©° ë¹„êµí•´ë³´ì. (ì˜¤ì¼€ì´) </li>
<li> PAMAP2 ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œë„ GAFë¡œ ë³€í™˜í•˜ê³ , ìœ„ì™€ê°™ì€ ì‘ì—…ì„ ìˆ˜í–‰í•´ì•¼ í•¨. </li>

### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (2-Teacher, 14cls)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) ê²°ê³¼ </li>
<ul>
<li> 69.7864 </li>
<li> 70.6720 </li>
<li> 70.3247 </li>
<li> í‰ê·  : 70.2610 </li>
<li> í‘œì¤€í¸ì°¨ : 0.3643 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) ê²°ê³¼ </li>
<ul>
<li> 70.1511 </li>
<li> 70.3421 </li>
<li> 70.5504 </li>
<li> í‰ê·  : 70.3479 </li>
<li> í‘œì¤€í¸ì°¨ : 0.1631 </li>
<br>
</ul>

<li> T1 : WRN28-1(GAF), T2 : WRN28-1(Sig) S : WRN16-1(sig) ê²°ê³¼ </li>
<ul>
<li> 69.2134 </li>
<li> 70.3073 </li>
<li> 69.7517 </li>
<li> í‰ê·  : 69.7575 </li>
<li> í‘œì¤€í¸ì°¨ : 0.4466 </li>
<br>
</ul>

<li> T1 : WRN28-3(GAF), T2 : WRN28-3(Sig) S : WRN16-1(sig) ê²°ê³¼ </li>
<ul>
<li> 70.5504 </li>
<li> 70.3247 </li>
<li> 69.5781 </li>
<li> í‰ê·  : 70.1511 </li>
<li> í‘œì¤€í¸ì°¨ : 0.4155 </li>
<br>
</ul>

### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (Teacher Network, 7cls, 500w)
<li> Single : WRN161 </li>
<ul>
<li> 84.5852 </li>
<li> 84.8465 </li>
<li> 85.0425 </li>
<li> í‰ê·  : 84.8247 </li>
<li> í‘œì¤€í¸ì°¨ : 0.1873 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4017 </li>
<li> 85.6956 </li>
<li> 86.1855 </li>
<li> í‰ê·  : 85.7609 </li>
<li> í‘œì¤€í¸ì°¨ : 0.3233 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 85.4344 </li>
<li> 84.7159 </li>
<li> 85.0098 </li>
<li> í‰ê·  : 85.0534 </li>
<li> í‘œì¤€í¸ì°¨ : 0.2949 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.8060 </li>
<li> 87.1326 </li>
<li> 86.0222 </li>
<li> í‰ê·  : 86.6536 </li>
<li> í‘œì¤€í¸ì°¨ : 0.4660 </li>
<br>
</ul>

### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (Teacher Network, 7cls, 1000w)
<li> Single : WRN161 </li>
<ul>
<li> 84.2913 </li>
<li> 84.9118 </li>
<li> 84.6179 </li>
<li> í‰ê·  : 84.6070 </li>
<li> í‘œì¤€í¸ì°¨ : 0.2534 </li>
<br>
</ul>

<li> Single : WRN163 </li>
<ul>
<li> 85.4997 </li>
<li> 86.4468 </li>
<li> 86.4468 </li>
<li> í‰ê·  : 86.1311 </li>
<li> í‘œì¤€í¸ì°¨ : 0.4465 </li>
<br>
</ul>

<li> Single : WRN281 </li>
<ul>
<li> 84.8465 </li>
<li> 84.8792 </li>
<li> 85.1078 </li>
<li> í‰ê·  : 84.9445 </li>
<li> í‘œì¤€í¸ì°¨ : 0.1162 </li>
<br>
</ul>

<li> Single : WRN283 </li>
<ul>
<li> 86.4794 </li>
<li> 86.6101 </li>
<li> 85.9569 </li>
<li> í‰ê·  :  </li>
<li> í‘œì¤€í¸ì°¨ :  </li>
<br>
</ul>

### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (1 Teacher KD, 7cls, 500w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.3860 </li>
<li> 89.6473 </li>
<li> 89.7453 </li>
<li> í‰ê·  : 89.5929 </li>
<li> í‘œì¤€í¸ì°¨ : 0.1516 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.3534 </li>
<li> 88.7002 </li>
<li> 89.0268 </li>
<li> í‰ê·  : 89.0268 </li>
<li> í‘œì¤€í¸ì°¨ : 0.2667 </li>
<br>
</ul>

### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (1 Teacher KD, 7cls, 1000w)
<li> Teacher : WRN161, Student : WRN1611 </li>
<ul>
<li> 89.8106 </li>
<li> 90.3005 </li>
<li> 90.1045 </li>
<li> í‰ê·  : 90.0719 </li>
<li> í‘œì¤€í¸ì°¨ : 0.2013 </li>
<br>
</ul>

<li> Teacher : WRN163, Student : WRN1611 </li>
<ul>
<li> 89.9412 </li>
<li> 89.8106 </li>
<li> 89.1574 </li>
<li> í‰ê·  : 89.6364 </li>
<li> í‘œì¤€í¸ì°¨ : 0.3429 </li>
<br>
</ul>


### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (2 Teacher KD, 7cls, 500w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 91.0516 </li>
<li> 89.5820 </li>
<li> 89.9412 </li>
<li> í‰ê·  : 90.1916 </li>
<li> í‘œì¤€í¸ì°¨ : 0.6255 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.6924 </li>
<li> 90.4637 </li>
<li> 90.3658 </li>
<li> í‰ê·  : 90.5073 </li>
<li> í‘œì¤€í¸ì°¨ : 0.1369 </li>
<br>
</ul>


### ì§€ê¸ˆê¹Œì§€ ê²°ê³¼ ì¶”ê°€ (2 Teacher KD, 7cls, 1000w)
<li> Teacher1 : WRN161, Teacher2 : WRN1611, Student : WRN1611 </li>
<ul>
<li> 90.0392 </li>
<li> 89.0268 </li>
<li> 89.3860 </li>
<li> í‰ê·  : 89.4840 </li>
<li> í‘œì¤€í¸ì°¨ : 0.4191 </li>
<br>
</ul>

<li> Teacher1 : WRN163, Teacher2 : WRN1631, Student : WRN1611 </li>
<ul>
<li> 90.0065 </li>
<li> 90.4311 </li>
<li> 89.7779 </li>
<li> í‰ê·  : 90.0718 </li>
<li> í‘œì¤€í¸ì°¨ : 0.2706 </li>
<br>
</ul>

### 500w, 14clsì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼
| Experiment (alpha = 0.3, 500w, 14cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| WRN16-1 (GAF ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬, 500w, 14cls)                                                      | 63.0491  | 63.3270  | 62.9797  | <u>**63.1186**</u> | 0.1501  |
| WRN16-3 (GAF ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬, 500w, 14cls)                                                      | 64.0389  | 63.9521  | 63.9173  | <u>**63.9694**</u> | 0.0511  |
| WRN28-1 (GAF ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬, 500w, 14cls)                                                      | 63.2922  | 64.0042  | 63.3096  | <u>**63.5353**</u> | 0.3316  |
| WRN28-3 (GAF ë‹¨ì¼ ë„¤íŠ¸ì›Œí¬, 500w, 14cls)                                                      | 64.1604  | 64.8029  | 65.3933  | <u>**64.7855**</u> | 0.5035  |
| T: WRN16-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.9703  | 67.6680  | 67.0429  | <u>**67.8937**</u> | 0.8029  |
| T: WRN16-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 68.2063  | 67.7201  | 68.3278  | <u>**68.0847**</u> | 0.2626  |
| T: WRN28-1(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.0082  | 66.4004  | 67.0603  | <u>**66.8230**</u> | 0.2996  |
| T: WRN28-3(GAF), S: WRN16-1(sig) (500w, 14cls)                                               | 67.5464  | 66.4872  | 67.7027  | <u>**67.2454**</u> | 0.5399  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.7864  | 70.6720  | 70.3247  | <u>**70.2610**</u> | 0.3643  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.1511  | 70.3421  | 70.5504  | <u>**70.3479**</u> | 0.1631  |
| T1: WRN28-1(GAF), T2: WRN28-1(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 69.2134  | 70.3073  | 69.7517  | <u>**69.7575**</u> | 0.4466  |
| T1: WRN28-3(GAF), T2: WRN28-3(Sig), S: WRN16-1(sig) (2-Teacher, 14cls)                       | 70.5504  | 70.3247  | 69.5781  | <u>**70.1511**</u> | 0.4155  |

### 500w, 7clsì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼
| Experiment (alpha = 0.3, 500w, 7cls)                                                         | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 500w)                                                 | 84.5852  | 84.8465  | 85.0425  | <u>**84.8247**</u> | 0.1873  |
| Single: WRN163 (Teacher Network, 7cls, 500w)                                                 | 85.4017  | 85.6956  | 86.1855  | <u>**85.7609**</u> | 0.3233  |
| Single: WRN281 (Teacher Network, 7cls, 500w)                                                 | 85.4344  | 84.7159  | 85.0098  | <u>**85.0534**</u> | 0.2949  |
| Single: WRN283 (Teacher Network, 7cls, 500w)                                                 | 86.8060  | 87.1326  | 86.0222  | <u>**86.6536**</u> | 0.4660  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.8106  | 90.3005  | 90.1045  | <u>**90.0719**</u> | 0.2013  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 500w)                                                | 89.9412  | 89.8106  | 89.1574  | <u>**89.6364**</u> | 0.3429  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0392  | 89.0268  | 89.3860  | <u>**89.4840**</u> | 0.4191  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 500w)                             | 90.0065  | 90.4311  | 89.7779  | <u>**90.0718**</u> | 0.2706  |

### 1000w, 7clsì— ëŒ€í•œ ì‹¤í—˜ ê²°ê³¼
| Experiment (alpha = 0.3, 1000w, 7cls)                                                        | Trial 1  | Trial 2  | Trial 3  | Mean               | Std     |
|----------------------------------------------------------------------------------------------|----------|----------|----------|--------------------|---------|
| Single: WRN161 (Teacher Network, 7cls, 1000w)                                                | 84.2913  | 84.9118  | 84.6179  | <u>**84.6070**</u> | 0.2534  |
| Single: WRN163 (Teacher Network, 7cls, 1000w)                                                | 85.4997  | 86.4468  | 86.4468  | <u>**86.1311**</u> | 0.4465  |
| Single: WRN281 (Teacher Network, 7cls, 1000w)                                                | 84.8465  | 84.8792  | 85.1078  | <u>**84.9445**</u> | 0.1162  |
| Single: WRN283 (Teacher Network, 7cls, 1000w)                                                | 86.4794  | 86.6101  | 85.9569  | <u>**86.3488**</u> | 0.2822  |
| T: WRN16-1(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3860  | 89.6473  | 89.7453  | <u>**89.5929**</u> | 0.1516  |
| T: WRN16-3(GAF), S: WRN16-1(Sig) (7cls, 1000w)                                               | 89.3534  | 88.7002  | 89.0268  | <u>**89.0268**</u> | 0.2667  |
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 91.0516  | 89.5820  | 89.9412  | <u>**90.1916**</u> | 0.6255  |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) (7cls, 1000w)                            | 90.6924  | 90.4637  | 90.3658  | <u>**90.5073**</u> | 0.1369  |

### ì¶”ê°€ì‹¤í—˜
<li> GENE_Active 14cls, 500w ë°ì´í„°ì— ëŒ€í•´ (T : WRN163, S : WRN161)ê³¼ (T : WRN281, S : WRN161)ì— ëŒ€í•´ì„œ ì¶”ê°€ ì‹¤í—˜ì„ ì§„í–‰í•¨ (ë†’ì€ ê²ƒì„ ìµœëŒ€í•œ ë½‘ì•„ë³´ê¸° ìœ„í•´ì„œ.) </li>
<li> T : WRN163, S : WRN 161 - 68.1195, 67.1992, 67.9285, 68.6578, 67.1644, 67.5638 </li>
<li> T : WRN281, S : WRN 161 - 67.0255, 67.1297, 67.4423, 67.5291, 66.2441 </li>


### alpha(ë‘ Teacherê°„ KD_lossì˜ ë¹„ì¤‘)ì„ ë°”ê¿”ê°€ë©° ì‹¤í—˜í•œ ê²°ê³¼
<li> alpha : 0.1 (T1(image)ê°€ 10%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.1 (T1(image)ê°€ 10%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.1 (T1(image)ê°€ 10%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>asdasdasd
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.1 (T1(image)ê°€ 10%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)ê°€ 50%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>asdasd
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)ê°€ 50%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)ê°€ 50%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.5 (T1(image)ê°€ 50%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)ê°€ 70%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)ê°€ 70%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)ê°€ 70%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.7 (T1(image)ê°€ 70%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)ê°€ 90%), T1 : WRN161, T2 : WRN1611, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)ê°€ 90%), T1 : WRN163, T2 : WRN1631, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)ê°€ 90%), T1 : WRN281, T2 : WRN2811, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>

<li> alpha : 0.9 (T1(image)ê°€ 90%), T1 : WRN283, T2 : WRN2831, S : WRN1611 </li>
<ul>
<li>  </li>
<li>  </li>
<li>  </li>
<li> mean :  </li>
<li> std :  </li>
</ul>


# Transformer ê³µë¶€ Attention is All You Need ë…¼ë¬¸
## 1ï¸âƒ£ Transformerê°€ ë“±ì¥í•œ ì´ìœ 
### âœ… ê¸°ì¡´ RNN, LSTMì˜ í•œê³„ì 
1. **ê¸´ ë¬¸ì¥ì„ ë‹¤ë£° ë•Œ ì •ë³´ ì†ì‹¤ ë¬¸ì œ**
   - RNN, LSTMì€ **ì…ë ¥ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬**í•˜ë¯€ë¡œ **ê¸´ ë¬¸ì¥ì—ì„œ ì•ìª½ ì •ë³´ê°€ ë’¤ìª½ì— ì˜ ì „ë‹¬ë˜ì§€ ì•ŠìŒ**  
   - (Long-Term Dependency ë¬¸ì œ ë°œìƒ)
   
2. **ë³‘ë ¬ ì—°ì‚° ë¶ˆê°€ëŠ¥ â†’ í•™ìŠµ ì†ë„ ëŠë¦¼**
   - RNN/LSTMì€ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµë˜ë¯€ë¡œ, **GPU ë³‘ë ¬ ì—°ì‚°ì´ ì–´ë ¤ì›€ â†’ í•™ìŠµ ì†ë„ê°€ ëŠë¦¼**
   
3. **Gradient Vanishing ë¬¸ì œ**
   - ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ **ì´ˆë°˜ ì…ë ¥ì˜ Gradientê°€ ì‚¬ë¼ì§€ëŠ” ë¬¸ì œ ë°œìƒ â†’ í•™ìŠµ ì–´ë ¤ì›€**

---

### âœ… TransformerëŠ” ì–´ë–»ê²Œ í•´ê²°í–ˆë‚˜?
1. **Attention Mechanism (Self-Attention) í™œìš©**
   - ëª¨ë“  ë‹¨ì–´ê°€ **ë‹¤ë¥¸ ë‹¨ì–´ì™€ ì–¼ë§ˆë‚˜ ì—°ê´€ì´ ìˆëŠ”ì§€ ë™ì‹œì— ê³„ì‚°**  
   - â†’ **ë©€ë¦¬ ë–¨ì–´ì§„ ë‹¨ì–´ë“¤ ê°„ì˜ ê´€ê³„ë„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥**
   
2. **ìˆœì°¨ ì²˜ë¦¬(X), ë³‘ë ¬ ì—°ì‚°(O) ê°€ëŠ¥**
   - ëª¨ë“  ë‹¨ì–´ë¥¼ **ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥ â†’ GPU ê°€ì† ìµœì í™”**
   
3. **Residual Connection ì‚¬ìš© â†’ Gradient Vanishing ë°©ì§€**
   - ê° Layerì—ì„œ **ì›ë³¸ ì…ë ¥ì„ ë”í•´ì¤Œ** â†’ ì •ë³´ ì†ì‹¤ ë°©ì§€

---

## 2ï¸âƒ£ Transformerì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œ
1. **Positional Encoding** â†’ Input Embedding ì‹œ ë‹¨ì–´ ìˆœì„œ ì •ë³´ë¥¼ ë°˜ì˜
2. **Multi-Head Self-Attention** â†’ ë‹¨ì–´ ê°„ì˜ ì—°ê´€ì„±ì„ ì—¬ëŸ¬ Headë¥¼ í†µí•´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í•™ìŠµ
3. **Feed Forward Network (FFN)** â†’ ë¹„ì„ í˜• ë³€í™˜ì„ í†µí•´ í‘œí˜„ë ¥ ì¦ê°€ + ê¹Šì´ ìˆëŠ” í•™ìŠµ ê°€ëŠ¥
4. **Residual Connection + Layer Normalization** â†’ ì•ˆì •ì ì¸ í•™ìŠµ

---

# 3ï¸âƒ£ Transformerì˜ Encoder ë™ì‘ ê³¼ì • (ì…ë ¥ â†’ Context ë²¡í„° ìƒì„±)

### âœ… ì˜ˆì œ: ì…ë ¥ ë¬¸ì¥
> **"I am a teacher" (ì˜ì–´) â†’ "ë‚˜ëŠ” ì„ ìƒë‹˜ì´ë‹¤" (í•œê¸€)ë¡œ ë²ˆì—­í•œë‹¤ê³  ê°€ì •**

### ğŸŸ¢ Step 1: Input Embedding + Positional Encoding
- ê° ë‹¨ì–´ë¥¼ **Embedding ë²¡í„°(ê³ ì°¨ì› í‘œí˜„)ë¡œ ë³€í™˜**
- TransformerëŠ” ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— **Positional Encoding ì¶”ê°€**
- ìµœì¢… ì…ë ¥ ë²¡í„° = `Embedding Matrix + Positional Encoding`

---

### ğŸŸ¢ Step 2: Multi-Head Self-Attention (ìê¸° ìì‹ ì—ê²Œ Attention)
> **"I am a teacher" ë¬¸ì¥ì—ì„œ ë‹¨ì–´ë“¤ ê°„ì˜ ì—°ê´€ì„±ì„ í•™ìŠµ**

âœ… **Query, Key, Value í–‰ë ¬ ìƒì„±**
- Self-Attentionì„ ìˆ˜í–‰í•˜ì—¬ **ê° ë‹¨ì–´ê°€ ë¬¸ì¥ ë‚´ ë‹¤ë¥¸ ë‹¨ì–´ì™€ ì–¼ë§ˆë‚˜ ê´€ë ¨ ìˆëŠ”ì§€ ê³„ì‚°**

âœ… **Scaled Dot-Product Attention ìˆ˜í–‰**
- Attention Score ê³„ì‚°: Attention(Q, K, V) = softmax((Q * K^T) / sqrt(d_k)) * V
- Softmaxë¥¼ ì ìš©í•˜ì—¬ **ëª¨ë“  ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ì •ê·œí™”í•˜ê³ , ê° ì¤‘ìš”ë„ì— ë”°ë¼ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬**
- **ëª¨ë“  ë‹¨ì–´ì˜ Valueë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìƒˆë¡œìš´ ë²¡í„°ë¥¼ ìƒì„±**

âœ… **Multi-Head Attention (hê°œë¡œ ë¶„í• í•˜ì—¬ ë‹¤ë¥¸ ì˜ë¯¸ í•™ìŠµ)**
- ê° HeadëŠ” **ì„œë¡œ ë‹¤ë¥¸ ë¬¸ë§¥ ì •ë³´ë¥¼ í•™ìŠµ** (CNNì˜ í•„í„°ì²˜ëŸ¼)
- ì—¬ëŸ¬ ê°œì˜ Attention ê²°ê³¼ë¥¼ Concatí•˜ì—¬ ìµœì¢… Attention ë²¡í„° ìƒì„±

---

### ğŸŸ¢ Step 3: Feed Forward Network (FFN)
> **Self-Attentionì„ ê±°ì¹œ ë²¡í„°ë¥¼ ë”ìš± ì •ì œëœ í‘œí˜„ìœ¼ë¡œ ë³€í™˜**

âœ… **ë¹„ì„ í˜• ë³€í™˜ì„ ì¶”ê°€í•˜ì—¬ í‘œí˜„ë ¥ì„ ì¦ê°€**
- FFN(X) = max(0, X * W1 + b1) * W2 + b2
- CNNê³¼ DNNì˜ í™œì„±í™” í•¨ìˆ˜ì²˜ëŸ¼, FFNì—ì„œë„ ë¹„ì„ í˜•ì„±ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì„ ì¦ê°€ì‹œí‚´
- ë” í’ë¶€í•œ íŠ¹ì§•ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë³€í™˜

âœ… **Residual Connectionê³¼ Layer Normalizationì„ ìˆ˜í–‰í•˜ì—¬ ì •ë³´ ì†ì‹¤ ë°©ì§€**

---

# 4ï¸âƒ£ Transformerì˜ Decoder ë™ì‘ ê³¼ì • (Context ë²¡í„° â†’ ë¬¸ì¥ ìƒì„±)

### âœ… Decoderì˜ ì…ë ¥
- í›ˆë ¨í•  ë•ŒëŠ” **ì •ë‹µ ë¬¸ì¥ì˜ ì•ë¶€ë¶„**ì´ Decoderì˜ ì…ë ¥ì´ ë¨
- ì˜ˆì¸¡í•  ë•ŒëŠ” **Decoderê°€ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©**
- ì˜ˆ) `"<SOS> ë‚˜ëŠ”"` â†’ `"ë‚˜ëŠ” ì„ ìƒë‹˜"` â†’ `"ë‚˜ëŠ” ì„ ìƒë‹˜ì´ë‹¤"`

---

### ğŸŸ¢ Step 1: Decoder Input Embedding + Positional Encoding
- Decoderë„ ì…ë ¥ì„ **Embedding ë²¡í„°ë¡œ ë³€í™˜ í›„, Positional Encodingì„ ì¶”ê°€**
- `"<SOS> ë‚˜ëŠ”"` â†’ Embedding â†’ Positional Encoding ì¶”ê°€

---

### ğŸŸ¢ Step 2: Masked Multi-Head Self-Attention
> **Decoderê°€ "ë‚˜ëŠ”"ì„ ì˜ˆì¸¡í•  ë•Œ, "ì„ ìƒë‹˜"ì„ ë³´ì§€ ëª»í•˜ë„ë¡ Masking ì ìš©(Training ê³¼ì •ì—ì„œ)**

âœ… **Masked Self-Attentionì´ë€?**
- í˜„ì¬ê¹Œì§€ ìƒì„±ëœ ë‹¨ì–´ê¹Œì§€ë§Œ ê³ ë ¤í•˜ë„ë¡ Future Tokenì„ ê°€ë¦¬ëŠ” Mask ì ìš©
- Softmax ì—°ì‚° ì‹œ, ë¯¸ë˜ ë‹¨ì–´ëŠ” **-âˆ (ë¬´í•œëŒ€ë¡œ ë‚®ì€ ê°’)** ì²˜ë¦¬í•˜ì—¬ ê³ ë ¤ë˜ì§€ ì•Šë„ë¡ í•¨

---

### ğŸŸ¢ Step 3: Encoder-Decoder Attention
> **Encoderì—ì„œ ìƒì„±í•œ Context ë²¡í„°ë¥¼ ì°¸ê³ í•˜ì—¬, ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡**

âœ… **Decoderì˜ Query, Encoderì˜ Key, Value ì‚¬ìš©**
- Query: Decoderì—ì„œ Self-Attentionì„ ìˆ˜í–‰í•œ í›„ ë²¡í„°
- Key & Value: Encoderì˜ ìµœì¢… ì¶œë ¥ (ì…ë ¥ ë¬¸ì¥ì˜ Context ë²¡í„°) # **Encoderì˜ Valueë¥¼ ì‚¬ìš©í•œë‹¤ëŠ”ê²ƒì€ ê¼­ ê¸°ì–µí•´ì•¼ í•¨.(ValueëŠ” Encoderì˜ ì •ë³´ì„)**
- Decoderì˜ Queryì™€ Encoderì˜ Keyë¥¼ Attention í•˜ì—¬ ë‚˜ì˜¨ ê²°ê³¼ì™€ Encoderì˜ Valueë¥¼ ê°€ì¤‘í•©í•œ ê²°ê³¼ë¥¼ FFNì— í†µê³¼ì‹œí‚¨ ë’¤ ê°€ì¥ ê°€ê¹Œìš´ ë‹¨ì–´ë¥¼ ì°¾ìœ¼ë©´ ê·¸ê²ƒì´ Predict ë‹¨ì–´ê°€ ëœë‹¤.

âœ… **Encoderì—ì„œ ì–»ì€ Context ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë”ìš± ì •í™•í•œ ë‹¨ì–´ ì˜ˆì¸¡**
- `"ë‚˜ëŠ”"`ì´ ë“±ì¥í–ˆì„ ë•Œ, `"ì„ ìƒë‹˜"`ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ì•„ì§€ë„ë¡ ì¡°ì •ë¨

---

### ğŸŸ¢ Step 4: Feed Forward Network (FFN)
> **Attentionì„ ìˆ˜í–‰í•œ ë²¡í„°ë¥¼ ë”ìš± ì •ì œëœ í‘œí˜„ìœ¼ë¡œ ë³€í™˜**

âœ… **Encoderì™€ ë™ì¼í•œ FFN ì ìš©**
- ReLU í™œì„±í™” í•¨ìˆ˜ + ë‘ ê°œì˜ Linear Layer ì‚¬ìš©
- ìµœì¢…ì ìœ¼ë¡œ ë¬¸ë§¥ ì •ë³´ë¥¼ ë”ìš± ì˜ ë°˜ì˜í•œ ë²¡í„°ë¥¼ ìƒì„±

---

### ğŸŸ¢ Step 5: Softmaxë¥¼ í†µí•´ ë‹¨ì–´ ì˜ˆì¸¡
> **Decoderì˜ ìµœì¢… ì¶œë ¥ì„ Softmaxì— í†µê³¼ì‹œì¼œ ê°€ì¥ ì ì ˆí•œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡**

âœ… **Softmaxë¥¼ í†µí•´ í™•ë¥  ë¶„í¬ ê³„ì‚°**
- P(ì„ ìƒë‹˜ | ë‚˜ëŠ”) = exp(z_ì„ ìƒë‹˜) / sum(exp(z_i))
- ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ë‹¨ì–´ `"ì„ ìƒë‹˜"`ì„ ì„ íƒ

âœ… **ì´ì „ê¹Œì§€ ìƒì„±ëœ ë‹¨ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ê³  ë°˜ë³µí•˜ì—¬ ìµœì¢… ë¬¸ì¥ ìƒì„±**
- `"ë‚˜ëŠ” ì„ ìƒë‹˜"` â†’ `"ë‚˜ëŠ” ì„ ìƒë‹˜ì´ë‹¤"`

---

# Transformer ì „ì²´ ê³¼ì • ìš”ì•½
1. **Encoder**
 - ì…ë ¥ ë¬¸ì¥ì„ **ë²¡í„°ë¡œ ë³€í™˜**
 - Self-Attentionì„ ìˆ˜í–‰í•˜ì—¬ ë‹¨ì–´ ê°„ ê´€ê³„ í•™ìŠµ
 - FFNì„ í†µí•´ ë”ìš± ì •ì œëœ ë²¡í„° ìƒì„±

2. **Decoder**
 - ì´ì „ê¹Œì§€ ìƒì„±ëœ ë‹¨ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
 - Masked Self-Attention ìˆ˜í–‰
 - Encoder-Decoder Attentionì„ í†µí•´ ì…ë ¥ ë¬¸ì¥ ì •ë³´ ë°˜ì˜
 - FFNì„ í†µí•´ ìµœì ì˜ ë²¡í„° ìƒì„± í›„ Softmaxë¥¼ í†µí•´ ë‹¨ì–´ ì˜ˆì¸¡

---

# âœ… ê²°ë¡ 
TransformerëŠ” **ë¬¸ì¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„(Encoding)í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì„±(Decoding)í•˜ëŠ” ëª¨ë¸**ì´ë‹¤.
- **Self-Attention** â†’ ë¬¸ë§¥ì„ ë°˜ì˜í•œ ë²¡í„° ìƒì„±
- **Multi-Head Attention** â†’ ë‹¤ì–‘í•œ ì˜ë¯¸ë¥¼ í•™ìŠµ
- **Residual Connection & FFN** â†’ ì •ë³´ ì†ì‹¤ ë°©ì§€ & í‘œí˜„ë ¥ ì¦ê°€

ğŸš€ **ì¦‰, "ì…ë ¥ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜ â†’ ë³€í™˜ëœ ë²¡í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ìƒì„±"í•˜ëŠ” ê²ƒì´ Transformerì˜ í•µì‹¬ ì›ë¦¬!** ğŸš€


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 ì½”ì–´, 12 ë…¼ë¦¬ í”„ë¡œì„¸ì„œ </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>