```python
- 아침줌 내용
-- 한 epoch에서, 2 step으로 나눔
   1 step : Recon에 집중하는 step.
   2 step : Cls만 집중하는 step. 
   - 2 step에서는 1step에서 학습된 얘들은 freeze
   - x_recon된 얘를 내가 전에 학습했던 wrn161을 freeze한 얘에 흘려보내서 acc를 확인해보면 recon이 얼마나 잘 되었는지 알 수 있음.
   - VQShape의 모델 가중치를 이용해서 바로 해봤을 때 GENEActiv의 데이터셋에서의 결과를 보는게 중요.
 
   -- UEA의 데이터셋에서 HAR관련된 데이터셋에서, Text를 출력해놔보고, 나중에 Text를 추가해봤을 때 어떻게 바뀌나 볼 수 있을 것임.

   -- Codebook을 quantified하게 평가를 할 수 있어야 하는데.. 그거는 나중에 생각해보자.

   -- UEA로 학습한 가중치를 사용하는 느낌으로.

   -- 3-Channel을 Convolution방식을 이용해서 어떻게 잘 합칠 수 있을까?

   -- Recon된 결과를 정성/정량 적으로 평가할 수 있는 방법? → 이게 Classification? MSE로 해볼 수 있나?
```

## VQShape
### 해야 할 것
1. UEA에서 학습한 가중치로 Transformer라던가 하는 Signal Encoder를 처리할 수 있을지 확인.
    - 여기서 아마 안될건데, 그러면 UEA를 GENEActiv에 맞게 바꿔서 하면 되려나? 모르겠네.
    - 아니면 GENEActiv를 input 512로 바꾸던가 해봐야할지도.
    - → 쓸 수 있는 가중치만 사용할 예정 (Transformer)
2. Classification을 하는 것을 Tokens 방식이 아니라, Recon된 x를 다시 pretrained WRN16-1같은데에 넣어서 수행하여 loss를 흘려보냄.
3. 위의 것들을 Flatten, Channel-wise 로 다 해볼 예정.

### 몇개 결과
- 가중치는 UEA에서 사용한 것 말고, 전에 GENEActiv에서 Reconstruction만 했을 때 가장 잘 됐던거로 사용하였고, UEA에서 학습한 가중치 중 중복으로 사용이 되는 Transformer에만 불러옴.
- Frozen시킨 것은 하나도 없고, 그냥 Student(WRN1611)모델만 불러와서 x_recon된것을 넣은 뒤 Classification 수행하게 하고, CE_loss 얻음.
- 앞에 20epoch까지는 CE_loss비중을 0으로 두고, 이후에 1로 올림. (전체 epoch은 200까지로 대폭 늘림.)
- Acc가 상당히 높아짐(저번주에 classifier(Linear)가 Tokens를 받아서 수행한 경우 14%) → 55%정도까지 오름.

- x_reconstruction은 이렇게 나왔는데 별 의미는 없어보인다.
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/12월/25.12.26/x_recon_1000.png" alt="results" width="700">
- s_recon도 이렇게 나왔는데 전에랑 큰 차이는 없음.
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/12월/25.12.26/s_recon_1000.png" alt="results" width="700">
- Accuracy가 상당히 높아졌는데 loss들 비중을 봤을 때 VQ-VAE loss(z_loss)가 상당히 높았음. 이게 z_hat과 Codebook 비슷해지게 하는 loss인데, 줄여봐야할지 늘려봐야할지 고민.
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/12월/25.12.26/log.png" alt="results" width="700">
- Codebook 사용 빈도는 비슷한 것 같다. (Classification을 미포함했을 때와.)
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/12월/25.12.26/code_usage.png" alt="results" width="700">


## 3Teacher 논문 진행상황
### Discussion & Conclusion Part
- Discussion은 작성을 하긴 했는데, 더 다듬고, 내용도 좀 바꾸거나 해야 할 것으로 보임.
- Conclusion도 마찬가지, Future work는 아직 미정

### 해야할 남은 것.
- PI이미지 샘플로 보여주는 부분 이미지 교체할거 찾아보기.
- Proposed쪽 완전 바꾸기.
    1. Image Representation Extraction
        - PI
        - GAF
    2. Knowledge Transfer Strategy
        - Single Teacher
        - Multiple Teacher
    3. Annealing Strategy
- histogram(wrn hetero) legend 위치 가로로 + y-max 줄이기 
- reference, processing time 표 수정
- Conclusion쪽에 noise처럼 되는 모달리티를 어떻게 개별 batch나 샘플에 적용할 수 있을지에 대한 future work같은 내용


### 해야 할 것 (25.12.26)
- 3Teacher - 3 → 12월 안에 초안 완성. # Architecture에 대한 설명 Introduction에서 살짝, Proposed Method 에서는 간략한 그림으로 설명 → 이거 해야함.
                    + Conclusion 작성
                    + 참고문헌 하나씩 달기.
- MMVQShape - 3 → Transformer 모델들은 UEA-multivariate에서 학습했던 가중치를 사용하는 방향으로. (가져다 쓸 수 있는게 transformer밖에 없는듯 하다.)
                    + Classifier는 x_recon된 얘를 wrn16-1에 넣어서 성능을 확인했음.
                    + Hyperparameter(loss 가중치)는 여러 가지 버전으로 하려고 함.
