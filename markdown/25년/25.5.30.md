## 이번주 하려고 하는 것.
<li> PAMAP2 3 Teacher 에서 최적의 alpha값 조합 찾기 </li>
<li> Baseline으로 사용할만한 KD 기법 찾기 + EBKD, CAMKD 코드 내거에 적용해보기. </li>
<li> 생성형 AI 공부 및 실습코드 </li>

## PAMAP2 3Teahcer에서 Alpha값 찾기.
- GENEActiv때랑 똑같은 조합들에 대해서 (226, 235, 325, 334, 333) 에 대해서 할것임.

## 학습 진행 상황
- 계속 쭉 진행중...
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.5.30/PAMAP.png" alt="PAMAP" width="700">

## 생성형 AI쪽 공부
- Stable Diffusion 공부 및 MNIST 실습 해보려고 함
- 그 다음에 Text Embedding까지 넣는 pretrained 모델에 적용해보려고 함.
- https://github.com/wjdwocks/ML-DNN/blob/main/markdown/my_study/Stable_Diffusion.md


## Baseline으로 사용할만한 KD 기법 찾기.
- EBKD, CAMKD 코드 받은거 적용해보기.
- CAMKD (여러 Teacher 중 더 똑똑한 Teacher의 Knowledge를 더 많이 반영하도록 하는 방식)
    * 각 Teacher 로부터 CE_Loss를 계산함. → Confidence 측정
    * 그 Teacher들의 CE_Loss들을 가지고, 1 - (Softmax)연산을 통해 loss가 더 낮은 Teacher의 가중치를 더 높게 설정함. → Attention 가중치 계산
    * 각 Teacher들의 KD_Loss를 계산함. 
    * 위에서 얻은 가중치로 KD_Loss의 비중을 결정한다.
    * Student의 CE_Loss와 얻어진 KD_Loss를 합쳐서 최종 Loss가 구성됨.
    * 얘는 쉬워서 적용완료
- EBKD (아직 안봄..)
- Frequency Attention for Knowledge Distillation (2024) 이거도 한번 해보려고 하는데 생각보다 어렵다...
    * FAMKD를 적용해보려고 했는데, 생각보다 적용하기 어렵다.
    * 좀 더 시간을 두고 코드작업을 해봐야할듯
    * FAMKD는 Student의 중간 Feature를 FAM모듈을 이용한(torch.fft) 주파수 표현으로 변환한다.
    * 그 후 Local Attention(자기 내부 Feature Map간 Self-Attention?) or Cross Attention(Teacher의 중간 Feature와의)을 수행한 후, Teacher의 Feature Map과 MSE를 계산하여 Loss항에 추가한다. (Teacher랑 Cross Attention을 통해 Fusion 시키는게 Review Distillation 방식이라고 함.)
    * 여기서 Multi Teacher에 적용 가능할 거라고 생각한 이유
        - 이 Attention을 통해 Student를 변환하고, 그 Key로 사용한 Teacher와 MSE를 계산하는 것을 각 Teacher에 가중치를 둬서 더할 수 있기 때문.
        - 근데, 1D CNN과 2D CNN은 모델 구조가 다름. 2D(b, c, w, h), 1D(b, c, t) 
        - 1D를 (b, c, t, t)로 변환한 후, t를 pooling이나 interpolation을 통해서 w, h와 크기를 맞춰주면 될거라고 생각함.
        - 나머지 코드는 github에 있는 코드를 직접 적용할 수 있을거라고 생각함.

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.5.30/FAMKD.png" alt="FAMKD" width="700">