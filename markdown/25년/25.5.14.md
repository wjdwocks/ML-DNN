## 이번주 하려고 하는 것.
<li> PAMAP2 alpha = 0.7에서 ann, annsp 를 수행. </li>
<li> PAMAP2 3Teacher 코드 작성 </li>
<li> 저번에 못한 VLM 논문리뷰한거 보기 </li>

## PAMAP2 spkd 적용 후 생긴 문제.
<li> WRN16-1 기준 gpu 메모리가 13000이 됨. </li>
<li> WRN16-3 기준 GPU의 메모리 초과가 발생함. </li>
<li> 아마도 PAMAP2 데이터가 엄청 많은데, 각 batch마다 SP Map을 생성하고, 이것들을 이용한 MSE loss들의 기울기들을 저장하고, backward() 때 사용하기 때문에, batch가 많아짐에 따라서 그 미분연산들이 엄청 많아진 거라고 생각함. </li>
<li> sp1t이런 teacher들에서 뽑은 얘들에 detach()를 해줘서 어차피 Teacher는 학습되지 않기에 계산조차 하지 않게 함. </li>
<li> 추가적으로 Teacher.eval()을 해도 cuda가 자동으로 기울기 연산을 추적하는데(auto_grad) Teacher의 Forward 연산을 수행할 때 torch.no_grad를 해줌으로써 이 계산을 없앰으로서 연산(학습)속도 + 메모리 최적화 를 수행해줌. </li>
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.5.14/no_grad.png" alt="no_grad" width="700">

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.5.14/detach.png" alt="detach" width="700">




## What matters when building vision-language models 논문 리뷰 보기.
- 2024년에 나온 논문임.
- VLM에 대한 관심이 증가하고 있지만, VLM 설계에 대한 중요한 결정들에 대한 근거가 부족한 것 같다고 한다.
- 이 문제를 해결하기 위해서 pretrained 모델, 아키텍처의 선택, 데이터 및 학습 방법에 대해서 실험을 통해 이 이것들의 선택에 대한 성능 비교를 한다고 한다.
- 아직 자세히는 못봤다...
