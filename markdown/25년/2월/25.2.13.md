### 이번주 하려고 하는 것.
<li> SPKD 논문 + 교수님 코드를 보며 중간 Layer의 Feature Map간의 비교를 통해 학습하는 코드 작성 후 실험 (GENE 7cls, 14cls) (ㅇ) </li>
<li> GENE Activ 7cls(500w, 1000w), 14cls에 대해서 Annealing을 적용한 2 Teacher(GAF, Sig)학습. (ㅇ) </li>
<li> PAMAP 데이터셋에 대해 Teacher로 사용할 네트워크 학습. (0 ~ 8 OVR로 9번 x 3 trial로 총 27개의 학습) (ㅇ) </li>
<li> Object Detection에 대해서 공부해보려고 한다. (Yolo, 어떤 식의 데이터로 어떤 식의 학습이 이루어지는가.) (ㅇ) </li>
<li> VAE, WAE에 대해 MNIST로 공부 (x) </li>


### SPKD에 대해서
1. SPKD는 Teacher와 Student의 특정 Layer를 지난 output 끼리 서로 MSE로 비교하여 비슷해지도록 하는 loss값을 추가함.
2. 그렇다면 특정 Layer를 지난 출력의 크기나, 깊이가 서로 다르다면? <<< 우리 코드에서는 어떻게 했는지도 보자.
3. 원래라면 크기가 다르다면, Interpolation이나, AvgPool, Upsample, ConvTranspose2d를 사용하여 넘겨줘서 크기를 맞춰준다.
4. 깊이가 다르다면, 1x1 Conv를 사용하여 Channel을 동일하게 변환한다.
5. 우리 코드에서는 깊이나 크기가 달라질 경우에 별개로 conv_inp() layer를 통해 맞춰주는 작업을 함.




## Object Detection 공부.
### Object Detection의 기본
- Bounding Box란? : 이미지 안에서 찾고자 하는 Object의 위치를 네모 박스로 표현하는 개념.
  - Object Detection 모델의 목표는 이 Bounding Box를 정확하게 예측하는 것이다.
  - 또한, Bounding Box는 다음과 같은 방식으로 표현된다.
  - (x_min, y_min, x_max, y_max) 또는, (x_center, y_center, width, height). 
  - 하지만, YOLO에서는 (x_center, y_center, width, height)의 방식을 주로 사용한다.


- IoU란? : Intersection over Union으로, Ground Truth와 Predicted Box가 얼마나 겹치는지 계산하는 지표.
  - 값이 1에 가까울수록 예측이 정확함을 의미한다.
  - Ground Truth는 정답 label(Bounding Box)을 의미하고, Predicted Box는 모델이 예측한 Bounding Box를 의미한다.


- NMS란? : Non-Maximum Suppression으로, Object Detection에서 중복된 Bounding Box를 제거하는 알고리즘이다.
  - Object Detection 모델은 일반적으로, 한 이미지에서 Object를 찾을 때, 하나의 객체에 대해 여러 개의 Bounding Box를 예측하는 경우가 많다.
  - 이때, 가장 신뢰도가 높은(Confidence Score가 높은) Bounding Box만 남기고, 나머지는 제거하는 과정을 의미한다.
  - 이것을 NMS가 자동으로 수행해준다.
  - 즉, NMS란, 모델이 최종 예측을 수행할 때 존재하는 Bounding Box들 중 하나를 선택하기 위한 것이다. (학습 때는 진행되지 않는다.)
  - 당연히 학습 때에는 존재하는 모든 Bounding Box들(Parameter)이 GT와 IoU가 높아질 수록 좋기 때문이다.


- Confidence Score : 특정 Bounding Box가 실제로 객체를 포함하는지를 예측하는 점수.
  - Confidence Score = P(Object) x IoU(Pred, GT)
  - P : 이 Grid Cell 안에 객체가 존재할 확률
  - IoU(Pred, GT) : 예측된 Bounding Box와 실제 정답(GT) 박스의 IoU
  - 즉, Confidence Score는 Bounding Box마다 가지는 점수이고, P(자신이 속해 있는 Grid Cell이 객체를 포함할 확률) x IoU(자신과 GT와의 IoU점수) 로 계산이 된다.


- Yolo의 동작 원리 (Prediction)
  1. YOLO는 이미지를 S x S 크기의 겹치지 않는 Grid로 나눈다.
  2. 그 Grid Cell들이 찾고 싶은 Object(객체)의 중심을 가지고 있는지에 대한 확률을 얻고, 가중치 Parameter를 통해 Bounding Box를 생성한다.
  3. 그 확률을 토대로, 각 Bounding Box의 Confidence Score를 계산한다.
  4. 각 Bounding Box의 Confidence Score가 특정 값(Threshold)를 넘는다면 그 Bounding Box는 두고, 아니라면 제거한다. (학습 때는 Threshold를 사용하지 않고 모두 챙겨간다.)
  5. 그렇게 Threshold를 넘는 Bounding Box만 남게 되고, 그 Bounding Box들은 loss 값을 통해 IoU가 높아지는 방향으로 조정이 된다.
  6. 이 때 남은 Bounding Box들은 굳이 Grid Cell 내부로만 한정되지 않기 때문에 여러 개의 Bounding Box가 유지되고, 조정된다.
  7. 학습이 끝난 이후, Validation 단계에서 NMS가 겹치는 여러 Bounding Box들(한 객체에 여러 Bounding Box가 있을 것임) 중 가장 Confidence Score가 높은 Bounding Box하나만을 남겨, 최종 예측 결과를 내놓게 된다.


- Yolo의 동작 원리 (Training)
  1. 각 Epoch마다 YOLO는 이미지를 S x S 크기의 겹치지 않는 Grid로 나눈다.
  2. 그 Grid Cell들이 찾고 싶은 Object(객체)의 중심을 가지고 있는지에 대한 확률(P(Object))을 얻고, 가중치 Parameter를 통해 Bounding Box를 생성한다. (Anchor Box개념의 도입도 공부하자.)
  3. 모든 Bounding Box의 Parameter 가중치와 P(Object)을 예측하는 Parameter들은 loss 값을 통해 업데이트된다.
  4. loss 값에는 Bounding Box의 위치 손실(Localization Loss), 객체 존재 여부 손실(Objectness Loss, P(Object)), 클래스 예측 손실(Classificaion Loss)가 포함된다.



- mAP란? : Mean Average Precision으로, Object Detection 모델의 성능을 평가하는 대표적인 지표.
  - Precision과 Recall 개념을 확장한 것으로, 여러 클래스에 대해 모델의 평균 Acc를 나타낸다.
  - Precision-Recall 곡선 아래의 면적을 계산한 값이 AP인데, 여러 개의 클래스(객체)에서 AP값을 평균 낸 값을 mAP라고 함
  - 이 때 Object Detection에서는 클래스를 예측(0 or 1)하는 것이 아닌, Bounding Box의 IoU를 예측하는 것이므로, Precision과 Recall을 어떻게 구현하는지가 의문임.
  - 그래서 YOLO에서는 mAP@0.5, mAP@0.75와 같이 (IoU가 0.5나 0.75 이상이라면 정답으로 인정)하는 등의 기준을 사용한다.
  - 그렇다면 각 Grid Cell이 가지는 B개의 Bounding Box에서 B도 조절가능한 파라미터일까?
  - B는 하이퍼 파라미터라서, 학습하면서 바뀌지는 않고, Bounding Box의 선택 방식을 조정하게 된다. YOLOv2부터는 Anchor Box 개념을 사용하여 다양한 크기의 객체를 탐지하는데, 특정 데이터셋에서 작은 객체 탐지가 중요한 경우, Anchor Box 크기를 조정하여 Recall을 높일 수 있다고 한다.


- Anchor Box 알고리즘이란?
  - YOLOv1에서는 Grid Cell이 직접 Bounding Box의 위치, 크기를 모두 예측했다.
  - 하지만 이러면 한 이미지 안에 작은 Object와 큰 Object가 모두 있는 경우 특정 크기의 객체만 잘 탐지되고, 다양한 크기의 객체를 잘 탐지하지 못함.
  - YOLOv1에서는 Bounding Box의 크기가 학습 초기에는 랜덤하게 설정되기 때문에, 매우 비효율적이었다.
  - Anchor Box에서는 미리 정해진 크기와 비율의 Anchor Box(기준 박스)를 사용한다.
  - 즉, 기존에는 B = 3이더라도, 완전히 랜덤한 크기를 가진 Bounding Box를 생성했지만, YOLOv2부터는 B = 3이라면, 각 Bounding Box들이 서로 다른 정해진 크기의 Anchor Box(12 x 12, 24 x 24, 36 x 36 이런느낌)로 초기화가 된다.
  - 여기서 각 Anchor Box의 크기는 YOLO모델이 K-Means Clustering으로 자동 설정된다.
  - 또한 이후에 크기에 맞는 객체로 맞춰지는 학습이 진행되게 된다.
  - 또한, 각 Bounding Box들은 처음 초기화된 후 가장 적절한 클래스를 목표로 학습이 진행된다. 즉, 처음 초기화 된 후 Bounding Box는 P(Object), Bounding Box위치, P(Class)를 예측하고 GT와 가장 IoU가 높은 Anchor Box가 해당 객체를 학습한다.
  - 즉, 각 GT는 자신과 가장 IoU가 높은 Anchor Box와 연결되어 해당 Box가 학습하도록 하지만, GT와 연결되지 않은 Bounding Box들은 Confidence Score를 낮추는 방향으로 학습이 진행된다. (Negative loss)



### SP를 사용한 학습
1. sp_param이 GENE_Activ에는 700, PAMAP2에는 200으로 설정이 되었는데 이것이 무엇일까?
2. sp_param(β)은 앞의 CE_Loss(Student)와 KD_Loss(T1, T2, S)간의 가중치는 λ로 관리하는데, 그 때 SP_Loss의 가중치를 몇으로 설정해야 학습이 골고루 잘 될까 하는 하이퍼 파라미터이다.
3. α는 두 Teacher 간 logits loss를 합칠 때의 가중치
4. k는 분할 개수(?) : Feature Representation을 나누는 개수. 이게 4였기 때문에, 우리 코드에서 최종 출력(1), At1(2), At2(3), At3(4)으로 총 4개임.

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/2월/25.2.13/TPKD_sp_param.png" alt="sp_param이 뭐냐" width="500">




## PAMAP2 데이터
### 데이터 형태
1. 83319개의 데이터와 각 데이터는 40개의 Feature를 가지고 있다.
2. 83319개의 데이터는 0 ~ 11에 포함하는 라벨을 각각 가지고 있다.
3. 데이터셋에서 9명의 사람으로부터 83319개의 데이터를 얻은 것이다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/2월/25.2.6/PAMAP.png" alt="PAMAP shape" width="500">

### 학습할 때 데이터의 사용
1. 9명의 사람은 0 ~ 8의 id로 구분되고, 학습을 할 때 그 중 한명을 Test_id로 지정하여 남은 8명은 Train set으로 학습한다.
2. 이렇기 때문에, 같은 사람의 데이터를 학습하고, 테스트하게 되는 불상사는 일어나지 않음.
3. 같은 사람에서 나온 데이터를 학습하고, 테스트때 사용한다면, Acc가 높게 나올 확률이 높다. (GENE_Active때 겪어본 문제.)

### 데이터의 사용
1. .data 파일의 형식으로 저장되어있다. (GENE_Active랑 다르다.)
2. 이렇게 .data 파일로 되어있는 경우 .npy파일과 다르게 pickle.load를 통하여 데이터를 얻어왔다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/2월/25.2.6/PAMAP_pickle.png" alt="PAMAP Pickle load" width="500">
3. 근데 cls_id = [24, 1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17] 인데, 원래 PAMAP2에는 24번 label이 없다. 24번은 뭘까
4. 0, 9, 10, 11을 묶어서 24로 놓은건가?
5. 근데 데이터를 로드해와서 np.unique(labels)를 찍어보니, 0 ~ 11로 정렬하게 나와있다. (아마 Encoding된 Label인듯.)
6. 뭐 어쨌든 데이터를 로드해왔다고 치고. (이해가 안감.) 데이터를 어떤식으로 불러와서 load하는지 보자.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/2월/25.2.6/PAMAP2_data_load.png" alt="PAMAP Data Load" width="500">
7. 데이터를 window크기 100씩으로 불러오는데, 시작 지점은 22씩만 늘어난다. (나머지 78개의 sequence는 계속 겹치면서 data를 로드하고 있다.)
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/2월/25.2.6/PAMAP2_step_size.png" alt="PAMAP Step Size" width="500">
8. 그 다음, 데이터를 만들고 싶은 크기로 x_train, y_train, x_test, y_test를 np.zeros로 만들어주고, 하나씩 넣어준다.
9. 왜하는가? > (idx, window_length, channel)의 형태를 (idx, batch_size, window_length, channel)의 형태로 바꿔주기 위함임.

### 내가 한 GAF데이터 형성 및 학습
1. pamap를 test_id를 기준으로 train, test를 얻어온 다음, 이거대로 바로 (64, 64, summation)GAF를 만듦.
2. 이거를 test_id : 0 ~ 8에 대해서 반복했는데, 나중에 test_set만 9개 만들어서 test_id = 0이면 0만 test_set으로 두고, 나머지 1~8은 train_set에 쌓는 방식으로 바꾸려고 함.
3. 데이터를 사용할 때 : np.load를 통해 이 데이터만 가져와서 빠른 데이터 load 후 로드 함수에서 정규화를 진행한 뒤, signal과 image, label을 반환하게 함.
4. 즉, GENE_Activ 데이터셋이랑 거의 똑같이 했는데, PAMAP는 test_id마다 만든것 뿐이다.



## 이번 주 학습 결과 (GENE_Activ, Ann, Ann + Sp)
### 학습 진행사항 (Annealing을 이용한 결과 - 14cls)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.7241 </li>
<li> 71.0540 </li>
<li> 70.8804 </li>
<li> 평균 : 70.8862 </li>
<li> 표준편차 : 0.1347 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 71.8875 </li>
<li> 71.6965 </li>
<li> 71.3145 </li>
<li> 평균 : 71.6328 </li>
<li> 표준편차 : 0.2382 </li>
<br>
</ul>


<li> T1 : WRN28-1(GAF), T2 : WRN28-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.4810 </li>
<li> 69.7343 </li>
<li> 70.4636 </li>
<li> 평균 : 70.2263 </li>
<li> 표준편차 : 0.3480 </li>
<br>
</ul>


<li> T1 : WRN28-3(GAF), T2 : WRN28-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.9845 </li>
<li> 70.0816 </li>
<li> 70.8630 </li>
<li> 평균 : 70.6430 </li>
<li> 표준편차 : 0.4001 </li>
<br>
</ul>



### 학습 진행사항 (Annealing + spKD를 이용한 결과 - 14cls)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 72.0264 </li>
<li> 71.3666 </li>
<li> 71.6617 </li>
<li> 평균 : 71.6849 </li>
<li> 표준편차 : 0.2699 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 71.0193 </li>
<li> 70.9325 </li>
<li> 71.0366 </li>
<li> 평균 : 70.9961 </li>
<li> 표준편차 : 0.0455 </li>
<br>
</ul>


<li> T1 : WRN28-1(GAF), T2 : WRN28-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 70.4810 </li>
<li> 70.6546 </li>
<li> 70.0469 </li>
<li> 평균 : 70.3942 </li>
<li> 표준편차 : 0.2556 </li>
<br>
</ul>


<li> T1 : WRN28-3(GAF), T2 : WRN28-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 71.5228 </li>
<li> 71.6270 </li>
<li> 70.0625 </li>
<li> 평균 : 71.0708 </li>
<li> 표준편차 : 0.7142 </li>
<br>
</ul>



### 학습 진행사항 (Annealing을 이용한 결과 - 7cls, 500w)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.6924 </li>
<li> 90.8556 </li>
<li> 90.2678 </li>
<li> 평균 : 90.6053 </li>
<li> 표준편차 : 0.2478 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.4637 </li>
<li> 91.0516 </li>
<li> 90.5291 </li>
<li> 평균 : 90.6815 </li>
<li> 표준편차 : 0.2631 </li>
<br>
</ul>


### 학습 진행사항 (Annealing을 이용한 결과 - 7cls, 1000w)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.0065 </li>
<li> 89.7779 </li>
<li> 90.0718 </li>
<li> 평균 : 89.9521 </li>
<li> 표준편차 : 0.1260 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.7903 </li>
<li> 90.8230 </li>
<li> 90.7250 </li>
<li> 평균 : 90.7794 </li>
<li> 표준편차 : 0.0407 </li>
<br>
</ul>


### 학습 진행사항 (Annealing + spKD를 이용한 결과 - 7cls, 500w)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.3984 </li>
<li> 90.6924 </li>
<li> 90.8230 </li>
<li> 평균 : 90.6379 </li>
<li> 표준편차 : 0.1776 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.1372 </li>
<li> 90.9536 </li>
<li> 90.2025 </li>
<li> 평균 : 90.4311 </li>
<li> 표준편차 : 0.3704 </li>
<br>
</ul>


### 학습 진행사항 (Annealing + spKD를 이용한 결과 - 7cls, 1000w)
<li> T1 : WRN16-1(GAF), T2 : WRN16-1(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.2351 </li>
<li> 89.9086 </li>
<li> 90.1045 </li>
<li> 평균 : 90.0827 </li>
<li> 표준편차 : 0.1342 </li>
<br>
</ul>

<li> T1 : WRN16-3(GAF), T2 : WRN16-3(Sig) S : WRN16-1(sig) 결과 </li>
<ul>
<li> 90.6924 </li>
<li> 90.9536 </li>
<li> 90.3658 </li>
<li> 평균 : 90.6706 </li>
<li> 표준편차 : 0.2405 </li>
<br>
</ul>


### 학습 진행사항 (Annealing을 이용한 결과 - 14cls)
| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) | 70.7241 | 71.0540 | 70.8804 | **70.8862** | 0.1347 |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) | 71.8875 | 71.6965 | 71.3145 | **71.6328** | 0.2382 |
| T1: WRN28-1(GAF), T2: WRN28-1(Sig), S: WRN16-1(Sig) | 70.4810 | 69.7343 | 70.4636 | **70.2263** | 0.3480 |
| T1: WRN28-3(GAF), T2: WRN28-3(Sig), S: WRN16-1(Sig) | 70.9845 | 70.0816 | 70.8630 | **70.6430** | 0.4001 |


### 학습 진행사항 (Annealing + spKD를 이용한 결과 - 14cls)

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) | 72.0264 | 71.3666 | 71.6617 | **71.6849** | 0.2699 |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) | 71.0193 | 70.9325 | 71.0366 | **70.9961** | 0.0455 |
| T1: WRN28-1(GAF), T2: WRN28-1(Sig), S: WRN16-1(Sig) | 70.4810 | 70.6546 | 70.0469 | **70.3942** | 0.2556 |
| T1: WRN28-3(GAF), T2: WRN28-3(Sig), S: WRN16-1(Sig) | 71.5228 | 71.6270 | 70.0625 | **71.0708** | 0.7142 |

---

### 학습 진행사항 (Annealing을 이용한 결과 - 7cls, 500w)

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) | 90.6924 | 90.8556 | 90.2678 | **90.6053** | 0.2478 |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) | 90.4637 | 91.0516 | 90.5291 | **90.6815** | 0.2631 |


### 학습 진행사항 (Annealing + spKD를 이용한 결과 - 7cls, 500w)

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) | 90.3984 | 90.6924 | 90.8230 | **90.6379** | 0.1776 |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) | 90.1372 | 90.9536 | 90.2025 | **90.4311** | 0.3704 |

---

### 학습 진행사항 (Annealing을 이용한 결과 - 7cls, 1000w)

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) | 90.0065 | 89.7779 | 90.0718 | **89.9521** | 0.1260 |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) | 90.7903 | 90.8230 | 90.7250 | **90.7794** | 0.0407 |

### 학습 진행사항 (Annealing + spKD를 이용한 결과 - 7cls, 1000w)

| Experiment | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| T1: WRN16-1(GAF), T2: WRN16-1(Sig), S: WRN16-1(Sig) | 90.2351 | 89.9086 | 90.1045 | **90.0827** | 0.1342 |
| T1: WRN16-3(GAF), T2: WRN16-3(Sig), S: WRN16-1(Sig) | 90.6924 | 90.9536 | 90.3658 | **90.6706** | 0.2405 |




## 이번 주 학습 결과 (PAMAP2, GAF 표현을 이용한 Teacher 네트워크 학습.)
### 학습 진행 사항 (Test_id : 0)
<li> WRN16-1 </li>
<ul>
<li> 77.8046 </li>
<li> 79.0660 </li>
<li> 80.9447 </li>
<li> 평균 : 79.2718 </li>
<li> 표준편차 : 1.2902 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 80.9447 </li>
<li> 81.1863 </li>
<li> 80.7032 </li>
<li> 평균 : 80.9447 </li>
<li> 표준편차 : 0.1972 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 83.1186 </li>
<li> 81.0521 </li>
<li> 81.2131 </li>
<li> 평균 : 81.7946 </li>
<li> 표준편차 : 0.9385 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 82.3671 </li>
<li> 82.0451 </li>
<li> 82.4745 </li>
<li> 평균 : 82.2956 </li>
<li> 표준편차 : 0.1825 </li>
</ul>

### 학습 진행 사항 (Test_id : 1)
<li> WRN16-1 </li>
<ul>
<li> 70.6752 </li>
<li> 73.3248 </li>
<li> 72.6115 </li>
<li> 평균 : 72.2038 </li>
<li> 표준편차 : 1.1194 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 72.3822 </li>
<li> 70.8790 </li>
<li> 72.4586 </li>
<li> 평균 : 71.9066 </li>
<li> 표준편차 : 0.7273 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 70.6242 </li>
<li> 69.9363 </li>
<li> 73.6051 </li>
<li> 평균 : 71.3885 </li>
<li> 표준편차 : 1.5923 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 75.2866 </li>
<li> 75.8471 </li>
<li> 78.2930 </li>
<li> 평균 : 76.4756 </li>
<li> 표준편차 : 1.3053 </li>
</ul>

### 학습 진행 사항 (Test_id : 2)
<li> WRN16-1 </li>
<ul>
<li> 77.2377 </li>
<li> 78.8580 </li>
<li> 79.6682 </li>
<li> 평균 : 78.5880 </li>
<li> 표준편차 : 1.0105 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 80.1698 </li>
<li> 77.1991 </li>
<li> 78.8194 </li>
<li> 평균 : 78.7294 </li>
<li> 표준편차 : 1.2145 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 78.0478 </li>
<li> 79.4367 </li>
<li> 76.8133 </li>
<li> 평균 : 78.0993 </li>
<li> 표준편차 : 1.0716 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 79.7454 </li>
<li> 79.2824 </li>
<li> 80.9414 </li>
<li> 평균 : 79.9897 </li>
<li> 표준편차 : 0.6990 </li>
</ul>

### 학습 진행 사항 (Test_id : 3)
<li> WRN16-1 </li>
<ul>
<li> 77.4877 </li>
<li> 77.4587 </li>
<li> 79.4604 </li>
<li> 평균 : 78.1356 </li>
<li> 표준편차 : 0.9368 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 81.6942 </li>
<li> 80.2727 </li>
<li> 78.7932 </li>
<li> 평균 : 80.2534 </li>
<li> 표준편차 : 1.1844 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 78.1259 </li>
<li> 77.6617 </li>
<li> 78.3000 </li>
<li> 평균 : 78.0292 </li>
<li> 표준편차 : 0.2694 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 81.6652 </li>
<li> 79.8666 </li>
<li> 80.2147 </li>
<li> 평균 : 80.5822 </li>
<li> 표준편차 : 0.7789 </li>
</ul>

### 학습 진행 사항 (Test_id : 4)
<li> WRN16-1 </li>
<ul>
<li> 79.3850 </li>
<li> 80.0492 </li>
<li> 80.4674 </li>
<li> 평균 : 79.9672 </li>
<li> 표준편차 : 0.4457 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 81.5744 </li>
<li> 80.5166 </li>
<li> 80.6642 </li>
<li> 평균 : 80.9184 </li>
<li> 표준편차 : 0.4678 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 80.4674 </li>
<li> 78.5732 </li>
<li> 81.0332 </li>
<li> 평균 : 80.0246 </li>
<li> 표준편차 : 1.0520 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 80.7380 </li>
<li> 80.5412 </li>
<li> 81.1316 </li>
<li> 평균 : 80.8036 </li>
<li> 표준편차 : 0.2455 </li>
</ul>

### 학습 진행 사항 (Test_id : 5)
<li> WRN16-1 </li>
<ul>
<li> 81.0086 </li>
<li> 78.8627 </li>
<li> 81.0622 </li>
<li> 평균 : 80.3112 </li>
<li> 표준편차 : 1.0245 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 81.0086 </li>
<li> 79.6064 </li>
<li> 81.2232 </li>
<li> 평균 : 80.6127 </li>
<li> 표준편차 : 0.7170 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 78.3262 </li>
<li> 80.4989 </li>
<li> 80.4721 </li>
<li> 평균 : 79.7657 </li>
<li> 표준편차 : 1.0180 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 79.8820 </li>
<li> 79.8552 </li>
<li> 80.0161 </li>
<li> 평균 : 79.9178 </li>
<li> 표준편차 : 0.0704 </li>
</ul>

### 학습 진행 사항 (Test_id : 6)
<li> WRN16-1 </li>
<ul>
<li> 84.9928 </li>
<li> 82.7706 </li>
<li> 84.2713 </li>
<li> 평균 : 84.0116 </li>
<li> 표준편차 : 0.9256 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 86.3492 </li>
<li> 86.9841 </li>
<li> 84.5310 </li>
<li> 평균 : 85.9548 </li>
<li> 표준편차 : 1.0396 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 86.3492 </li>
<li> 85.2237 </li>
<li> 85.2237 </li>
<li> 평균 : 85.5989 </li>
<li> 표준편차 : 0.5306 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 84.0115 </li>
<li> 85.5988 </li>
<li> 83.8672 </li>
<li> 평균 : 84.4925 </li>
<li> 표준편차 : 0.7845 </li>
</ul>

### 학습 진행 사항 (Test_id : 7)
<li> WRN16-1 </li>
<ul>
<li> 80.6856 xxx </li>
<li> 80.6089 </li>
<li> 80.0972 </li>
<li> 평균 :  </li>
<li> 표준편차 :  </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 80.0460 </li>
<li> 80.7623 </li>
<li> 78.6646 </li>
<li> 평균 : 79.8243 </li>
<li> 표준편차 : 0.8706 </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 80.2763 </li>
<li> 80.2251 </li>
<li> 79.8414 </li>
<li> 평균 : 80.1143 </li>
<li> 표준편차 : 0.1941 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 80.1740 </li>
<li> 79.6623 </li>
<li> 81.1205 </li>
<li> 평균 : 80.3189 </li>
<li> 표준편차 : 0.6041 </li>
</ul>

### 학습 진행 사항 (Test_id : 8)
<li> WRN16-1 </li>
<ul>
<li> 92.3913 </li>
<li> 83.6957 </li>
<li> 88.0435 </li>
<li> 평균 : 88.0435 </li>
<li> 표준편차 : 3.5500 </li>
</ul>

<li> WRN16-3 </li>
<ul>
<li> 94.5652 </li>
<li> 85.8696 </li>
<li> 90.2174 xxx </li>
<li> 평균 :  </li>
<li> 표준편차 :  </li>
</ul>

<li> WRN28-1 </li>
<ul>
<li> 94.5652 </li>
<li> 84.7826 </li>
<li> 89.1304 </li>
<li> 평균 : 89.4927 </li>
<li> 표준편차 : 4.0019 </li>
</ul>

<li> WRN28-3 </li>
<ul>
<li> 93.4783 </li>
<li> 83.6957 </li>
<li> 95.6522 </li>
<li> 평균 : 90.9421 </li>
<li> 표준편차 : 5.2002 </li>
</ul>



## 학습 진행 사항 (Test_id : 0)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 77.8046 | 79.0660 | 80.9447 | **79.2718** | 1.2902 |
| WRN16-3  | 80.9447 | 81.1863 | 80.7032 | **80.9447** | 0.1972 |
| WRN28-1  | 83.1186 | 81.0521 | 81.2131 | **81.7946** | 0.9385 |
| WRN28-3  | 82.3671 | 82.0451 | 82.4745 | **82.2956** | 0.1825 |

---

## 학습 진행 사항 (Test_id : 1)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 70.6752 | 73.3248 | 72.6115 | **72.2038** | 1.1194 |
| WRN16-3  | 72.3822 | 70.8790 | 72.4586 | **71.9066** | 0.7273 |
| WRN28-1  | 70.6242 | 69.9363 | 73.6051 | **71.3885** | 1.5923 |
| WRN28-3  | 75.2866 | 75.8471 | 78.2930 | **76.4756** | 1.3053 |

---

## 학습 진행 사항 (Test_id : 2)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 77.2377 | 78.8580 | 79.6682 | **78.5880** | 1.0105 |
| WRN16-3  | 80.1698 | 77.1991 | 78.8194 | **78.7294** | 1.2145 |
| WRN28-1  | 78.0478 | 79.4367 | 76.8133 | **78.0993** | 1.0716 |
| WRN28-3  | 79.7454 | 79.2824 | 80.9414 | **79.9897** | 0.6990 |


## 학습 진행 사항 (Test_id : 3)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 77.4877 | 77.4587 | 79.4604 | **78.1356** | 0.9368 |
| WRN16-3  | 81.6942 | 80.2727 | 78.7932 | **80.2534** | 1.1844 |
| WRN28-1  | 78.1259 | 77.6617 | 78.3000 | **78.0292** | 0.2694 |
| WRN28-3  | 81.6652 | 79.8666 | 80.2147 | **80.5822** | 0.7789 |

---

## 학습 진행 사항 (Test_id : 4)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 79.3850 | 80.0492 | 80.4674 | **79.9672** | 0.4457 |
| WRN16-3  | 81.5744 | 80.5166 | 80.6642 | **80.9184** | 0.4678 |
| WRN28-1  | 80.4674 | 78.5732 | 81.0332 | **80.0246** | 1.0520 |
| WRN28-3  | 80.7380 | 80.5412 | 81.1316 | **80.8036** | 0.2455 |

---

## 학습 진행 사항 (Test_id : 5)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 81.0086 | 78.8627 | 81.0622 | **80.3112** | 1.0245 |
| WRN16-3  | 81.0086 | 79.6064 | 81.2232 | **80.6127** | 0.7170 |
| WRN28-1  | 78.3262 | 80.4989 | 80.4721 | **79.7657** | 1.0180 |
| WRN28-3  | 79.8820 | 79.8552 | 80.0161 | **79.9178** | 0.0704 |

---

## 학습 진행 사항 (Test_id : 6)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 84.9928 | 82.7706 | 84.2713 | **84.0116** | 0.9256 |
| WRN16-3  | 86.3492 | 86.9841 | 84.5310 | **85.9548** | 1.0396 |
| WRN28-1  | 86.3492 | 85.2237 | 85.2237 | **85.5989** | 0.5306 |
| WRN28-3  | 84.0115 | 85.5988 | 83.8672 | **84.4925** | 0.7845 |

---

## 학습 진행 사항 (Test_id : 7)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 80.6856 | 80.6089 | 80.0972 | **80.4639** | 0.2612 |
| WRN16-3  | 80.0460 | 80.7623 | 78.6646 | **79.8243** | 0.8706 |
| WRN28-1  | 80.2763 | 80.2251 | 79.8414 | **80.1143** | 0.1941 |
| WRN28-3  | 80.1740 | 79.6623 | 81.1205 | **80.3189** | 0.6041 |

---

## 학습 진행 사항 (Test_id : 8)

| Experiment  | Trial 1 | Trial 2 | Trial 3 | Mean | Std |
|------------|---------|---------|---------|---------|---------|
| WRN16-1  | 92.3913 | 83.6957 | 88.0435 | **88.0435** | 3.5500 |
| WRN16-3  | 94.5652 | 85.8696 | 90.2174 | **90.2174** | 3.5500 |
| WRN28-1  | 94.5652 | 84.7826 | 89.1304 | **89.4927** | 4.0019 |
| WRN28-3  | 93.4783 | 83.6957 | 95.6522 | **90.9421** | 5.2002 |



## 모든 Test_id를 평균 낸 결과
| Experiment  | Mean | Std |
|------------|---------|---------|
| WRN16-1  | **80.1107** | 4.3123 |
| WRN16-3  | **81.0402** | 4.9223 |
| WRN28-1  | **80.4786** | 5.0272 |
| WRN28-3  | **81.7575** | 4.2402 |

- 결과에 대한 나의 생각
  - 지금 이전 reference 논문에 의해서 lambda와 t값을 0.99 / 4로 사용하고 있다.
  - 근데, 그것은 그 논문에서 signal로 학습한 결과가 image로 학습한 결과보다 낮았기 때문이라고 생각한다.
  - (86.93, 87.23, 87.45, 87.88)이었음.
  - 그래서 lambda값을 0.1, 0.3, 0.5, 0.7, 0.9로 바꿔가며 최적의 값을 찾아버려 함.


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log), PAMAP(이것도 HAR 데이터) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>