## 이번주 한 것.
<li> PAMAP2 3 Teacher 에서 최적의 alpha값 조합 선정 및 ann, annsp 결과 확인중 </li>
<li> GENE Activ 데이터셋 EBKD 적용해서 결과 확인함. </li>
<li> DCD 논문 읽어보고 baseline으로 적용할만 한지 판단 </li>

## 실험 진행 상황
### PAMAP2와 GENEActiv 실험결과
- PAMAP2는 3Teacher에 대해서 4:3:3을 최적의 alpha조합으로 선정하여 base에서의 wrn161, wrn283 을 추가로 실험, ann도 실험, annsp까지 쭉 이어서 실험할 예정
- 현재 ann은 거의 끝난 상태이고, base의 wrn16-1, wrn28-3 진행중
- 이번주 내로 annsp와 PAMAP2에서의 ebkd까지 완료하는것이 목표.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.24/PAMAP.png" alt="PAMAP" width="700">


- GENEActiv에 EBKD를 적용한 결과를 포함한 전체 결과
- EBKD는 Teacher들의 logits 분포를 확인하고, 그것을 통해 Entropy(출력 분포의 불확실성)을 측정한다.
- Entropy가 낮을수록 Teahcer의 출력이 확신이 있다는 의미이고, Entropy가 클 수록 출력이 불확실하다는 것이므로 가중치를 낮게 설정하는 식임.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.24/GENEActiv.png" alt="GENEActiv" width="700">

## 6.25 할 일
- DCD 1Teacher (Signal to Signal) 코드 작성해놓고 돌아가는거 확인 (PAMAP, GENE 둘 다)
- 논문하나 읽고 적용할 수 있을지 판단 (Understanding the Role of the Projector in Knowledge Distillation)
- github에 코드를 잘 뒤져봐서 코드 적용할 수 있는지 확인.
- 

## Understanding the Role of the Projector in Knowledge Distillation 논문 리뷰
### Abstract
- 기존의 기본적은 Knowledge Distillation의 설계에서 단순히 projector, normalization, soft metric을 잘 사용하면, 복잡한 KD 기법들보다 더 좋은 성능을 낼 수 있다는 것을 설명한다.
- 즉, 새로운 loss를 제안하는 논문이 아닌듯 함.

### Introduction
- 기존 지식 증류 방식에서는 여전히 computational and memory overheads의 한계가 있고, insufficient theoretical explanation for the underlying core principles 문제가 있다.
- 그래서 이 논문의 저자는 function matching과 metric learning 을 이용한다.
- distance metric(function), normalisation, projector network가 그것들임.
- 이 ablation을 통해 theorical perspective와 unification of these design principles through exploring the underlying training dynamics를 제공할것임.
- 

### Proposed Method
- 일반적인 지식 증류는 teacher의 soft logits과 student의 soft logits을 비교하여 학습이 진행되는데, 이건 classifier에서만 가능함.
- feature based distillation은 두가지 문제가 있다.
    - teacher와 student의 architecture가 다른 경우 (중간 feature map도 다르게 생길거다)
    - inductive bias문제, 예를들어 CNN은 지역적인 정보에 민감하고, Transformer는 전역적인 정보에 민감한데, 이런 식으로 inductive bias가 다르다면(Student가 자신이 잘하는 것을 버리고, Teacher에 맞춰버리게 되면) 성능이 오히려 저하될 수 있음.
- Representation Distillation은 Teacher의 Feature 중 출력 직전 Layer의 Feature를 Student에게 전달하는 방식이다. 
    - 이 논문에서는 Representation Distillation에 집중한다고 한다.
    - 서로 다른 아키텍처라도, 출력 직전의 feature shape은 linear를 통과시켜 shape이 일치할 수 있게 하는 경우가 많다
    - task(classification이나 regression)에 필요한 고차원의 feature 정보를 그대로 전달할 수 있다.
    - inductive bias또한 너무 강제하지 않는다.
- 즉 이 논문에서 주장하는 것.
    - 같은 input x를 Student와 Teacher에 넣는다.
    - 각 모델을 통과하여 output y와 출력 직전의 feature z를 얻음.
    - 출력을 통해 Ground Truth와 비교하여 loss를 얻어서 CE_loss 얻음.
    - student의 z_s를 Teacher의 z_t와 shape을 맞춰주기 위해서 Linear Projection에 통과시킨다.
        - 이때 z_s와 z_t의 shape이 이미 같더라도 Linear Projection을 무조건 해야한다고 논문에서 강조함.
        - 이 논문에서 Projection은 차원을 맞추는 것 이상의 역할을 한다고 나와있다.
        - 이 Projection을 통하여 Relational Gradients를 전달하여, teacher-student간 sample 간의 관계 구조를 학습할 수 있게 된다.
    - 그렇게 얻은 P(z_s)와 z_t를 Batch normalization 을 적용한 뒤 LogSum Distance를 계산하여 이것을 KD_loss로 사용한다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/RoleofProject.png" alt="Fig1" width="700">

- 
---

## 해야 할 남은것들
<li> KD baseline 찾기 (Discord에 교수님이 올려주신거 논문 다 읽어보기.) </li>
<li> Discriminative and Consistent Representation Distillation (DCD) </li>
<li>https://ieeexplore.ieee.org/abstract/document/10965820</li>
<li>https://ojs.aaai.org/index.php/AAAI/article/view/32266</li>
<li>https://ojs.aaai.org/index.php/AAAI/article/view/34031</li>

<li> 교수님이 보라고 하신 논문 읽어보기 (후순위) </li>
<li> https://arxiv.org/abs/2305.15775 - Concept-Centric Transformer (아직) </li>
<li> https://arxiv.org/pdf/2502.11418 - TimeCAP (아직) </li>
<li> 영어공부(심심할 때 틈틈히) </li>

---

### Teacher 특이조합에 대해서 실험 진행 (GENE_Activ, PAMAP 둘다 적용) ! 코드작업 선행 필요. (아마 오래 걸릴 것.)
- 특이조합에 대해서는 Wide Resnet 기준 2Teacher(GAF+Sig, PI+Sig) 특이조합과 3Teacher(GAF+PI+Sig) 특이조합에 대해서 실험해야함. (Student는 wrn161 고정.)
- 각 조합에 대해서 Base와 annsp로 실험. (alpha나 lambda는 이미 찾은 값으로.)
- 2Teacher (Image + Signal)
    * Depth-wise - (wrn281 + wrn161), (wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161), (wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163), (wrn163 + wrn281)
- 3Teacher (GAF + PI + Sig)
    * Depth-wise - (wrn281 + wrn161 + wrn161), (wrn161 + wrn281 + wrn161), (wrn161 + wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161 + wrn161), (wrn161 + wrn163 + wrn161), (wrn161 + wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163 + wrn161), (wrn163 + wrn281 + wrn161)

### Teacher - Student 특이조합(모델 구조 완전변형)
- 완전특이조합 Mobile net이나, Resnet, VGG를 이용한 특이조합에 대한 추가 실험
- 여기는 Teacher끼리는 같은 모델을 사용한다.
- 아레의 조합에 대해서 찾아야 할 값 
    - Student의 성능.
    - Sig → Sig의 성능 (1Teacher)
    - Pi+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Pi+Sig → Sig의 성능 (3Teacher, base, ann, annsp)
```
    - T : wrn-163 → S : RN8
    - T : wrn-281 → S : RN20, vgg8
    - T : RN44 → S : vgg8, wrn-161
    - T : MN.V2 → S : RN8
```