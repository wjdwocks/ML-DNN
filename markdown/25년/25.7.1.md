## 이번주 한 것.
<li> PAMAP2 3 Teacher 에서 최적의 alpha값 조합 선정 및 ann, annsp 결과 확인중 </li>
<li> GENE Activ 데이터셋 EBKD 적용해서 결과 확인함. </li>
<li> DCD 논문 읽어보고 baseline으로 적용할만 한지 판단 </li>

## 실험 진행 상황
### PAMAP2와 GENEActiv 실험결과
- PAMAP2는 3Teacher에 대해서 4:3:3을 최적의 alpha조합으로 선정하여 base에서의 wrn161, wrn283 을 추가로 실험, ann도 실험, annsp까지 쭉 이어서 실험할 예정
- 현재 ann은 거의 끝난 상태이고, base의 wrn16-1, wrn28-3 진행중
- 이번주 내로 annsp와 PAMAP2에서의 ebkd까지 완료하는것이 목표.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.24/PAMAP.png" alt="PAMAP" width="700">


- GENEActiv에 EBKD를 적용한 결과를 포함한 전체 결과
- EBKD는 Teacher들의 logits 분포를 확인하고, 그것을 통해 Entropy(출력 분포의 불확실성)을 측정한다.
- Entropy가 낮을수록 Teahcer의 출력이 확신이 있다는 의미이고, Entropy가 클 수록 출력이 불확실하다는 것이므로 가중치를 낮게 설정하는 식임.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.24/GENEActiv.png" alt="GENEActiv" width="700">


## Understanding the Role of the Projector in Knowledge Distillation 논문 리뷰
### Abstract
- 기존의 기본적은 Knowledge Distillation의 설계에서 단순히 projector, normalization, soft metric을 잘 사용하면, 복잡한 KD 기법들보다 더 좋은 성능을 낼 수 있다는 것을 설명한다.
- 즉, 새로운 loss를 제안하는 논문이 아닌듯 함.

### Introduction
- 기존 지식 증류 방식에서는 여전히 computational and memory overheads의 한계가 있고, insufficient theoretical explanation for the underlying core principles 문제가 있다.
- 그래서 이 논문의 저자는 function matching과 metric learning 을 이용한다.
- distance metric(function), normalisation, projector network가 그것들임.
- 이 ablation을 통해 theorical perspective와 unification of these design principles through exploring the underlying training dynamics를 제공할것임.

### Proposed Method
- 일반적인 지식 증류는 teacher의 soft logits과 student의 soft logits을 비교하여 학습이 진행되는데, 이건 classifier에서만 가능함.
- feature based distillation은 두가지 문제가 있다.
    - teacher와 student의 architecture가 다른 경우 (중간 feature map도 다르게 생길거다)
    - inductive bias문제, 예를들어 CNN은 지역적인 정보에 민감하고, Transformer는 전역적인 정보에 민감한데, 이런 식으로 inductive bias가 다르다면(Student가 자신이 잘하는 것을 버리고, Teacher에 맞춰버리게 되면) 성능이 오히려 저하될 수 있음.
- Representation Distillation은 Teacher의 Feature 중 출력 직전 Layer의 Feature를 Student에게 전달하는 방식이다. 
    - 이 논문에서는 Representation Distillation에 집중한다고 한다.
    - 서로 다른 아키텍처라도, 출력 직전의 feature shape은 linear를 통과시켜 shape이 일치할 수 있게 하는 경우가 많다
    - task(classification이나 regression)에 필요한 고차원의 feature 정보를 그대로 전달할 수 있다.
    - inductive bias또한 너무 강제하지 않는다.
- 즉 이 논문에서 주장하는 것.
    - 같은 input x를 Student와 Teacher에 넣는다.
    - 각 모델을 통과하여 output y와 출력 직전의 feature z를 얻음.
    - 출력을 통해 Ground Truth와 비교하여 loss를 얻어서 CE_loss 얻음.
    - student의 z_s를 Teacher의 z_t와 shape을 맞춰주기 위해서 Linear Projection에 통과시킨다.
        - 이때 z_s와 z_t의 shape이 이미 같더라도 Linear Projection을 무조건 해야한다고 논문에서 강조함.
        - 이 논문에서 Projection은 차원을 맞추는 것 이상의 역할을 한다고 나와있다.
        - 이 Projection을 통하여 Relational Gradients를 전달하여, teacher-student간 sample 간의 관계 구조를 학습할 수 있게 된다.
    - 그렇게 얻은 P(z_s)와 z_t를 Batch normalization 을 적용한 뒤 LogSum Distance를 계산하여 이것을 KD_loss로 사용한다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/RoleofProject.png" alt="Fig1" width="700">

- batch norm을 굳이 해야하나?
    - 이 사람이 normalization을 하지 않은 것, L2 Normalization, Group Normalization, Batch Normalization을 한 것을 비교했음.
    - 그 때 Batch normalization을 한 후에, LogSum Distance로 loss를 설정한 것의 성능이 훨씬 좋았다고 함.
    - 그 이유로는, Projection을 할 때 input이 특정 축으로 쏠리는(편향되는, Singular Value Collapse) 현상이 발생할 수 있다고 함. 
    - 그것을 Batchnorm을 통해서 완화시켜주기 위해서 Normalization을 하는 거라고 한다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/Normalization.png" alt="Fig2" width="700">

- Projection Layer의 뉴런 개수에 따른 input-output 간의 decorrelation 비교
    - 이게 Projection Layer를 너무 복잡하게 만들면, input의 정보가 output에서 너무 옅어지는 현상이 발생할 수 있다고 함.
    - 그래서 이 저자는 MLP의 뉴런을 Linear, 64, 512, 2048개에 대해서 비교했을 때 Linear에서 correlation이 가장 높았다.
    - 그래서 이 저자는 Linear를 사용했다고 함. (Hidden Neruon 0개)
    - 근데, 결국 Projection를 해서 정보 손실이 일어난다는건데, 그럼 굳이 안해도 되는거 아닌가?
        - 논문의 저자가 수식으로 알려줬듯이 이 Projection을 통해서 Student-Teacher간의 relational information을 잘 전달해줘서 student가 teacher의 정보를 잘 흡수할 수 있다고 했어서 Projection을 하기는 해야함.
        - 근데 내 논문처럼 Teacher와 Student가 둘다 Wide Resnet구조라면 또 굳이 해야하나 싶기도 하고
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/Decorrelation.png" alt="Fig3" width="700">

- LogSum distance의 역할과 필요성
    - Teacher와 Student 간의 Capacity Gap이 클 때 Feature Space Alignment가 어렵다(Student가 완벽하게 따라하려고 하면 오히려 성능이 나빠지는 경우가 있음.)
    - 기존의 Feature Based Distillation에서 사용하는 L2 Loss는 모든 Feature의 요소별로 강하게 패널티를 부여했었다. 
    - LogSum Distance는 상대적으로 거리가 작은 feature 차이는 덜 중요하게, 큰 차이에는 더 중요하게 초점을 맞춰주는 soft maximum 함수를 사용함.
    - 즉, Soft maximum은 전체 feature의 차이를 균일하게 반영하는 것이 아니라, feature의 차이가 큰 지점에 더 집중하여 차이값을 계산한다는 것임.
    - 이 방식을 사용했을 때 CNN → CNN 뿐만 아니라 ViT → CNN 에서도 일관된 성능 향상을 확인했다고 함.

---

## Understanding the Role of the Projector in Knowledge Distillation 코드 적용 반드시.
<li> 아레처럼 구현이 되어있음. (약간 수정한거임) </li>
<li> 마지막 Feature를 가져와서 Linear로 Student-Teacher간 Shape을 맞춰준 뒤, Batch Norm을 하고 LogSum을 하는 것 까지는 논문에서 이해한 대로이다. </li>
<li> 근데, 기존의 KD_Loss를 LogSum으로 완전 대체한 것처럼 논문에 쓰여있는데, LogSum을 통한 Feature Distillation Loss + Logits Distillation Loss 의 형태로 코드가 작성되어있어서 이렇게 그대로 사용함. </li>
<li> 또한 논문에서 각 Loss의 가중치가 1 : 1 : 1인것 처럼 표현했지만, 코드를 보니 CE_Loss : LogSum Loss : KD_Loss 가 1 : 2 : 1 인것 같다. </li>

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/FinalLoss.png" alt="FinalLoss" width="700">

```python
class PRLoss(nn.Module):
    def __init__(self, s_dim=64, mode='mse', t_dim=None):
        super().__init__()
        self.mode = mode

        t_dim = t_dim
        s_dim = 64

        # projector
        self.embed = nn.Linear(s_dim, t_dim).cuda()
        self.bn_s = torch.nn.BatchNorm1d(t_dim, eps=0.0001, affine=False).cuda()
        self.bn_t = torch.nn.BatchNorm1d(t_dim, eps=0.0001, affine=False).cuda()

        if mode not in ('mse', 'bn_mse', 'bn_corr', 'bn_corr_4', 'log_bn_corr_4'):
            raise ValueError('mode `{}` is not expected'.format(mode))

    def forward_loss(self, z_s, z_t):
        f_t = z_t

        f_s = self.embed(z_s)
        n, d = f_s.shape

        f_s_norm = self.bn_s(f_s)
        f_t_norm = self.bn_t(f_t)

        c_diff = f_s_norm - f_t_norm
        c_diff = torch.abs(c_diff)
        c_diff = c_diff.pow(4.0)

        loss = torch.log(c_diff.sum())
        return loss

    def forward(self, z_s, y_s, z_t, y_t):
        # Feature-based KD Loss (Representation Loss)
        l_rep = 2.0 * self.forward_loss(z_s, z_t)

        # Logit-based KD Loss
        T = 1.0
        p_s = F.log_softmax(y_s / T, dim=1)
        p_t = F.softmax(y_t / T, dim=1)
        l_logit = F.kl_div(p_s, p_t, reduction='batchmean') * (T ** 2)

        total_loss = l_rep + l_logit

        return total_loss
```

---

## 다른 Baseline 논문 읽어보고 이해 및 적용 가능성 탐방 (MTKD_RL) : Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition
### 요약
<li> EBKD나 CAMKD처럼 각 Teacher의 KD_Loss 비중을 간단한 Policy Gradient로 학습하는 강화학습으로 정하게 한다. </li>
<li> 이때 Agent가 고려하는것은 각 Teacher의 CE_Loss, 각 Teacher와 Student간의 logits의 KL_Divergence, 각 Teacher와 Student간의 마지막 Feature 간의 KL_Divergence, Teacher의 마지막 Layer의 Feature. </li>
<li> 그래서 결론적으로, 각 Teacher1, Teacher2, Teacher3의 마지막 Feature를 MSE한 Feature_Based KD loss의 가중합 + logits_based_kd loss의 가중합 + Student의 CE Loss 로 최종 Loss가 됨. </li>

### 코드 작업중...


---

## CCT 및 Latent Workspace with Multimodal Knowledge Distillation 이 논문 읽어보고 공부.
<li> 아직.. </li>

---


### Ablation Study 결과 계속 보는중.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/2teacher_ablation.png" alt="FinalLoss" width="700">

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.7.1/3teacher_ablation.png" alt="FinalLoss" width="700">

---


## 해야 할 남은것들
<li> KD baseline 찾기 (Discord에 교수님이 올려주신거 논문 다 읽어보기.) </li>
<li> Discriminative and Consistent Representation Distillation (DCD) </li>
<li>https://ieeexplore.ieee.org/abstract/document/10965820 (LSTM얘기가 있는걸로 봐서 어려울듯.) </li>
<li>https://ojs.aaai.org/index.php/AAAI/article/view/32266 (단일 Teacher 기반 논문) </li>
<li>https://ojs.aaai.org/index.php/AAAI/article/view/34031 (단일 Teacher에서 Logits + Feature Based KD) </li>

<li> 교수님이 보라고 하신 논문 읽어보기 (CCT 및 그거관련 아레 얘들 다 관련 있다고 함.) </li>
<li> https://arxiv.org/abs/2305.15775 - Concept-Centric Transformer (아직) </li>
<li> https://arxiv.org/pdf/2502.11418 - TimeCAP (아직) </li>
<li> Improved Knowledge Distillation Based on Global Latent Workspace with MultiModal Knowledge Fusion for Understanding Topological Guidance on Wearable Sensor Data </li>
<li> CTPD (Cross-Modal Temporal Pattern Discovery) </li>
<li> 영어공부(심심할 때 틈틈히) </li>

---

### Teacher 특이조합에 대해서 실험 진행 (GENE_Activ, PAMAP 둘다 적용) ! 코드작업 선행 필요. (아마 오래 걸릴 것.)
- 특이조합에 대해서는 Wide Resnet 기준 2Teacher(GAF+Sig, PI+Sig) 특이조합과 3Teacher(GAF+PI+Sig) 특이조합에 대해서 실험해야함. (Student는 wrn161 고정.)
- 각 조합에 대해서 Base와 annsp로 실험. (alpha나 lambda는 이미 찾은 값으로.)
- 2Teacher (Image + Signal)
    * Depth-wise - (wrn281 + wrn161), (wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161), (wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163), (wrn163 + wrn281)
- 3Teacher (GAF + PI + Sig)
    * Depth-wise - (wrn281 + wrn161 + wrn161), (wrn161 + wrn281 + wrn161), (wrn161 + wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161 + wrn161), (wrn161 + wrn163 + wrn161), (wrn161 + wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163 + wrn161), (wrn163 + wrn281 + wrn161)

### Teacher - Student 특이조합(모델 구조 완전변형)
- 완전특이조합 Mobile net이나, Resnet, VGG를 이용한 특이조합에 대한 추가 실험
- 여기는 Teacher끼리는 같은 모델을 사용한다.
- 아레의 조합에 대해서 찾아야 할 값 
    - Student의 성능.
    - Sig → Sig의 성능 (1Teacher)
    - Pi+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Pi+Sig → Sig의 성능 (3Teacher, base, ann, annsp)
```
    - T : wrn-163 → S : RN8
    - T : wrn-281 → S : RN20, vgg8
    - T : RN44 → S : vgg8, wrn-161
    - T : MN.V2 → S : RN8
```



## 7.2 오늘 하려는거
<li> Role of Projection 저 코드 GENE_14cls에 적용 후 결과확인 </li>
<li> GENE_14cls에 대해 Ablation 결과 표로 정리(되는 대로) </li>
<li> GENE 14cls RN44, MN.V2 Teacher, WRN40-1 만들어놓기 </li>
<li> 이제 멀티 모달 Multi Teacher에 대해 적용해볼 수 있는 KD Baseline 찾아보기...(논문한편읽어보기) </li>