## 이번주 한 것.



## 돌려야 할 것들
<li> PAMAP2 baseline으로 사용할 Role of Projection, MTKD_RL (gpu 남으면 돌리면 될듯) </li>
<li> PAMAP2 내꺼 3 Teacher annsp, ann 좀 더 돌려서 더 좋은 결과로 </li>
<li> GENE MobileNet GAF Teacher 학습 해놓고, 완전다른 Ablation 학습 코드 짜놓기 </li>

## 돌아가고 있는 것들
    - 0 : GENE 2Teacher Ablation (annsp), 
    - 1 : PAMAP2 3Teacher Ablation (base)
    - 2 : PAMAP2 3Teacher Ablation (base)
    - 3 : PAMAP2 3Teacher Ablation (ann)


## CCT 및 Latent Workspace with Multimodal Knowledge Distillation 이 논문 읽어보고 공부.
## Concept-Centric Transformers (CCT)
### Interpretable AI 
- 설명 가능한 인공지능 : AI가 내리는 결정 과정을 사람이 이해할 수 있도록 설명할 수 있는 AI를 의미함.
    - 기존 딥러닝 모델은 Black-Box 라고 불림(왜 그런 결론을 내렸는지 설명할 수 없다.)
    - 기존 딥러닝 모델은 Pixel, Feature Map, Vector 수준에서 작동하기 때문에 사람이 직관적으로 이해하기 어려움.
    - Interpretable AI의 주요 접근법
        - Post-Hoc Explanation (사후적 설명)
            - 이미 학습된 블랙박스 모델을 해석하는 방석
            - LIME : 입력 데이터를 랜덤하게 조금씩 변경하면서 어떤 Feature가 결과에 얼마나 영향을 주는지 설명
            - SHAP : 게임이론 기반으로, 각 Feature가 결과에 미친 기여도를 계산함.
            - Grad-CAM : CNN기반 모델에서 어느 영역을 보고 판단했는지 시각화하는거 (논문에서 본 적 있는것 같다.)
        - Intrinsic Interpretability (내재적 설명 가능성)
            - 모델을 설계할 때부터 해석 가능하도록 구조를 설계하는것.
            - Decision Tree : 각 Feature에 대해 결정 경로가 명확하게 보인다.
            - Linear Model : 각 Feature의 가중치를 그대로 해석이 가능함.
            - Concept-based Model : 사람이 이해할 수 있는 개념 단위로 의사결정을 수행한다.
- Concept : 사람이 직관적으로 이해할 수 있는 의미 단위를 의미함.
- Concept based Model : 모델이 사람이 이해할 수 있는 개념 단위로 정보를 처리하거나, 설명을 유도하는 것을 의미함.
    - 기존 모델에서는 [1. 이미지 입력 → 2. Conv / Transformer를 통한 입력의 벡터연산 → 3. 결과 예측] 이런 순서대로 학습을 진행함.
    - Concept based Method는 [1. 이미지 입력 → 2. concept 단위 정보 처리 (부리:있음, 날개:큼, 다리:길다) → 3. 결과:독수리] 이런식으로 예측의 이유가 설명이 됨.

### Introduction
- Interpretable Model은 각 부분이 명확한 의미를 가진 모듈들로 구성된 설명을 생성해야 한다. 
    - Concept-Centroc Transformer라는 구조를 제안한다. 
    - CCT는 Shard Global Workspace(SGW)이론에서 영감을 받았으며, 여러 개의 특화된 모듈들이 공유 메모리에 경쟁적으로 접근하도록 설계되어 모듈화를 자연스럽게 유도함.
    - 이 CCT 라는 구조를 통해 모델은 단순하고, 모듈화된 구조로 사람이 이해 가능한 개념을 추출할 수 있게 된다.
- CCT는 세 가지 주요 구성 요소로 설계된다.
    - 1. Concept-Slot-Attention(CSA) 모듈
        - Backbone에서 나온 이미지 embedding을 입력으로 받음. (여기서 백본은 ViT, CNN 같은 모델을 쓴다고 했으니, Patch Embedding → Transformer Encoder → Latent Feature 이므로, 이 정제된 Latent Vector를 입력해준다는 거임.)
        - Input Feature와 Slot 간의 Slot Attention을 수행하여 task-specific한 Slot이 되도록 GRU를 통해 업데이트됨. (사람이 해석 가능한 개념에 대응하는 벡터)
        - 여기서 Slot은 interpretable한 Concept을 의미하고, 이 Slot을 더 Task-Specific하게 업데이트하기 위해서 Slot Attention을 수행한다. (Slot Attention은 Related Work에서 설명)
    - 2. Cross-Attention(CA) 모듈
        - CSA 모듈이 만든 개념 임베딩과 Input Feature(백본의 Latent Feature(Feature Map)) 간의 Cross-Attention을 수행한다.
        - Query : Input Feature (백본의 Feature)
        - Key/Value : Concept Embedding (CSA모듈로 얻은 Concept Embedding)
        - Cross-Attention 결과 : 각 백본의 Feature가 어떤 Concept과 관련이 있는지 가중합된 Vector가 생성됨. (어떤 Concept에 주로 의존했는가)
        - 이 가중합된 Vector가 최종 Classification Head로 들어가서 클래스 예측을 수행한다.
    - 3. Loss 설계
        - Explanation Loss(설명 손실) : 도메인 지식을 활용하여 올바른 개념에 집중하도록 유도함.
            - 목적: 모델이 도메인 전문가(Ground Truth)가 중요하다고 생각하는 개념에 집중하도록 유도.
            - 데이터셋에 '이 이미지에서 중요한 개념은 무엇이다' 라는 Ground Truth Concept Annotation이 존재함.
            - 모델이 예측한 Attention 가중치가 Ground Truth Concept Annotation과 얼마나 일치하는지 비교함.
            - Cross-Entropy 또는 Distance Loss로 이 차이를 줄이도록 학습함.
        - Sparsity Loss(희소성 손실) : Attention이 정말 중요한 Feature에 집중하도록, 희소하게 분포하도록 강제함.
            - 목적 : 모델이 모든 Concept에 골고루 Attention 하지 말고, 정말 중요한 Concept에만 집중하도록 강제하는 loss
            - Cross-Attention 가중치의 Entropy를 최소화하도록 학습함
            - Entropy가 작다 → 특정 Concept에 집중 → Sparse한 분포

### Related Work
- SGW(Shared Global Workspace)
    - 여러 모듈이 '공유 메모리'를 통해 정보를 주고받으며, 중요한 정보만 Bottleneck을 통해 공유한다.
    - 기존의 Concept Transformer 논문에서는 개별 image patch만을 기반으로 Concept을 추출하기 때문에, Global Concept을 잘못 학습할 수 있었다.
    - 이 논문에서는 SGW를 통해 서로 다른 모듈간 정보 공유와 Concept 일반화를 동시에 달성하고자 함.

- Slot Attention
    - Slot은 한정된 개수의 Concept을 저장하는 공간이다.
    - CSA에서 Slot Attention을 수행하여 Slot이 더 좋은 Concept Vector표현이 되도록 업데이트됨.
    - 1. 입력
        - Backbone의 Latent Feature가 입력으로 들어옴.
    - 2. Slot의 상태
        - Slot은 고정된 개수로 존재한다.
        - k개의 slot이 있다면, 모델이 학습할 Concept의 수가 k개이고, 사람이 해석할 수 있는 개념 단위가 k개 있다는거임.
    - 3. 경쟁적 Attention 수행
        - Query : 각 Slot들
        - Key/Value : 입력 Feature Vector들
        - 각 slot이 모든 Feature Vector를 보고, '이 Feature는 날 설명하는 feature야' 라고 Weight를 부여함.
        - Attention Weight는 Softmax를 통해 각 Slot별로 나뉘어진다. (합이 1)
        - 즉, feature가 n개 있을 때 각 feature마다 slot에 가중치로 나뉘어지는거임.
    - 4. Feature Aggregation
        - Attention Weight를 이용해 Slot들은 자신이 바라본 Feature의 Weighted Sum을 가져옴.
        - 즉, 한 번의 Input에서 Feature들이 n개로 나뉘어지고, 각 Feature 조각마다 Slot은 Weight를 가지고 있는데, 이 Feature 조각 x weight를 해서 각 Slot이 이 Feature 정보를 요약한다.
        - 이 Feature에서 각 Slot(Concept)이 자신에 맞는 정보만을 잘 나누어 가지고 요약한 것이 됨.
    - 5. Slot Update (반복)
        - 각 Slot은 GRU를 통해 자신의 상태를 업데이트함 → 점점 특정 Concept에 맞춰지게 됨.
        - 이 과정을 여러 번 반복하면 slot이 안정적인 concept embedding으로 수렴하게 된다.

### Proposed Method
- 그래서 이 논문이 뭘 어떻게 하는거냐~~




## 해야 할 남은것들
<li> 교수님이 보라고 하신 논문 읽어보기 (CCT 및 그거관련 아레 얘들 다 관련 있다고 함.) </li>
<li> https://arxiv.org/abs/2305.15775 - Concept-Centric Transformer (아직) </li>
<li> https://arxiv.org/pdf/2502.11418 - TimeCAP (아직) </li>
<li> Improved Knowledge Distillation Based on Global Latent Workspace with MultiModal Knowledge Fusion for Understanding Topological Guidance on Wearable Sensor Data </li>
<li> CTPD (Cross-Modal Temporal Pattern Discovery) </li>
<li> 논문 어떻게 작성할지에 대한 초안 작성 </li>
<li> 영어공부(심심할 때 틈틈히) </li>

---

### Teacher 특이조합에 대해서 실험 진행 (GENE_Activ, PAMAP 둘다 적용) ! 코드작업 선행 필요. (아마 오래 걸릴 것.)
- 특이조합에 대해서는 Wide Resnet 기준 2Teacher(GAF+Sig, PI+Sig) 특이조합과 3Teacher(GAF+PI+Sig) 특이조합에 대해서 실험해야함. (Student는 wrn161 고정.)
- 각 조합에 대해서 Base와 annsp로 실험. (alpha나 lambda는 이미 찾은 값으로.)
- 2Teacher (Image + Signal)
    * Depth-wise - (wrn281 + wrn161), (wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161), (wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163), (wrn163 + wrn281)
- 3Teacher (GAF + PI + Sig)
    * Depth-wise - (wrn281 + wrn161 + wrn161), (wrn161 + wrn281 + wrn161), (wrn161 + wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161 + wrn161), (wrn161 + wrn163 + wrn161), (wrn161 + wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163 + wrn161), (wrn163 + wrn281 + wrn161)

### Teacher - Student 특이조합(모델 구조 완전변형)
- 완전특이조합 Mobile net이나, Resnet, VGG를 이용한 특이조합에 대한 추가 실험
- 여기는 Teacher끼리는 같은 모델을 사용한다.
- 아레의 조합에 대해서 찾아야 할 값 
    - Student의 성능.
    - Sig → Sig의 성능 (1Teacher)
    - Pi+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Pi+Sig → Sig의 성능 (3Teacher, base, ann, annsp)
```
    - T : wrn-163 → S : RN8
    - T : wrn-281 → S : RN20, vgg8
    - T : RN44 → S : vgg8, wrn-161
    - T : MN.V2 → S : RN8
```


## 논문 초안 (어떻게 구성할지)
### Abstract
- 시계열 데이터에 대한 분석이 의료, 산업, IoT 등 여러 분야에서 주목받고 있다.
- Edge단에서 직접 데이터를 수집하고, 데이터를 분석하는 데에는 두 가지 주요 문제점이 있다.
- 데이터에 noise가 많이 껴서 Robustness가 약해진다는 것과, 적은 Resource로 인해 성능이 낮아진다는 점.
- TDA는 Robust한 Feature를 포착하는 것에 강점이 있어서 최근의 머신러닝(딥러닝) 분야에서 매우 주목받고 있다.
- 또한, GAF는 시각 간 상관관계를 잘 표현하여 분류 성능의 향상에 도움이 된다고 널리 알려져 있다.
- 그래서 나는 TDA와 GAF, Signal을 모두 이용한 3-Teacher Multi Modality Knowledge Distillation 방법을 제안하고자 한다.

### Introduction (Background로 넘겨야 할 것 같은거는 제거)
- HAR이 여러 분야에서 사용되고, 각 분야들이 Edge Device(Wearable Sensor Device) 상에서 성능을 높이기 위한 노력들을 하는 중이다.
- 하지만, 노이즈가 많이 낀다는 문제, fps가 떨어진다는 문제, 성능이 낮다는 문제를 해결하기란 쉽지 않다.
- 여기서 노이즈에 대한 Robustness를 해결하기 위한 방법으로는 TDA가 주목받고 있다. (왜? - 너가 채워줘)
- 또한 GAF는 Time step 내의 시각 간 상관관계를 수학적으로 표현할 수 있어서, 원본 Time Series와는 다른 정보를 표현해줄 수 있다.
- 또한 Knowledge Distillation 기법은 큰 모델로부터 작은 모델로 지식을 증류하여, 적은 parameter를 가진 모델로도 높은 성능을 달성할 수 있게 해주는 기법이다.
- Knowledge Distillation 기법에는 logits based와 feature based가 있는데, 이 논문에서는 logits based(기존 kd)와 feature based (spkd)를 모두 이용한 loss를 사용할 것이다.
- 그래서 노이즈 → TDA를 이용한 Teacher, fps가 떨어진다는 문제 + 성능이 낮은 문제 → Knowledge Distillation (logits 기반, spkd를 이용한 feature 기반) + GAF, PI, Sig 모두를 이용한 Multi Modality Teacher 로 해결하려고 한다.
- Contribution은 다음과 같음.
    - 원본 Signal과 이미지 변환 기법으로 얻어낸 이미지들을 가지고, 새로운 framework인 3Teacher Multi Modality Knowledge Distillation 을 제안한다.
    - 각각 개별의 Teacher 성능이 Student보다 낮거나 같더라도 지식 증류로 학습된 Student의 성능이 더 높아질 수 있음을 알 수 있다.

### Background
- 1. TDA에 대한 자세한 설명 (어떻게 고차원 Point Cloud로 넘기고, Persistent Homology가 어떻게 만들어지며, 이것을 어떻게 해서 Persistence Diagram이 되고, 이것을 머신러닝에 적용할 수 있도록 고정 이미지인 Persistent Image가 된다는것에 대한 완벽한 설명)
- 2. GAF에 대한 자세한 설명 (각 time step의 값을 Arctangent를 통해서 극좌표계의 각도로 표현한 뒤 각 각도 사이의 Cosine Similarity를 얻어서 이를 더하거나 빼는 방식으로 GASF, GADF로 나뉘는데 나는 GASF를 사용했다는 것 설명) (구하는 공식이나 행렬은 이미 가진 그림이 있다.)
- 3. Knowledge Distillation에 대한 설명 + MultiModality 지식 증류에서 어떻게 할 것인지 (각 Teacher마다 KD_loss를 따로 계산해서 가중치 alpha를 통해 합이 1이 되도록 반영) + logits based KD와 Feature based KD에 대한 설명 (여기서 feature based KD로 SPKD를 이용한다고 명시)
- 4. Annealing에 대한 설명 (이거는 내가 원래의 의미나 그런걸 잘 몰라서 너가 잘 작성해줘봐 자세한 질문은 나중에 할테니)

### Proposed Method (여기는 세부 목차만 작성할거임.)
- PI 이미지를 추출한 방식.
- GAF 이미지를 추출한 방식.
- Multiple Teacher에서 KD를 어떻게 수행하였는지와 최종 loss설명
    - Multi Teacher에서의 Logits based KD의 loss를 어떻게 구성하였는지에 대한 설명
    - Feature based KD를 하기 위한 SP_Map을 어떻게 만들었는지 (Teacher와 Student의 같은 위치(3군대)에서 Feature Map을 추출해서 Batch x Batch크기의 SP_Map을 얻고, 그것을 MSE로 각각 비교한 뒤, KD_loss에서의 가중치와 같게 하여 반영함.)
    - 각 Teacher마다 추출된 SP Map을 비교해보는것도 해보면 좋을지도. (PI, GAF, Sig 각각 SP_Map을 같은 sample에서 얻어본 뒤 실제로 어떻게 다르게 생겼나 보기)
    - 이들을 종합한 최종 loss가 어떻게 생겼는지 설명.
- 이 논문에서 사용한 Annealing기법 설명.

### Experiment (여기도 세부 목차만)
- 1. 사용한 데이터셋에 대한 설명
- 2. 각 데이터셋마다 적용한 하이퍼파라미터와 그 이유 (실험을 통해 얻었다 or 이전 논문의 결과를 인용한 설명)
- 3. 실험 결과 쭉 나열 및 설명 (parameter 압축 비율 설명, Acc 비교 등.)
- 4. 