## 이번주 한 것.
<li> PAMAP2 3 Teacher 에서 최적의 alpha값 조합 선정 및 ann, annsp 결과 확인중 </li>
<li> GENE Activ 데이터셋 EBKD 적용해서 결과 확인함. </li>
<li> DCD 논문 읽어보고 baseline으로 적용할만 한지 판단 </li>

## 실험 진행 상황
### PAMAP2 3Teahcer에서 Alpha값 찾기.
- 학습 진행을 했는데, 사실상 규칙성같은게 안보여서 뭘 선택할지 고민입니다..
- 일단 wrn-163, wrn-281 에서 top4개 뽑았을 때 중복으로 있는게 [0.3 : 0.2 : 0.5] , [0.4 : 0.3 : 0.3] 둘이었어서 [3:2:5]를 학습중이긴 한데, GAF, PI, Sig 중 더 높은 가중치를 줄 것을 찾고싶은데 어떻게 해야할지..

### PAMAP2와 GENEActiv 실험결과
- 계속 쭉 진행중...
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.19/PAMAP.png" alt="PAMAP" width="700">

- GENEActiv에 EBKD를 적용한 결과 (오늘 저녁에 14cls 학습 끝날 것 같아서 끝나고 추가하겠습니다.)
- EBKD는 Teacher들의 logits 분포를 확인하고, 그것을 통해 Entropy(출력 분포의 불확실성)을 측정한다.
- Entropy가 낮을수록 Teahcer의 출력이 확신이 있다는 의미이고, Entropy가 클 수록 출력이 불확실하다는 것이므로 가중치를 낮게 설정하는 식임.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.19/GENEActiv.png" alt="GENEActiv" width="700">


## DCD 논문리뷰 (Discriminative and Consistent Representation Distillation)
### Introduction
- 최근의 대조 학습(Contrastive Learning)에서 Label 없이(self-supervised) 특징 표현 학습 분야를 크게 revolutional하게 바꿨고, Label 정보 없이도, 샘플간 구별력(Discriminative Features)을 효과적으로 학습할 수 있음을 보여줬다.
- 그래서 최근에 KD 프레임워크에도 대조 학습 방식을 적용하기 시작함.
- 그러나 기존의 대조 학습 기반 Distillation 기법들은 아레와 같은 한계가 있다.
    * 부정 샘플(negative)을 저장하기 위한 거대 메모리 뱅크가 필요함.
    * 대조 학습에 쓰이는 하이퍼파라미터(temperature, negative 계수)가 고정되어 있어 상황에 맞춰 적응하기 어렵다.
    * 교사 모델이 학습한 샘플 간 구조적 관계(클래스 간 상대거리)를 학생 모델이 받아먹지 못하는 문제.
    * 판별력(discrimination)만 강화하는데 집중하면, 교사와 학생이 표현 공간에서 공유해야 할 일관된 패턴을 놓쳐, 지식 전이가 최적화되지 않을 수 있음.
- 그래서 DCD를 제안한다. DCD는 다음과 같음
    * Discriminative(변별력)능력과 Structually Consistence representation을 모두 챙기는 방식을 사용함.
    * DCD는 기존의 Crossentropy Loss, KD Loss에 Contrast Loss와 Consist Loss가 추가됨.

### Related Work
- Knowledge Distillation은 logits based distillation과 feature based distillation으로 나뉘어진다.
- CRD(Contrastive Representation Distillation)라는 논문이 있었음.
- CRD는 교사 표현(f_t) vs 학생(f_s) 표헌의 positive쌍과 과거 배치에서 뽑힌 표현들을 많이 Negative로 삼아야 강한 대조 학습 효과를 낼 수 있기에 메모리적 부담이 컸다.
- DCD에서는 배치 크기가 N이라면, 1개만 positive(서로 같은거), 나머지 N-1개를 Negative로 해서 메모리 부담을 대폭 감소시킴.















---
---
---
---


## 해야 할 남은것들
<li> KD baseline 찾기 (Discord에 교수님이 올려주신거 논문 다 읽어보기.) </li>
    * Discriminative and Consistent Representation Distillation (DCD)
    * 

<li> 교수님이 보라고 하신 논문 읽어보기 (후순위) </li>
<li> https://arxiv.org/abs/2305.15775 - Concept-Centric Transformer (아직) </li>
<li> https://arxiv.org/pdf/2502.11418 - TimeCAP (아직) </li>
<li> 영어공부 </li>

### Teacher 특이조합에 대해서 실험 진행 (GENE_Activ, PAMAP 둘다 적용)
- 특이조합에 대해서는 Wide Resnet 기준 2Teacher(GAF+Sig, PI+Sig) 특이조합과 3Teacher(GAF+PI+Sig) 특이조합에 대해서 실험해야함. (Student는 wrn161 고정.)
- 각 조합에 대해서 Base와 annsp로 실험. (alpha나 lambda는 이미 찾은 값으로.)
- 2Teacher (Image + Signal)
    * Depth-wise - (wrn281 + wrn161), (wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161), (wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163), (wrn163 + wrn281)
- 3Teacher (GAF + PI + Sig)
    * Depth-wise - (wrn281 + wrn161 + wrn161), (wrn161 + wrn281 + wrn161), (wrn161 + wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161 + wrn161), (wrn161 + wrn163 + wrn161), (wrn161 + wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163 + wrn161), (wrn163 + wrn281 + wrn161)

### Teacher - Student 특이조합(모델 구조 완전변형)
- 완전특이조합 Mobile net이나, Resnet, VGG를 이용한 특이조합에 대한 추가 실험
- 여기는 Teacher끼리는 같은 모델을 사용한다.
- 아레의 조합에 대해서 찾아야 할 값 
    - Student의 성능.
    - Sig → Sig의 성능 (1Teacher)
    - Pi+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Pi+Sig → Sig의 성능 (3Teacher, base, ann, annsp)
```
    - T : wrn-163 → S : RN8
    - T : wrn-281 → S : RN20, vgg8
    - T : RN44 → S : vgg8, wrn-161
    - T : MN.V2 → S : RN8
```