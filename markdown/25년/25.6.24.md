## 이번주 한 것.
<li> PAMAP2 3 Teacher 에서 최적의 alpha값 조합 선정 및 ann, annsp 결과 확인중 </li>
<li> GENE Activ 데이터셋 EBKD 적용해서 결과 확인함. </li>
<li> DCD 논문 읽어보고 baseline으로 적용할만 한지 판단 </li>

## 실험 진행 상황
### PAMAP2와 GENEActiv 실험결과
- PAMAP2는 3Teacher에 대해서 4:3:3을 최적의 alpha조합으로 선정하여 base에서의 wrn161, wrn283 을 추가로 실험, ann도 실험, annsp까지 쭉 이어서 실험할 예정
- 현재 ann은 거의 끝난 상태이고, base의 wrn16-1, wrn28-3 진행중
- 이번주 내로 annsp와 PAMAP2에서의 ebkd까지 완료하는것이 목표.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.24/PAMAP.png" alt="PAMAP" width="700">


- GENEActiv에 EBKD를 적용한 결과를 포함한 전체 결과
- EBKD는 Teacher들의 logits 분포를 확인하고, 그것을 통해 Entropy(출력 분포의 불확실성)을 측정한다.
- Entropy가 낮을수록 Teahcer의 출력이 확신이 있다는 의미이고, Entropy가 클 수록 출력이 불확실하다는 것이므로 가중치를 낮게 설정하는 식임.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.24/GENEActiv.png" alt="GENEActiv" width="700">


## DCD 논문리뷰 (Discriminative and Consistent Representation Distillation)
### Introduction
- 최근의 대조 학습(Contrastive Learning)에서 Label 없이(self-supervised) 특징 표현 학습 분야를 크게 revolutional하게 바꿨고, Label 정보 없이도, 샘플간 구별력(Discriminative Features)을 효과적으로 학습할 수 있음을 보여줬다.
- 그래서 최근에 KD 프레임워크에도 대조 학습 방식을 적용하기 시작함.
- 그러나 기존의 대조 학습 기반 Distillation 기법들은 아레와 같은 한계가 있다.
    * 부정 샘플(negative)을 저장하기 위한 거대 메모리 뱅크가 필요함.
    * 대조 학습에 쓰이는 하이퍼파라미터(temperature, negative 계수)가 고정되어 있어 상황에 맞춰 적응하기 어렵다.
    * 교사 모델이 학습한 샘플 간 구조적 관계(클래스 간 상대거리)를 학생 모델이 받아먹지 못하는 문제.
    * 판별력(discrimination)만 강화하는데 집중하면, 교사와 학생이 표현 공간에서 공유해야 할 일관된 패턴을 놓쳐, 지식 전이가 최적화되지 않을 수 있음.

### Related Work
- Knowledge Distillation은 logits based distillation과 feature based distillation으로 나뉘어진다.
- CRD(Contrastive Representation Distillation)라는 논문이 있었음.
- CRD는 교사 표현(f_t) vs 학생(f_s) 표헌의 positive쌍과 과거 배치에서 뽑힌 표현들을 많이 Negative로 삼아야 강한 대조 학습 효과를 낼 수 있기에 메모리적 부담이 컸다.
- DCD에서는 배치 크기가 N이라면, 1개만 positive(서로 같은거), 나머지 N-1개를 Negative로 해서 메모리 부담을 대폭 감소시킴.


### Proposed Method
- DCD는 다음과 같음
    * Discriminative(변별력)능력과 Structually Consistence representation을 모두 챙기는 방식을 사용함.
    * DCD는 기존의 Crossentropy Loss, KD Loss에 Contrast Loss와 Consist Loss가 추가됨.
    * Contrastive Distillation Loss : Teacher와 Student에 넣었을 때 같은 샘플에서 나온 Feature Map에 대해서 같은 샘플에서 나온거라면 비슷해지도록, 다른 샘플에서 나온거라면 멀어지게 하는 손실이다.
    * Consistent Distillation Loss : 배치 전체의 샘플 간에 유사도 분포를 Teacher 모델과 Student 모델에서 각각 추출한 뒤, KL 발산을 통해 그 분포를 비슷해지게 하는 손실 항임.
    * 또한, DCD에서는 temperature와 bias를 학습 가능한 파라미터로 설정함.

- 또한 추가적으로 기존 CRD 에서의 메모리 문제를 해결하기 위해 Memory Efficient Sampling 방식을 사용함.
    * CRD에서는 수천~수만 개의 negative 새믈들을 저장하는 메모리 뱅크를 썼지만, DCD는 현재 batch안에 이미 존재하는 샘플들을 그대로 Negative로 쓴다.
    * 이거로 구현이 단순화되고, 메모리를 절약할 수 있다고 함.

- Projection Head와 정규화
    * Teacher와 Student의 중간 Layer의 Feature Map이 shape이 다른 경우 이를 맞춰주기 위함.
    * 작은 MLP에 넣거나 1x1 conv를 통과시켜 같은 차원으로 매핑한다고 함.

- Learnable Temperature & Bias
    * 대부분의 Contrastive 학습은 온도 하이퍼파라미터를 고정하는데, DCD에서는 이를 학습 가능한 파라미터로 구현함.
    * 학습 도중에 모델의 학습 상태에 따라 자동으로 조절되어 사용된다.

- 논문에 그림도 없고, 자세히 어떻게 저렇게 한다는건지 모르겠어서 코드 보면서 적용할만한지와 어떻게 동작하는건지 공부해보려 합니다.

---

## 해야 할 남은것들
<li> KD baseline 찾기 (Discord에 교수님이 올려주신거 논문 다 읽어보기.) </li>
    * Discriminative and Consistent Representation Distillation (DCD) 
    * https://ieeexplore.ieee.org/abstract/document/10965820
    * https://ojs.aaai.org/index.php/AAAI/article/view/32266
    * https://ojs.aaai.org/index.php/AAAI/article/view/34031

<li> 교수님이 보라고 하신 논문 읽어보기 (후순위) </li>
<li> https://arxiv.org/abs/2305.15775 - Concept-Centric Transformer (아직) </li>
<li> https://arxiv.org/pdf/2502.11418 - TimeCAP (아직) </li>
<li> 영어공부(심심할 때 틈틈히) </li>

### Teacher 특이조합에 대해서 실험 진행 (GENE_Activ, PAMAP 둘다 적용)
- 특이조합에 대해서는 Wide Resnet 기준 2Teacher(GAF+Sig, PI+Sig) 특이조합과 3Teacher(GAF+PI+Sig) 특이조합에 대해서 실험해야함. (Student는 wrn161 고정.)
- 각 조합에 대해서 Base와 annsp로 실험. (alpha나 lambda는 이미 찾은 값으로.)
- 2Teacher (Image + Signal)
    * Depth-wise - (wrn281 + wrn161), (wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161), (wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163), (wrn163 + wrn281)
- 3Teacher (GAF + PI + Sig)
    * Depth-wise - (wrn281 + wrn161 + wrn161), (wrn161 + wrn281 + wrn161), (wrn161 + wrn161 + wrn281)
    * Width-wise - (wrn163 + wrn161 + wrn161), (wrn161 + wrn163 + wrn161), (wrn161 + wrn161 + wrn163)
    * D+W-wise - (wrn281 + wrn163 + wrn161), (wrn163 + wrn281 + wrn161)

### Teacher - Student 특이조합(모델 구조 완전변형)
- 완전특이조합 Mobile net이나, Resnet, VGG를 이용한 특이조합에 대한 추가 실험
- 여기는 Teacher끼리는 같은 모델을 사용한다.
- 아레의 조합에 대해서 찾아야 할 값 
    - Student의 성능.
    - Sig → Sig의 성능 (1Teacher)
    - Pi+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Sig → Sig의 성능 (2Teacher, base, ann)
    - GAF+Pi+Sig → Sig의 성능 (3Teacher, base, ann, annsp)
```
    - T : wrn-163 → S : RN8
    - T : wrn-281 → S : RN20, vgg8
    - T : RN44 → S : vgg8, wrn-161
    - T : MN.V2 → S : RN8
```