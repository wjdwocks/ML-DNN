## 이번주 하려고 하는 것.
<li> PAMAP2 3 Teacher 에서 최적의 alpha값 조합 찾기 </li>
<li> EBKD 이해 (모르고 쓸 순 없으니...) </li>
<li> U-Net 기반 Stable Diffusion을 ImageNet으로 학습(Pretrain) </li>
<li> 이후, Pretrain된 모델을 가지고, 원하는 도메인의 이미지를 추가 학습한 후, Prompt Embedding을 추가한 후 직접 이미지를 출력해보는 실습을 해보려고 함. </li>
<li> 혹은 BERT와 같은 오픈 라이브러리 LLM을 통해서 생성형 AI 고도화 실습 진행해보고 싶음. </li>

## PAMAP2 3Teahcer에서 Alpha값 찾기.
- GENEActiv때랑 똑같은 조합들에 대해서 (226, 235, 325, 334, 333) 에 대해서 할것임.

## 학습 진행 상황
- 계속 쭉 진행중...
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.6.6/PAMAP.png" alt="PAMAP" width="700">

## 생성형 AI쪽 공부
- Stable Diffusion 공부 및 MNIST 실습 해보려고 함
- 그 다음에 Text Embedding까지 넣는 pretrained 모델에 적용해보려고 함.
- Kaggle에 애니메이션 캐릭터 이미지 데이터셋이 있어서 생성형 AI 모델 학습 해보는중.
- 각 애니메이션 제목-데이터들 이 있어서, 위에서 본 MNIST-Diffusion(Unet) 모델에, Text_emb을 추가하여 학습을 하는 모델을 만듦.
- CLIPTokenizer와 CLIPTextModel을 이용해서, Text Embedding을 U-Net에 넘겨줬다.
- 해보려고 하는것은 내가 Sampling을 할 때 Text로 특정 애니메이션 제목을 같이 넣어주면 그 그림채로 이미지가 생성되게 하는것.
- 자세한 정리는 시험 끝나고 하겠습니다..
- 아레 폴더에 train_stable_diffusion.py에서 학습을 돌리고, Diffusion_Model.py와 UNet.py에 모델 정의가 되어있습니다.
https://github.com/wjdwocks/ML-DNN/tree/main/ML/pytorch/auto_encoder