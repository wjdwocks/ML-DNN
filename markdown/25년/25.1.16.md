## 25.1.16 까지 공부할 내용
<li> GAF 코드로 단일 네트워크 (Teacher의 가중치를 위한) Train Manager부분 분석  </li>
<li> GENE_7cls 에 있는 코드로 2 Teacher KD (sig + GAF)를 하면 성능이 이상하게 나옴. 이쪽 코드를 좀 더 보고, KD쪽이랑, sp1t 이런얘들 뭐였는지 공부. </li>
<li> 그 후 GENE_7cls 부분의 코드를 내꺼랑 맞춰서 학습 돌려보고, 2 Teacher KD (sig + GAF) 성능 뽑아보기. </li>
<li> 논문 리뷰 (너무 자세히는 말고) </li>
<ol>
<li> Semantics-Aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition </li>
<li> CROSS-MODAL Knowledge Distillation for Vision-To-Sensor Action Recognition </li>
<li> Human Activity Recognition Based on Gramian Angular Field and Deep Convolutional Neural Network </li>
</ol>

## Semantics-Aware Adaptive Knowledge Distillation for Sensor-to-Vision Action Recognition 논문
### Abstract
<li> Vision 기반의 행동 인식은 카메라의 특성상 가려짐, 밝기 변화, 시점의 변화 등에 취약하지만, Wearable Sensor Data의 경우 1차원 시계열 신호를 통해 인간의 동작을 포착함으로써 이러한 문제를 완화할 수 있다. </li>
<li>  </li>

### Introduction
<li>  </li>

### Related Work
<li>  </li>

### Proposed Method
<li>  </li>

### 내 생각.
<li>  </li>


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>