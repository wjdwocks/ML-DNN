### 이번주 하려고 하는 것.
<li> SPKD 논문 + 교수님 코드를 보며 중간 Layer의 Feature Map간의 비교를 통해 학습하는 코드 작성 후 실험 (GENE 7cls, 14cls) </li>
<li> GENE Activ 7cls(500w, 1000w), 14cls에 대해서 Annealing을 적용한 2 Teacher(GAF, Sig)학습. </li>
<li> PAMAP 데이터셋에 대해 Teacher로 사용할 네트워크 학습. (0 ~ 8 OVR로 9번 x 3 trial로 총 27개의 학습) </li>
<li> Object Detection에 대해서 공부해보려고 한다. (Yolo, 어떤 식의 데이터로 어떤 식의 학습이 이루어지는가.) </li>
<li> VAE, WAE에 대해 MNIST로 공부... (어렵구만.) </li>


### SPKD 논문 리뷰
1. SPKD는 Teacher와 Student의 특정 Layer를 지난 output 끼리 서로 MSE로 비교하여 비슷해지도록 학습하는 것을 의미함.
2. 그렇다면 특정 Layer를 지난 출력의 크기나, 깊이가 서로 다르다면? <<< 우리 코드에서는 어떻게 했는지도 보자.
3. 원래라면 크기가 다르다면, Interpolation이나, AvgPool, Upsample, ConvTranspose2d를 사용하여 넘겨줘서 크기를 맞춰준다.
4. 깊이가 다르다면, 1x1 Conv를 사용하여 Channel을 동일하게 변환한다.


### Wide Resnet을 구현하는 우리 코드 - WideResNet_sp
1. WideResNet_sp를 만들어서 얘는 spkd를 사용할 때만 사용하도록 했음.
2. conv1을 생성하고, 처음 Stage에 들어가기 전 out_features를 16개로 늘려줌.
3. Residual Stage를 3개로 구성할 것이므로, 각 Residual Stage에 포함될 Residual Block의 개수 N = (d-4)//6 으로 계산한다. (WRN16-3에서 d = 16, k = 3, strides = [1, 1, 2, 2])
4. 여기서 N = (d - 4) // 6 으로 6으로 나누는 이유는, 각 Stage마다, N개의 Block이 나올텐데, 각 Block마다 2개의 Conv layer를 가지기 때문.
5. 첫 번째 Residual Stage를 들어갈 때에는 모든 Block의 stride = 1로 설정됨 (feature map(입력)의 크기를 줄이지 않기 위해서이다.)
6. 즉, 첫 번째 Residual Stage에서의 모든 Block에서는 stride = 1임. 즉, subsample_input = False로 고정되고, skip connection을 할 때 원본 x의 사이즈를 조절해 줄 필요는 없다.
7. 두 번째 Residual Stage에서는 첫 번째 Block의 stride = 2, 나머지 Block의 stride = 1로 하여, 첫 번째 블록에서 입력 크기를 반으로 줄이고, 깊이를 두배 늘리는 작업을 한다.
8. 즉, 두 번째 Residual Stage부터는 Skip Connection을 할 때 원본 x의 입력 크기도 그에 맞춰 조절해 주어야 한다는 것을 의미함. (첫 번째 블록의 conv_inp()가 그것을 의미함.)
9. 그렇다면 layer에서 Depth를 조절할 때 고려해야할 것은 무엇일까.
10. WRN161과 WRN163의 차이는 첫 번째 Residual Stage에서 output_features를 1배로 고정하는가, 3배로 늘리는가의 차이가 있다.
11. 이에 따라서, 첫 번째 Residual Stage의 Residual Block에서도 입력 크기의 변화는 없지만, 입출력 channel의 차이가 생기므로, skip connection을 할 때 원본 입력에 channel을 조절해 주어야 한다.
12. 이 때에도 위의 입력 크기 문제와 같이 IndividualBlock1에 있는 conv_inp에서 channel의 크기를 맞춰주는 역할을 같이 수행하게 된다.




# WideResNet_sp 모델 구현 및 구조 설명

WideResNet_sp는 SPKD(Similarity-Preserving Knowledge Distillation)에 특화된 모델로, Wide ResNet 구조를 기반으로 한다. 이 모델의 주요 구성 요소와 Depth 계산, Residual Stage에서의 Skip Connection 처리 등을 설명한다.

---

## 1. 모델 개요

- **WideResNet_sp**는 SPKD 적용 시 사용하도록 설계됨.
- 모델은 **초기 Convolution Layer**로 시작하여, 이후 **3개의 Residual Stage**로 구성됨.
- 각 Residual Stage에는 여러 개의 Residual Block이 포함되며, 최종적으로 **Global Average Pooling(GAP)**와 **Fully Connected(FC) Layer**를 통해 분류 결과를 도출함.

---

## 2. 초기 Convolution Layer 및 Feature 확장

- **conv1 레이어**를 통해 입력 이미지를 먼저 처리함.
- conv1 이후, 첫 번째 Residual Stage에 들어가기 전에 **출력 채널 수를 16(또는 k에 따른 16×k)으로 확장**하여 후속 Stage들이 일관된 채널 수를 기반으로 동작할 수 있도록 함.

---

## 3. Residual Stage 구성과 Residual Block 반복 횟수

- 전체 Residual Stage는 **3개**로 구성됨.
- 각 Stage에 포함될 **Residual Block의 개수 N은 (d - 4) / 6 으로 계산됨.**
  - 여기서 **d**는 전체 Depth를 의미함.
  - 예를 들어, **WRN16-3**에서는 d = 16, k = 3, 그리고 strides = [1, 1, 2, 2]로 설정됨.
  - 6으로 나누는 이유는 각 Residual Block 내에 **2개의 Convolution 레이어**가 포함되고, 이 블록이 **3개의 Stage**에 걸쳐 반복되기 때문.

---

## 4. Residual Stage의 특징 및 Skip Connection 처리

### 첫 번째 Residual Stage

- **모든 Residual Block의 stride가 1**로 설정되어, 입력 Feature Map의 크기를 유지함.
- 입력과 출력의 크기가 동일하므로 Skip Connection 시 원본 입력 x를 그대로 더할 수 있음.
- 단, 채널 수가 달라질 경우에는 **1×1 Convolution(conv_inp)**을 통해 채널 수를 맞춰 줌.

### 두 번째 및 세 번째 Residual Stage

- **첫 번째 Block에서는 stride가 2**로 설정되어 입력 Feature Map의 크기를 절반으로 줄임.
- 이때 Skip Connection을 위해 원본 입력 x의 크기와 채널 수가 변경된 출력 x1과 일치해야 하므로, **1×1 Convolution(conv_inp)**을 사용하여 x의 크기(및 채널 수)를 변환함.

---

## 5. Depth 계산에 포함되는 구성 요소

- WideResNet의 Depth d는 **학습 가능한 가중치를 가진 Convolution Layer와 Fully Connected Layer만 포함하여 계산됨.**
- **BatchNorm, ReLU, Pooling** 등은 가중치가 없거나 단순한 연산이므로 Depth 계산에서 제외됨.
- Depth 계산 공식:
  
  d = 1 (conv1) + 6N (Residual Blocks 내 2개의 Conv per Block, 3 Stage) + 1 (FC)
  
  - 예를 들어, WRN16-3에서는 1 + 6N + 1 = 16이 되어야 하며, 첫 번째 Residual Stage에서는 입력 크기가 변하지 않으므로 subsample_input = False가 적용됨.

---

## 6. WRN161 vs. WRN163의 차이 및 첫 번째 Stage 처리

- **WRN161과 WRN163**는 **첫 번째 Residual Stage에서 출력 채널 수를 늘릴지(increase_filters 적용 여부)에 따라 구분됨.**
  - WRN161의 경우, 첫 번째 Stage에서는 채널 수를 그대로 유지하여 increase_filters = False가 될 수 있음.
  - 반면, WRN163는 첫 번째 Stage에서도 채널 수를 확장하여 increase_filters = True가 적용됨.
- 이 차이에 따라 Skip Connection 시 사용되는 1×1 Convolution(conv_inp)의 적용 여부와 방식이 달라지며, 이는 최종 모델의 Depth와 Feature Representation에도 영향을 미침.

---

## 7. Skip Connection과 conv_inp(x)의 필요성

- **Skip Connection (x + x1)을 수행하기 위해서는 x와 x1의 크기와 채널 수가 동일해야 함.**
- Residual Block 내에서 **Downsampling(stride=2)나 채널 증가가 발생하면** x와 x1의 shape가 달라지므로,  
  **1×1 Convolution(conv_inp)**을 사용하여 x를 변환해 두 텐서의 shape를 맞춤.
- 이 과정는 두 가지 상황에서 필요함:
  - **크기(Spatial Dimension)가 다른 경우:**  
    - 예) x의 shape이 (B, C, 32, 32)인데, x1이 (B, C, 16, 16)인 경우
  - **채널 수가 다른 경우:**  
    - 예) x의 채널이 16인데, x1의 채널이 32인 경우

---

## 8. 요약 및 결론

1. **WideResNet_sp**는 SPKD 적용을 위해 설계된 모델로, 초기 Convolution Layer와 3개의 Residual Stage, GAP 및 FC Layer로 구성됨.
2. **Residual Block의 반복 횟수 N**은 (d - 4) / 6 으로 계산되며, 각 Block 내에 2개의 Conv Layer가 포함됨.
3. **첫 번째 Residual Stage**에서는 Feature Map의 크기를 유지하기 위해 stride가 1로 설정되며, 채널 수가 변할 경우에만 1×1 Convolution(conv_inp)를 사용하여 채널을 맞춤.
4. **두 번째 및 세 번째 Residual Stage**에서는 첫 번째 Block에서 stride가 2로 설정되어 다운샘플링을 수행하며, 이때 conv_inp를 사용해 입력 x를 변환하여 Skip Connection을 올바르게 수행함.
5. **Depth 계산**은 학습 가능한 가중치를 가진 Conv Layer와 FC Layer만 포함하며, BatchNorm, ReLU, Pooling 등은 제외됨.
6. **WRN161 vs. WRN163**는 첫 번째 Residual Stage에서 채널 수 확장의 여부에 따라 구분되며, 이로 인해 Skip Connection을 위한 conv_inp의 적용 방식이 달라짐.
7. **Skip Connection (x + x1)을 올바르게 수행하기 위해서는 두 텐서의 shape가 동일해야 하며, conv_inp(x)는 이를 맞추기 위한 핵심 역할을 함.

WideResNet_sp의 구조는 모델의 Depth, 채널 수, 그리고 Feature Map의 크기를 조절하여 Residual Connection과 SPKD 적용에 최적화된 구조를 구현함.





### Object Detection 공부.
1. 난중에에에에에에에에엥엥ㅇ엥엥엥ㅇ




### SP를 사용한 학습 설명
1. sp_param이 GENE_Activ에는 700, PAMAP2에는 200으로 설정이 되었는데 이것이 무엇일까?
2. sp_param(β)은 앞의 CE_Loss(Student)와 KD_Loss(T1, T2, S)간의 가중치는 λ로 관리하는데, 그 때 SP_Loss를 몇으로 설정해야 학습이 골고루 잘 될까 하는 하이퍼 파라미터이다.
3. α는 두 Teacher 간 logits loss를 합칠 때의 가중치
4. k는 분할 개수(?) : Feature Representation을 나누는 개수. 이게 4였기 때문에, 우리 코드에서 최종 출력(1), At1(2), At2(3), At3(4)으로 총 4개임.

<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.13/TPKD_sp_param.png" alt="sp_param이 뭐냐" width="500">




## PAMAP2 데이터 분석
### 데이터 형태
1. 83319개의 데이터와 각 데이터는 40개의 Feature를 가지고 있다.
2. 83319개의 데이터는 0 ~ 11에 포함하는 라벨을 각각 가지고 있다.
3. 데이터셋에서 9명의 사람으로부터 83319개의 데이터를 얻은 것이다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP.png" alt="PAMAP shape" width="500">

### 학습할 때 데이터의 사용
1. 9명의 사람은 0 ~ 8의 id로 구분되고, 학습을 할 때 그 중 한명을 Test_id로 지정하여 남은 8명은 Train set으로 학습한다.
2. 이렇기 때문에, 같은 사람의 데이터를 학습하고, 테스트하게 되는 불상사는 일어나지 않음.
3. 같은 사람에서 나온 데이터를 학습하고, 테스트때 사용한다면, Acc가 높게 나올 확률이 높다. (GENE_Active때 겪어본 문제.)

### 데이터의 사용
1. .data 파일의 형식으로 저장되어있다. (GENE_Active랑 다르다.)
2. 이렇게 .data 파일로 되어있는 경우 .npy파일과 다르게 pickle.load를 통하여 데이터를 얻어왔다.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP_pickle.png" alt="PAMAP Pickle load" width="500">
3. 근데 cls_id = [24, 1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17] 인데, 원래 PAMAP2에는 24번 label이 없다. 24번은 뭘까
4. 0, 9, 10, 11을 묶어서 24로 놓은건가?
5. 근데 데이터를 로드해와서 np.unique(labels)를 찍어보니, 0 ~ 11로 정렬하게 나와있다. (아마 Encoding된 Label인듯.)
6. 뭐 어쨌든 데이터를 로드해왔다고 치고. (이해가 안감.) 데이터를 어떤식으로 불러와서 load하는지 보자.
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP2_data_load.png" alt="PAMAP Data Load" width="500">
7. 데이터를 window크기 100씩으로 불러오는데, 시작 지점은 22씩만 늘어난다. (나머지 78개의 sequence는 계속 겹치면서 data를 로드하고 있다.)
<img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/25.2.6/PAMAP2_step_size.png" alt="PAMAP Step Size" width="500">
8. 그 다음, 데이터를 만들고 싶은 크기로 x_train, y_train, x_test, y_test를 np.zeros로 만들어주고, 하나씩 넣어준다.
9. 왜하는가? > (idx, window_length, channel)의 형태를 (idx, batch_size, window_length, channel)의 형태로 바꿔주기 위함임.


### 내가 한 GAF데이터 형성 및 학습
1. test_id(0 ~ 8)을 기준으로 각각 총 9개의 Train, Test 데이터셋을 만듦. (좀 비효율적인듯.)
2. 그러고 나서 np.load를 통해 이 데이터만 가져와서 빠른 데이터 load 후 단일(Teacher로 사용할) 네트워크를 학습함.

### 나중에 KD에 사용할 데이터 로더함수
1. Signal과 GAF Image를 모두 받아오도록 작성해야함. (이미 GENE Activ에서 했던거라 금방 할듯.)
2. Teacher 네트워크들 학습하는데 한세월 걸릴것같아서 천천히 하려고 한다.


## Trainning Environment
<ul>
<li> Dataset = GENE(life_log), PAMAP(이것도 HAR 데이터) </li> 
<li> python = 3.8.18 </li>
<li> pytorch = 2.3.0 + (cu12.1), CUDA 11.8 </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 200 </li>
<li> batch size = 128 </li>
<li> learning rate = 0.05 - 0.0001 (Adjust Learning Rate) </li>
<li> optimizer = Momentum + SGD </li>
</ul>