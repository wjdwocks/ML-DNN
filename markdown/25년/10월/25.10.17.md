### 25.10.13 줌미팅 내용
- Reconstruction Loss : 아키텍처에 포함되어있는 대부분의 Encoder, Decoder의 성능 향상을 위해 하는 Loss
- Codebook 내의 Code가 뭔지? 
    - CCT에서는 각 Concept이 무엇을 의미하는지 이미지 내에서 하이라이트가 가능함
        - "이게 뭐다" 라는게 가능함.
    - 하지만 VQShape에서는 그 Code를 Time-series로 변환했을 때 그 time-series가 뭘 의미하는지를 모르기 때문에 그게 안됨. 
        - "이게 뭔지 모름"


### 이 논문(vqshape)을 통해 가져갈 Motivation
- Codebook에 포함된 Sequence가 '다리를 올린다', '주먹을 쥔다' 와 같은 그룹화된 파형이 되도록 하면 좋겠다.
- Code 하나하나에 어떻게 의미를 만들까?
    - Text로 표현하게 한다?
    - LSA 논문을 다시 처음부터 읽고, 어떻게 Semantic을 넣어줄 수 있을지 알아봐야 함.
    - Code에 Text 표현을 넣으면 이 각 Code(subsequence)가 어떤 의미(semantic)를 가지는지를 알 수 있다.
- Text라는 정보가 포함될 때 z, mu, sigma, t, l이 어떻게 바뀔 수 있을까?
- Text Guidance를 가져왔을 때(Label을 이용한 Sentence 등) 이 Text Guidance에는 시간 순서에 대한 정보가 없는데, 우리는 Time-series의 SubSequence로 Code를 얻을텐데, 이것에는 시간 정보가 있고, 각 Subsequence마다 의미를 부여해주고 싶은건데, 그것을 어떻게 할 수 있을까?
    - 어떤 논문에서는 이렇게 했다? (에반디)
    - 대충 Concept적으로 이렇게 해볼 수 있을지 않을까? 를 알아와라..
    - 잘 설명할 수 있도록 잘 정리해서 다음 미팅때 쭉 말해보자.

### 코드 작성에 관한 것.
- VQShape쪽을 Base로, GLW를 가져와서 붙여보는 것으로.
- 아직 때가 아님.

---

## 이번 주 할 일.
1. LSA 논문을 다시 읽고, Codebook Code에 어떻게 Text를 통해서 각 Code가 Semantic을 지니게 할 수 있을지 생각해보자. (내일)
2. 그런데 전달되는 Text가 아레의 두 개가 될 수 있음. (각 방식의 장단점을 생각해보면 좋을듯.) 
    - Time-series → LLM → Tokenizer
    - Label y → LLM → Tokenizer
3. 그렇게 Text Guidance를 넘겨줬을 때 각 Attributes들이 z, mu, sigma, t, l이 있을텐데, 얘네가 뭘 의미하게 하면 좋을지나/ 다르게 바꾸는 것도 생각해보면 좋을듯.


4. 부족한 코드 정보에 대해서 더 공부해보아야 함.

5. 3Teacher 논문 작성 및 실험
    - WRN 다른 조합 2Teacher 그림 Robustness쪽 논문 참고해서 히스토그램으로 처리.

    - Tolerance Noise : 모든 Teacher에 대해서 다 그려보기
    - Confusion Matrix : 다른 Student로 더 좋은게 있나 보기
    - 실험 중 학습하는 / 학습한 SP Maps들 시각화 및 비교 : 2Teacher꺼들 지워서도 그려보고

    - Hyper parameter : 3Teacher만 표로 Alpha 비교해서 한다.
    - Parametric Plot : 하던대로


## 써야할 논문에 대해서 지식 구체화 (1 ~ 3)
### 지금까지 알고 있는 정보 정리.
- LSA의 방식
    - [Training Time]
    - 각 Sample마다의 Label을 이용하여 Text Guidance를 제공한다.
    - Label → LLM을 이용하여 얻은 Text (Keyward, Sentence, Paraphrase) → Text Encoder → Text Tokenizer
    - 이후, Time-series와 같은 Shape으로 만들기 위한 Projection Layer를 통과한 후, 각각을 Query, key/value로 하여 Cross Attention을 통해 정보 혼합 후 Cross Attention 전의 Main 모달리티 벡터(Query로 썼던)와 더하여 그것을 이용하여 최종 Prediction을 수행.(Sig를 메인, Text를 메인 으로 각각 1번씩 총 2번) 
    - [Texting Time]
    - 테스트 시에는 Time-series만을 입력으로 받음 → Signal Encoder를 통해서 signal Embedding으로 만듦 → 이것만 받아서 Classifier를 통해 Prediction 수행.
    - 이것이 되는 이유는 학습 때 각각의 모달리티 정보를 메인으로 해서, 정보를 융합했고, 그 융합된 정보로 Classifier가 학습이 되었음.
    - 또한, 융합된 정보를 가지고 Prediction을 한 것으로 각 Signal Encoder, Text Encoder까지도 Loss가 흘러갔기 때문에 이 Encoder들도 같이 학습이 됨.
    - 마지막으로, 각 모달리티를 메인으로 융합된 정보끼리도 Contrastive Loss를 통해 비슷해지도록 했기 때문에 결국 Signal Encoder나, Text Encoder는 서로 Embedding을 생성할 때 최대한 두 모달리티가 잘 융합된 정보처럼 생성하도록 자동으로 학습이 되기 때문에 이 Testing이 가능하게 된다. (물론 Text Encoder는 아마 Freeze)

- VQShape의 방식
    - Time-series만 사용함.
    - Time-series 정보만을 이용해서 Codebook을 만들었고, 한 Sample을 TS Encoder → Tokenizer → Attribute Decoder를 통해서 여러 개의 Subsequence Vector를 만들었을 때 그것들을 Codebook과 비교해서 어떤 Code들이 얼마나 선택되었는지를 가지고 최종 Prediction에 이용한다.(Code Histogram 방식)
    - 한 샘플을 TS Encoder → Tokenizer → Attribute Decoder를 통해 여러 개의 Token마다 각각 Attributes들을 갖게 하고, 그 중 Subsequence를 의미하는 z만 가장 비슷한 Code로 바꾼 Token들을 이용하여 Prediction을 수행함 (Tokens 방식) - 검증 필요(z를 Codebook으로 안바꾸고 바로 사용한다는 것 같기도 해서 검증좀.)
    - 물론 실험에서 본 것처럼, 한 Sample을 여러 부분 구간으로 나누고, 각 Subsequence를 Codebook의 Code로(Shape Decoder 통과시키고) 대체해서, 이 Sample이 어떤 Code들의 구성으로 만들어졌는지를 알 수도 있지만, 그렇다고 해서 각 Code가 어떤 행동을 '의미'하는지는 모름.

- 내가 쓸 논문의 Motivation이 되어야 할 것.
    - 한 Sample을 Codebook의 Code들의 조합으로 구성할 수는 있지만, 각 Code가 어떤 행동을 '의미'하는지를 모른다면, 그것에 대한 Semantic 정보를 주어서 각 Code가 어떤 행동을 의미하는지까지 알 수 있으면 좋겠다.
    - 즉, Text Guidance를 이용해서 각 Codebook의 Code가 어떤 행동을 의미하는지를 알 수 있다면 우리가 원하는 설명 가능한 AI에도 더 가까워 질 것임.
    - Codebook에 포함된 Sequence가 '다리를 올린다', '주먹을 쥔다' 와 같은 그룹화된 파형이 되도록 하면 좋겠다.
    - 이 때, 어떻게 Text Guidance를 통해서 Codebook에 Semantic적인 의미를 줄 수 있을지를 LSA논문을 참고해서 할 수 있을 것 같다.

### 생각해 보아야 할 것.
- LSA에서는 어떻게 Semantic이라는 정보를 넘겨줄 수 있었나?
    - Signal embedding과 Text embedding을 CrossAttention과 Contrastive Loss를 통해서 비슷한 의미 공간(Semantic Space)에 정렬시켰고, 각각이 더 높은 품질을 가질 수 있도록 Prediction Loss를 같이 수행함.

- LSA처럼 Sample마다 Label을 이용해서 Text Guidance를 준다면 어떻게 할 지.
    - '같은 Class에 속하는 Sample마다 같은 Guidance 정보가 된다'가 밑의 방식과 가장 차별화될 텐데, 그에 따른 장단점이 어떻게 될 것인가?
        - 장점 : 각 Text를 만드는 데 걸리는 시간이 줄어듦. → 근데, 어차피 GAF이미지처럼 각 샘플마다 미리 만들어서 npy파일로 만들고 나중에 Loader로 부를 텐데 큰 상관 없을듯.
        - 단점 : Codebook에 Semantic한 정보를 넣어준다는 관점에서는 모르겠지만, 각 샘플마다 항상 존재하는 Scale의 차이가 있는데, 이를 Text에서는 전달해주지 못하게 됨. (어른 vs 아이)

- 전에 이야기 한 것 처럼 각 Sig → Text(Sequence) 를 직접 수행해주는 LLM을 이용해서 Text Guidance를 준다.
    - 방식 
        1. Time-series를 입력으로 받고, Vector Sequence(Text)를 뱉어주는 LLM에 각 Sample을 넣음.
        2. 각 Sample마다 서로 모두 다르게 Text가 형성됨.
        3. 그 Text들을 Encoder → Tokenizer를 거쳐 Time-series와 같은 형태의 Token으로 만듦. (1, 48, 512)로 동일하게 (sample, token_num, token_len)

- Text Guidance를 준다고 함은 Cross Attention을 통한 정보 융합을 의미하는데, 어떻게 할 것인가?
    1. Time-series를 Query, Text를 Key/Value로 하여 합침.
    2. LSA와 같이 두 모달리티를 각각 Query, Key/Value로 하여 두 번 수행한다.
        - 이렇게 했을 때 LSA에서는 이 얻어진 정보를 바로 Classification에 사용해서 Loss를 얻고, 둘을 비교하는 Contrastive Loss로 학습이 되었는데, VQShape에서는 과정일 뿐이다.
        - 이 두 결과물을 평균내고, Contrastive Loss로 비슷해지게만 한다고 하면, 아마도 비슷해지게만 할 뿐 좋은 정보가 되진 않을 것임.
        - 그 좋은 정보를 만들어내기 위함이 LSA에서는 Prediction Loss로 이루어졌는데, 여기서는 어떻게 해야하지? (Reconstruction Loss를 어떻게 이용할 수 있을지)
        - 이렇게도 될까? : 그냥 Time-series를 Query로한 것만 사용해서 Codebook 업데이트 및 뒤의 과정을 수행하고, Text를 Query로 한 것과 Contrastive Loss만 둬서, 위의 Time-series가 Main으로 합쳐진 정보가 최대한 Text 정보를 더 많이 포함하게 해주는 느낌. → 근데 이렇게 한다고 해서 Codebook Code에 Semantic을 잘 전달할 수 있을지는 모르겠음.


- Text를 만드는 방법이 위의 2 가지로 나뉘어지지만, 결국 그 이후에는 똑같아도 됨 어떻게 할 수 있을까?
    - Text를 어떻게 만들 것인가만 다르고, 결국 만들어서 Time-series와 같은 수의 Token으로 구성하는 것 부터는 똑같다.
    - 그렇다면, Token 단계에서 두 Modality의 정보를 합치고, Attribute Decoder부터는 똑같이 해도 내가 원하는 바를 다 이룰 수 있나? (아닐 거라고 봄.)
    - 그리고, Token단계에서 두 Modality 정보를 합친다면, Attribute Decoder를 통해서 얻을 수 있는 각 Attributes들에 뭔가가 추가되거나 할 순 없을까? 
        - 지금은 z, mu, sigma, t, l을 생성하는데, 결국 이거는 뒤의 Loss계산 등의 Frame을 따라가기 위함이고, 우리는 여기에 Text를 추가했으니 어떤 추가적인 Attribute를 더 얻어서 Text만을 위해 사용하거나, 전체 Framework에서 같이 사용해도 됨.
        - 예를 들면, Signal과 Text의 정보를 합친 Token을 통해서 이 subshape z가 어떤 행위 그룹에 들어갈 것인지를 action이라는 새로운 Attribute로 예측하게 하고, 이것을 이용해서 Text Token을 위한 새로운 Loss를 챙길 수도 있을지?

- 결국 최종적인 Test는 이렇게 되었으면 좋겠다.
    1. Time-series만을 입력으로 받음.
    2. 하던대로 수행 후, Tokens들을 만들고, 각 Tokens의 z를 Codebook의 z'으로 변환한 후, 바뀐 Token을 이용해서 Classifier같은 Downstream Task에 이용한다.(Tokens 방식)
    3. 원하는 결과 : Token에는 순서 정보가 있고, 각 Token의 Subsequence인 z'(code)를 Shape Decoder를 통해 Code들을 이어붙여서 원본 형태와 가깝게 만들어지고, 각각의 Code가 특정 서브 행위를 '의미'하기 때문에 이 행동 이라고 Classification을 할 수 있게 하고 싶음. 
        - 즉, 각 Code들을 이어 붙였을 때 각 Code에도 의미가 있어서, (다리를 듦 → 더 높게 듦 → 딛음 → 반대쪽 다리를 듦 → 더 높게 듦 → 딛음)이런 순서가 왔을 때 '계단을 오른다'라는 것을 예측하고, 사람이 그것을 이해할 수 있게 하고싶다는것임.


### 어떻게 할 수 있을까?
- 각 Token별로 어떤 Sub-Action인지까지도 예측하게 한다.
    1. Query : Signal, Key/Value : Text로 Cross Attention을 수행한다. 
        - Time-series에는 발견할 수 없는 새로운 정보를 Text로 전달받기 위함.
    2. LLM을 이용해서 각 Label에 대한 Sub-Action을 10개 단어로 만들고, 이 단어들을 CLIP과 같이 단어 간의 유사성을 반영할 수 있는 Vector로 만든다.
        - 각 단어에서 leg up과 leg down은 많이 차이가 나고, leg up과 knee up과 같은 것은 유사함을 유지할 수 있도록 함.
    3. 의미가 중복되는 단어는 Vector간의 Similarity 차이를 계산하여 일정 임계값 이하인 것은 하나만 남긴다. (30개정도 남았다고 가정)
        - '다리를 올린다'와 '다리를 든다'와 같은 것은 중복으로 처리하여 각 Sub-Action끼리 일정 거리를 유지하게 함.
    4. 이 남은 30개의 Sub-Action Vector는 Dataset 내의 대부분의 Action을 설명할 수 있을 거라고 가정한다.
        - 중복을 제외한 30개의 Action은 Semantic Space상에서 일정 거리 이상 떨어져 있을 것이므로, Cluster의 역할을 하게 함.
    5. Signal과 Text의 정보가 섞인 Token을 이용해서 Action이라는 Attributes도 예측하게 한다.
        - 위의 Sub-Action Vectors와 같은 Shape의 값을 예측하게 하고, 가장 가까운 Sub-Action Vector와는 더 가까워지게, 다른 것은 더 멀어지게 함.
        - 하지만 이 때 Token을 통해 예측한 Action이 가장 가까운 Sub-Action과 일치하지는 않을 것이다. (그 SubSequence가 실제로는 '팔을 든다' 이지만 Vector상에서는 '다리를 내린다'와 가까울 수도 있다. CLIP의 Image-Text처럼 의미가 통하지 않을 테니까)
    6. Action에 대한 것을 반복적으로 학습함.
        - Action을 예측하고, 가장 가까운 Sub-Action Vector와 가까워지다보면, 비슷한 SubSequence에 대해서는 같은 Sub-Action으로 그룹지어지게 됨.
        - 특정 Token으로 얻어진 SubSequence가 '팔을 든다'를 의미한다고 했을 때 이 Subsequence들은 비슷한 Action Vector를 만들어 낼 것이다. 실제로는 다른 Sub-Action Vector와 가까워질 지라도.
    7. Codebook에 미치는 영향
        - Token이 예측한 z(SubSequence)는 결국 코드북 내의 Code(z')과 비슷하고, 이 둘은 모두 Subsequence의 Shape을 의미하기 때문에 '이 Shape이 특정 Action 그룹에 속해있구나 그럼 이 두 Code가 같은 Sub-Action을 의미하겠네'라는 정보를 갖게 되고, 이것이 성능 향상에 도움이 될 수 있음.
    8. Testing 때
        - Attribute Decoder의 Action 파트가 Signal+Text로 학습되었기 때문에 Time-series만 줘도 아마 어떤 그룹에 속하는지 알듯. (근데 알아서 뭐함? 그게 실제로 무슨 의미인지를 모르는데.)
    - 문제점
        - Motivation과 맞지 않는다 : 각 Code가 실제로 어떤 Sub-Action을 의미하는지 알고 싶었는데 그건 안됨.
        - 학습 후에 이 Action이 무엇인지 Post-hoc느낌으로 안다고 해도, 이걸 원했던 게 아님. (이건 애초에 VQShape도 될듯)

- 그러면 어떻게 Sub-Action이 실제 Action과 의미상 일치하게 할 수 있을까?
    - Text Guidance를 Label을 이용한 Word의 Vector까지 Sub-Action Concept과 같은거로 같이 넣어준다면?
    - Time-series를 Vector Sequence의 형태로 만든 것과 Word Label을 Vector로 만든 것을 합쳐서 + 그 사람에 대한 정보(PAMAP2의 Subject 정보같은거도)까지 넣어준 뒤에 Time-series와 Cross Attention으로 의미를 정렬해준다.
    - 예를 들어 Time-series Encoder 후 나온 Signal Embedding, Text도 어느정도 정리한 Text Embedding 에서 Cross Attention으로 하나의 정보로 합친 후에 Tokenizer를 통해서 Token으로 만든다. → 그 후, Attribute Decoder를 통해서 z, mu, sigma, t, l을 이용해서 Codebook의 Sub Sequence형태를 일치하게 하고, Action을 통해서 그 값이 Sub-Action Concept과 비슷해지도록 Loss를 설계함.
    - 이렇게 하면, 같은 LLM으로 얻어진 Text Embedding(Sub-Action에 대한)을 가지고 정렬이 되기 때문에, 그 다음에 Text Encoder와 Signal Encoder에 대해서 Align을 해주면, 최종적으로는 Signal Encoder만을 통해서 얻어진 Token으로 Sub-Action을 유추할 수 있게 되고, 그 Sub-Action과 매치되는 Codebook Code가 그 Sub-Action을 의미하게 됨.