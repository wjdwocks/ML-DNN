### 25.10.13
- Reconstruction Loss : 아키텍처에 포함되어있는 대부분의 Encoder, Decoder의 성능 향상을 위해 하는 Loss
- Codebook 내의 Code가 뭔지? 
    - CCT에서는 각 Concept이 무엇을 의미하는지 이미지 내에서 하이라이트가 가능함
        - "이게 뭐다" 라는게 가능함.
    - 하지만 VQShape에서는 그 Code를 Time-series로 변환했을 때 그 time-series가 뭘 의미하는지를 모르기 때문에 그게 안됨. 
        - "이게 뭔지 모름"


### 이 논문(vqshape)을 통해 가져갈 Motivation
- Codebook에 포함된 Sequence가 '다리를 올린다', '주먹을 쥔다' 와 같은 그룹화된 파형이 되도록 하면 좋겠다.
- Code 하나하나에 어떻게 의미를 만들까?
    - Text로 표현하게 한다?
    - LSA 논문을 다시 처음부터 읽고, 어떻게 Semantic을 넣어줄 수 있을지 알아봐야 함.
    - Code에 Text 표현을 넣으면 이 각 Code(subsequence)가 어떤 의미(semantic)를 가지는지를 알 수 있다.
- Text라는 정보가 포함될 때 z, mu, sigma, t, l이 어떻게 바뀔 수 있을까?
- Text Guidance를 가져왔을 때(Label을 이용한 Sentence 등) 이 Text Guidance에는 시간 순서에 대한 정보가 없는데, 우리는 Time-series의 SubSequence로 Code를 얻을텐데, 이것에는 시간 정보가 있고, 각 Subsequence마다 의미를 부여해주고 싶은건데, 그것을 어떻게 할 수 있을까?
    - 어떤 논문에서는 이렇게 했다? (에반디)
    - 대충 Concept적으로 이렇게 해볼 수 있을지 않을까? 를 알아와라..
    - 잘 설명할 수 있도록 잘 정리해서 다음 미팅때 쭉 말해보자.

### 코드 작성에 관한 것.
- VQShape쪽을 Base로, GLW를 가져와서 붙여보는 것으로.
- 아직 때가 아님.

## 이번 주 할 일.
1. LSA 논문을 다시 읽고, Codebook Code에 어떻게 Text를 통해서 각 Code가 Semantic을 지니게 할 수 있을지 생각해보자.
2. 그런데 전달되는 Text가 아레의 두 개가 될 수 있음. (각 방식의 장단점을 생각해보면 좋을듯.)
    - Time-series → LLM → Tokenizer
    - Label y → LLM → Tokenizer
3. 그렇게 Text Guidance를 넘겨줬을 때 각 Attributes들이 z, mu, sigma, t, l이 있을텐데, 얘네가 뭘 의미하게 하면 좋을지나/ 다르게 바꾸는 것도 생각해보면 좋을듯.
4. 부족한 코드 정보에 대해서 더 공부해보아야 함. (중요!)
5. 3Teacher 논문 작성
    - Tolerance Noise
    - Hyper parameter
    - Parametric Plot