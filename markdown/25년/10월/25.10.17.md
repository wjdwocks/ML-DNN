### 25.10.13
- Reconstruction Loss : 아키텍처에 포함되어있는 대부분의 Encoder, Decoder의 성능 향상을 위해 하는 Loss
- Codebook 내의 Code가 뭔지? 
    - CCT에서는 각 Concept이 무엇을 의미하는지 이미지 내에서 하이라이트가 가능함
        - "이게 뭐다" 라는게 가능함.
    - 하지만 VQShape에서는 그 Code를 Time-series로 변환했을 때 그 time-series가 뭘 의미하는지를 모르기 때문에 그게 안됨. 
        - "이게 뭔지 모름"


### 이 논문(vqshape)을 통해 가져갈 Motivation
- Codebook에 포함된 Sequence가 '다리를 올린다', '주먹을 쥔다' 와 같은 그룹화된 파형이 되도록 하면 좋겠다.
- Code 하나하나에 어떻게 의미를 만들까?
    - Text로 표현하게 한다?
    - LSA 논문을 다시 처음부터 읽고, 어떻게 Semantic을 넣어줄 수 있을지 알아봐야 함.
    - Code에 Text 표현을 넣으면 이 각 Code(subsequence)가 어떤 의미(semantic)를 가지는지를 알 수 있다.
- Text라는 정보가 포함될 때 z, mu, sigma, t, l이 어떻게 바뀔 수 있을까?
- Text Guidance를 가져왔을 때(Label을 이용한 Sentence 등) 이 Text Guidance에는 시간 순서에 대한 정보가 없는데, 우리는 Time-series의 SubSequence로 Code를 얻을텐데, 이것에는 시간 정보가 있고, 각 Subsequence마다 의미를 부여해주고 싶은건데, 그것을 어떻게 할 수 있을까?
    - 어떤 논문에서는 이렇게 했다? (에반디)
    - 대충 Concept적으로 이렇게 해볼 수 있을지 않을까? 를 알아와라..
    - 잘 설명할 수 있도록 잘 정리해서 다음 미팅때 쭉 말해보자.

### 코드 작성에 관한 것.
- VQShape쪽을 Base로, GLW를 가져와서 붙여보는 것으로.
- 아직 때가 아님.

## 이번 주 할 일.
1. LSA 논문을 다시 읽고, Codebook Code에 어떻게 Text를 통해서 각 Code가 Semantic을 지니게 할 수 있을지 생각해보자. (내일)
2. 그런데 전달되는 Text가 아레의 두 개가 될 수 있음. (각 방식의 장단점을 생각해보면 좋을듯.) 
    - Time-series → LLM → Tokenizer
    - Label y → LLM → Tokenizer
3. 그렇게 Text Guidance를 넘겨줬을 때 각 Attributes들이 z, mu, sigma, t, l이 있을텐데, 얘네가 뭘 의미하게 하면 좋을지나/ 다르게 바꾸는 것도 생각해보면 좋을듯.


4. 부족한 코드 정보에 대해서 더 공부해보아야 함. (중요!)

5. 3Teacher 논문 작성
    - Tolerance Noise
    - Hyper parameter
    - Parametric Plot


## VQShape에 대한 생각.
### 지금까지 알고 있는 정보 정리.
- LSA의 방식
    - [Training Time]
    - 각 Sample마다의 Label을 이용하여 Text Guidance를 제공한다.
    - Label → LLM을 이용하여 얻은 Text (Keyward, Sentence, Paraphrase) → Text Encoder → Text Tokenizer
    - 이후, Time-series와 같은 Shape으로 만들기 위한 Projection Layer를 통과한 후, 각각을 Query, key/value로 하여 Cross Attention을 통해 정보 혼합 후 Cross Attention 전의 Main 모달리티 벡터(Query로 썼던)와 더하여 그것을 이용하여 최종 Prediction을 수행.(Sig를 메인, Text를 메인 으로 각각 1번씩 총 2번) 
    - [Texting Time]
    - 테스트 시에는 Time-series만을 입력으로 받음 → Signal Encoder를 통해서 signal Embedding으로 만듦 → 이것만 받아서 Classifier를 통해 Prediction 수행.
    - 이것이 되는 이유는 학습 때 각각의 모달리티 정보를 메인으로 해서, 정보를 융합했고, 그 융합된 정보로 Classifier가 학습이 되었음.
    - 또한, 융합된 정보를 가지고 Prediction을 한 것으로 각 Signal Encoder, Text Encoder까지도 Loss가 흘러갔기 때문에 이 Encoder들도 같이 학습이 됨.
    - 마지막으로, 각 모달리티를 메인으로 융합된 정보끼리도 Contrastive Loss를 통해 비슷해지도록 했기 때문에 결국 Signal Encoder나, Text Encoder는 서로 Embedding을 생성할 때 최대한 두 모달리티가 잘 융합된 정보처럼 생성하도록 자동으로 학습이 되기 때문에 이 Testing이 가능하게 된다. (물론 Text Encoder는 아마 Freeze)

- VQShape의 방식
    - Time-series만 사용함.
    - Time-series 정보만을 이용해서 Codebook을 만들었고, 한 Sample을 TS Encoder → Tokenizer → Attribute Decoder를 통해서 여러 개의 Subsequence Vector를 만들었을 때 그것들을 Codebook과 비교해서 어떤 Code들이 얼마나 선택되었는지를 가지고 최종 Prediction에 이용한다.(Code Histogram 방식)
    - 물론 실험에서 본 것처럼, 한 Sample을 여러 부분 구간으로 나누고, 각 Subsequence를 Codebook의 Code로(Shape Decoder 통과시키고) 대체해서, 이 Sample이 어떤 Code들의 구성으로 만들어졌는지를 알 수도 있지만, 그렇다고 해서 각 Code가 어떤 행동을 '의미'하는지는 모름.

- 내가 쓸 논문의 Motivation이 되어야 할 것.
    - 한 Sample을 Codebook의 Code들의 조합으로 구성할 수는 있지만, 각 Code가 어떤 행동을 '의미'하는지를 모른다면, 그것에 대한 Semantic 정보를 주어서 각 Code가 어떤 행동을 의미하는지까지 알 수 있으면 좋겠다.
    - 즉, Text Guidance를 이용해서 각 Codebook의 Code가 어떤 행동을 의미하는지를 알 수 있다면 우리가 원하는 설명 가능한 AI에도 더 가까워 질 것임.
    - Codebook에 포함된 Sequence가 '다리를 올린다', '주먹을 쥔다' 와 같은 그룹화된 파형이 되도록 하면 좋겠다.
    - 이 때, 어떻게 Text Guidance를 통해서 Codebook에 Semantic적인 의미를 줄 수 있을지를 LSA논문을 참고해서 할 수 있을 것 같다.

### 생각해 보아야 할 것.
- LSA에서는 어떻게 Semantic이라는 정보를 넘겨줄 수 있었나?

- LSA처럼 Sample마다 Label을 이용해서 Text Guidance를 준다면 어떻게 할 지.
    - '같은 Class에 속하는 Sample마다 같은 Guidance 정보가 된다'가 밑의 방식과 가장 차별화될 텐데, 그에 따른 장단점이 어떻게 될 것인가

- 전에 이야기 한 것 처럼 각 Sig → Text(Sequence) 를 직접 수행해주는 LLM을 이용해서 Text Guidance를 준다.
    - 방식 
        1. Time-series를 입력으로 받고, Vector Sequence(Text)를 뱉어주는 LLM에 각 Sample을 넣음.
        2. 각 Sample마다 서로 모두 다르게 Text가 형성됨.
        3. 그 Text들을 Encoder → Tokenizer를 거쳐 Time-series와 같은 형태의 Token으로 만듦. (1, 48, 512)로 동일하게 (sample, token_num, token_len)

- Text를 만드는 방법이 위의 2 가지로 나뉘어지지만, 결국 그 이후에는 똑같아도 됨 어떻게 할 수 있을까?
    - Text를 어떻게 만들 것인가만 다르고, 결국 만들어서 Time-series와 같은 수의 Token으로 구성하는 것 부터는 똑같다.
    - 그렇다면, Token 단계에서 두 Modality의 정보를 합치고, Attribute Decoder부터는 똑같이 해도 내가 원하는 바를 다 이룰 수 있나? (아닐 거라고 봄.)
    - 그리고, Token단계에서 두 Modality 정보를 합친다면, Attribute Decoder를 통해서 얻을 수 있는 각 Attributes들에 뭔가가 추가되거나 할 순 없을까? 
        - 지금은 z, mu, sigma, t, l을 생성하는데, 결국 이거는 뒤의 Loss계산 등의 Frame을 따라가기 위함이고, 우리는 여기에 Text를 추가했으니 어떤 추가적인 Attribute를 더 얻어서 Text만을 위해 사용하거나, 전체 Framework에서 같이 사용해도 됨.
        - 예를 들면, Signal과 Text의 정보를 합친 Token을 통해서 이 subshape z가 어떤 행위 그룹에 들어갈 것인지를 action이라는 새로운 Attribute로 예측하게 하고, 이것을 이용해서 Text Token을 위한 새로운 Loss를 챙길 수도 있을지?