1. codebook → z, mu, sigma, l, t : Attribute Decoder를 통해서 나눠주는 역할만임.
2. hugging face text embedding을 사용해서 각자 decoder를 둬서 뒤에서 합쳐도 됨.
3. LLM을 통해서 time-series를 text로 출력할 수도 있음. text라는게 sequence vector로 표현할 수도 있음.
4. 각 text Tokenizer가 서로 다름 ( , 포함/비포함)
5. LSA paper? 에서도 이렇게 했는데, CrossAttention이 이것 또한 해소가 된다.
6. 왜 cross modality가 잘 되냐? 
    - 둘을 사용해서 codebook을 만들어도, 하나만 사용하더라도 잘 동작할것임.
7. parallel하게 signal을 위한 loss쪽 하나, text쪽 loss하나로 둬서 
    - 같은 Token으로 출발하는데, 말이 안됨.
    - 그러려면, Text embedding 따로(hugging face) → 샘플마다 PI 만들어놓듯이, Time-series Encoder 따로 두고 각자의 token을 통한 loss를 구현하고, 뒤에서 하나의 token으로 맞추는 느낌으로 함.
8. Text는 Tokenizer를 통해서 굳이 mu, sigma, t, l을 할 필요 없으니, z만 가진다고 생각하면 됨.
    - 이거로, 중앙에 Codebook을 업데이트하는 Operation을 포함하게 함.
    - Tokenizer Reverse를 통해서 다시 Time-series로 돌린 다음에, Recon Loss를 얻으면 된다.
9. Cross Attention을 통해서 다른 Modality 차이를 해소해줄 수 있다 + 같은 time-series로부터 출발했다는 것 이것에 집중해서 이번주 공부를 해야할듯.
10. 좋은 논문이 되기 위함이란? → 방법론까지는 거의 다 똑같음. 그렇지만 여기에 어떤 재미난 것을 더 추가할 수 있을 것인가? 어떻게 말을 표현할 수 있을 것인가?
11. toy model : GLW에 codebook 바꾸고, pi를 text로 바꾸는거. 
    - loss부분은?
    - 아마 Codebook 부분 바꾸는게 어려울듯.??
    - Problem : 어떻게 Time-series데이터를 interpretable 하게, subsequence로 분할할 수 있을까? → 이것을 time-series만으로는 할 수 없기 때문에, Text Modality를 가져오게 되었다.
→ 어떻게 Text를 넣을건데? : huggingface tokenizer를 통해서 같은 shape의 token으로 만듦.

GLW 논문, LLM Guided semantic augmentation, Frequency-Semantic Enhanced Variational Autoencoder for zeroshot skeleton based action ... 이런얘들 읽어봐야함.
   - Skeleton : Motivation이 되어야 할 논문. 
    - LLM을 어떻게 Text에 사용할지. → Prompt engineering : 어떻게 더 자세히 text를 잘 뽑아낼 수 있을까.




## 왜 Cross Attention을 사용했을 때 Modality 차이를 해소할 수 있을까?
### Cross Attention이 그래서 뭐길래
- 한 Modality를 Query, 다른 Modality를 Key-Value로 하여, 두 표현 간의 의미적 대응 관계(Alignment)를 학습함. (학습한다는게 중요)
- 예를 들어 Query : Time-series's latent vector, Key/Value : LLM(TS → Text)에서 생성된 Text embedding일 때, Cross Attention의 결과는 센서 신호의 어떤 부분이 어떤 Text Semantic(의미)에 대응하는지를 학습한 것임.
- 결론 : Cross Attention은 서로 다른 Modality의 정보를 같은 의미 표현으로 Alignment하기 위함.

### Cross Attention의 학습 효과
- 두 Modality가 반복적으로 Cross Attention을 한다는 것은?
    - Time-series Encoder의 Latent Representation이 Text의 의미 공간을 반영하도록 업데이트됨.
    - Text Embedding 역시 Sensor Pattern의 특징에 정렬되도록 Fine-tune됨. → 근데 나는 ts → txt의 LLM을 사용하면, freeze한 LLM을 사용할 텐데 어떤 영향을 받지?
- 결과적으로 학습이 끝난 후?
    - Time-series Encoder를 통해 얻어진 Sensor latner space 자체가 Text와 의미적으로 일치하도록 정규화 되게 됨.
    - 즉, 이 Encoder를 그대로 사용하면, Time-series만으로, Text와 Cross Attention을 하지 않아도, 이미 한 것 처럼의 효과를 주는것임.
- 결론1 : Cross Attention을 통해서 각 Modality 정보를 생산한 Encoder 자체도 서로 Alignment됨. → 그러니까 이걸 이용해서 Codebook을 만들어도 하나의 Modality만 사용해도 Prediction이 가능함.
- 결론2 : LLM을 이용한 Encoder라면 Freeze시킬 것이기 때문에 이 Encoder는 변하지 않음 → Time-series Encoder만 Text 공간에 적응하게 되고, Aligment 속도가 늦어지거나 정렬이 잘 안될 수도 있음. → Cross Attention을 위한 W_k, W_v가 충분히 학습되면 괜찮음.

### Codebook에 적용한다면?
- Time-series Encoder를 통해서 Embedding을 하나 생성. + Time-series → text로 변환해주는 LLM과, Tokenizer를 통해서 비슷한 의미의 Text Embedding을 얻음.
- 이 둘을 Cross Attention으로 합쳐서 Time-series와 Text의 정보가 혼합된 Vector를 얻는다.
    - 이 때, Time-series Embedding은 Attribute Decoder를 통과해야 하는데, 사실상 z, mu, sigma, t, l마다 같은 입력을 서로다른 MLP에 넣어주는거라서, z에 넣을 입력에만 Text와 Cross Attention된 것을 넣어준다.
    - 이렇게 정보가 합쳐진 z를 통해서 Codebook과 비교한다면, Codebook은 Text + Time-series의 정보를 받아들인 후 구성이 될 것이고, Cross Attention을 통해서 둘의 의미가 더 잘 맞춰지도록 Time-series Encoder자체가 학습이 될 것임.
- 그렇다면, 얻어진 Text Embedding(하나의 샘플마다 존재)를 Time-series와 동일하게 여러 개의 Token으로 나누어야 하는데, 이것을 Tokenizer를 통해서 할 수 있다.
    - 그래도 어떻게 Tokenizer를 통해서 이것을 하는 지도 알아봐야 함 + 어떤 Tokenizer를 사용할 것인지. (종류가 다양함.)
- 결론1 : 데이터셋에 잘 맞춰진 "좋은 Codebook"을 만드는 것이 이 학습의 목표이다
- 결론2 : z, mu, sigma, t, l 에서 z만이 Codebook내부의 code(shape)에 관여하고, 나머지는 특정 Loss를 얻기 위해 부가적으로 존재함. → z에만 Text정보를 추가해도 괜찮다고 판단 (물론 다 해보긴 할듯.)

### Text관련된 Loss는 어떻게 구성해야 할까?
- Time-series쪽은 굳이 Loss를 바꿀 필요가 없어보임.
- 하지만, Text쪽을 추가한다면, Text쪽에서 수행하는 Cross Attention이라던가, Codebook에 직접적으로 영향을 주는 Loss라던가 추가되는게 좋을 것 같음.
    - 여기에서, 어떤 Loss를 흐르게 해야 Text의 CrossAttention을 위한 Key/Value Linear Projection이 효과적으로 학습될 수 있을지 (Reconstruction Loss이 좋을 수도 있다는데 뭔지??)
    - Text쪽 Token Embedding을 이용해서 Codebook 구성에 직접적으로 영향을 미칠 수 있는 방법이 있을지도 생각해 보면 좋을듯.
- Text-TS Alignmnet (InfoNCE) Loss : 
- Consistency Loss (TS-only vs TS+Text) : 
- Text Reconstruction Loss : 


- 결론 : (아직.....)




## GLW코드와 VQShape 코드를 돌려보면서, 어떻게 위의 일을 할 수 있을 지 생각해봐야할듯.
### VQShape 코드 따라가보기. (Main함수부터 쭉)
- 

### 환경설정에서부터 막힘...

### Lightning이 뭘까?
- Pytorch 코드를 구조화하고, 훈련 절차를 자동화해주는 고수준의 프레임워크이다.
- 지금까지의 코드는 Pytorch에서 train()루프, optimizer.step(), loss.backward()를 직접 작성해서 동작시켰는데, Lightning은 그 반복적이고 당연한 코드 구조를 자동으로 관리해주어 코드를 깔끔하게 유지해주는 역할을 함.

### Pytorch Lightning을 사용하는 목적.
1. 코드 구조화 및 가독성 향상 
    - 모델, 데이터, 학습 루프, 평가 루프 등을 명확하게 분리한다.
2. 반복 작업 자동화
    - GPU관리, epoch루프, validation/test주기, checkpoint 저장 등을 자동으로 처리함.
3. 분산 학습(DP, DDP, DDP2) 지원
    - 코드 수정 없이 멀티 GPU, TPU, CPU 환경에서 실행이 가능함.
4. 로깅 및 콜백 통합
    - TensorBoard, WandB, CSVLogger 등과 자연스럽게 통합이 가능하다.
5. 연구와 실험의 재현성 확보
    - 하이퍼파라미터 훈련 설정을 일관되게 관리할 수 있음.

### LightningModule이란?
- Pytorch의 nn.Module을 확장한 클래스로, 단순한 모델 정의 뿐 아니라 "학습 절차 전체"를 정의하도록 설계함.
- 아레는 LightningModule을 상속했을 때 일반적으로 포함해야 하는 주요 메서드들임.
1. __init__(self, ...)
    - 역할 : 모델, 손실함수 초기화.
    - 손실함수(loss_fn)은 __init__()에서 정의하는 것이 정석이지만, Optimizer는 configure_optimizers() 여기서 정의해야 한다.
    - 아레처럼 Lightning이 순차적으로 진행한다고 생각하면 됨.
    ```python
    optimizer_dict = model.configure_optimizers()
    optimizer = optimizer_dict['optimizer']
    scheduler = optimizer_dict.get('lr_scheduler')
    for batch in train_loader:
        loss = model.training_step(batch)
        loss.backward()
        optimizer.step()
        scheduler.step()

    ```
2. forward(self, x)
    - 역할 : 모델에 넣어 forward pass를 진행한 결과를 준다.
    - 그런데, 굳이 설정 안해놓고, self.model(x)를 직접 사용하는 경우도 있는듯.
3. training_step(self, batch, batch_idx)
    - 역할 : 학습 단계에서 수행할 연산을 정의함.
    - 입력으로 batch, batch_idx를 받고, 계산된 loss를 출력한다.
    ```python
    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("train_loss", loss)
        return loss

    ```
4. validation_step(self, batch, batch_idx)
    - 역할 : 검증 단계의 연산을 정의함.
    - 여기도 똑같이 batch, batch_idx를 입력으로 받고, 이것을 이용하여 loss와 metric을 계산한 후, loss와 metric을 포함한 dictionary를 출력으로 내보낸다.
    ```python
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("val_loss", loss, prog_bar=True)
        return loss
    ```
5. test_step(self, batch, batch_idx)
    - 역할 : 테스트 데이터셋에서 평가할 때 실행되는 부분.
    - 당연히 validation_step과 동일하게 진행되지만, 가져오는 데이터 로더만 다름.
6. configure_optimizers(self)
    - 역할 : optimizer 및 Learning Rate Scheduler 정의.
    - Lightning이 내부적으로 optimizer.zero_grad(), backward(), step()을 자동으로 처리한다.
    ```python
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9) # 동적 lr을 위한 줄.
        return {"optimizer": optimizer, "lr_scheduler": scheduler}
    ```
7. 그 외의 것들(필수 아님)
    - on_train_epoch_start() : epoch단위에서, 학습이 시작하기 전에 실행할 method.
    - on_train_epoch_end() : epoch단위에서, 학습이 끝난 후 실행할 method.
    - on_validation_epoch_start() : epoch단위에서, validation이 시작하기 전에 실행할 method.
    - validation_epoch_end(outputs) : epoch단위에서 검증이 끝난 후 실행할 method 인데, 다른 것들과 다른 것은 outputs(validation_step의 반환값 list)을 인자로 받아서, 이것을 이용한 무언가를 할 수 있음.
    - on_validation_epoch_end() : validation_epoch_end(outputs)가 끝난 후 마지막으로 할 것을 함.

### Trainer란?
- Lightning Module에서 정의한 것들 : 모델, Optimizer, Loss, Learning Rate, Training Step, Validation Step 등의 '무엇을 할 지 == 모델의 내용'를 정의함.
- Trainer에서 정의해야 하는 것들 : '훈련을 어떻게 수행할지'를 정하는 역할. 훈련 환경, 장치, epoch수, 로깅 주기 등을 설정함.
    1. Epoch
    2. devices(GPU/CPU)
    3. acclerator(cpu/gpu/tpu)
    4. precision
    5. logger
    6. callbacks
    7. log_every_n_steps(로그 출력 주기)
    8. check_val_every_n_epoch(몇 epoch마다 val을 수행할지)
    9. gradient_clip_val(gradient clipping용)
    10. limit_train_batches(디버깅용 batch 수 제한)
    11. enabel_progress_bar(tqdm progress bar 출력 여부)
- 대충의 Lightning Module로 선언된 모델의 학습 절차
    - 아레처럼 LightningModule의 클래스를 Lightning.trainer객체의 fit 함수에 각 dataloader들과 함께 넣어주면, 다음과 같은 순서대로 학습을 진행함.
    ```python
    import lightning as L
    import torch
    from torch import nn
    # -----------------------------
    # 1️⃣ LightningModule 정의
    # -----------------------------
    class LitClassifier(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(784, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        )
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("val_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

    # -----------------------------
    # 2️⃣ Trainer 정의
    # -----------------------------
    trainer = L.Trainer(
    max_epochs=20,
    accelerator="gpu",
    devices=1,
    precision=16,
    logger=True,
    log_every_n_steps=10
    )

    # -----------------------------
    # 3️⃣ 학습 시작
    # -----------------------------
    model = LitClassifier()
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
    ```
    - <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.13/lightning_trainer.png" alt="results" width="700">

### LightningModule을 상속한 LitVQShape 클래스 분석
```python
class LitVQShape(L.LightningModule):
    def __init__(self, args):
      super().__init__()

      self.save_hyperparameters(args)
      try:
        self.model = VQShape(
            len_input=self.hparams.normalize_length,
            dim_embedding=self.hparams.dim_embedding,
            patch_size=self.hparams.patch_size,
            num_patch=self.hparams.num_patch,
            num_token=self.hparams.num_token,
            num_enc_head=self.hparams.num_transformer_enc_heads,
            num_enc_layer=self.hparams.num_transformer_enc_layers,
            num_tokenizer_head=self.hparams.num_tokenizer_heads,
            num_tokenizer_layer=self.hparams.num_tokenizer_layers,
            num_dec_head=self.hparams.num_transformer_dec_heads,
            num_dec_layer=self.hparams.num_transformer_dec_layers,
            len_s=self.hparams.len_s,
            s_smooth_factor=self.hparams.s_smooth_factor,
            num_code=self.hparams.num_code,
            dim_code=self.hparams.dim_code,
            codebook_type=self.hparams.codebook_type,
            lambda_commit=self.hparams.lambda_vq_commit,
            lambda_entropy=self.hparams.lambda_vq_entropy,
            entropy_gamma=self.hparams.entropy_gamma,
            mask_ratio=self.hparams.mask_ratio
        )
      except:
        warnings.warn("Incompatible configs. Trying the legacy version...")
        self.model = VQShape(
            len_input=self.hparams.normalize_length,
            dim_embedding=self.hparams.dim_embedding,
            patch_size=self.hparams.patch_size,
            num_patch=self.hparams.num_patch,
            num_token=self.hparams.num_token,
            num_enc_head=self.hparams.num_transformer_enc_heads,
            num_enc_layer=self.hparams.num_transformer_enc_layers,
            num_tokenizer_head=self.hparams.num_transformer_enc_heads,
            num_tokenizer_layer=self.hparams.num_transformer_enc_layers,
            num_dec_head=self.hparams.num_transformer_dec_heads,
            num_dec_layer=self.hparams.num_transformer_dec_layers,
            len_s=self.hparams.len_s,
            s_smooth_factor=self.hparams.s_smooth_factor,
            num_code=self.hparams.num_code,
            dim_code=self.hparams.dim_code,
            codebook_type='standard',
            lambda_commit=self.hparams.lambda_vq_commit,
            lambda_entropy=self.hparams.lambda_vq_entropy,
            entropy_gamma=self.hparams.entropy_gamma,
            mask_ratio=self.hparams.mask_ratio
        )

      self.validation_step_outputs = {}
      for i in range(1):
        self.validation_step_outputs[i] = {'z': [], 't': [], 'l': []}

    def training_step(self, batch, batch_idx):
      _, loss_dict = self.model(batch, mode='pretrain')

      loss = self.hparams.lambda_x * loss_dict['ts_loss'].mean() + \
        self.hparams.lambda_z * loss_dict['vq_loss'].mean() + \
        self.hparams.lambda_s * loss_dict['shape_loss'].mean() + \
        self.hparams.lambda_dist * loss_dict['dist_loss'].mean()
      
      loss_dict = {
        "TRAIN/total": loss, "TRAIN/x": loss_dict['ts_loss'].mean(), "TRAIN/z": loss_dict['vq_loss'].mean(), 
        "TRAIN/s": loss_dict['shape_loss'].mean(), 'TRAIN/dist': loss_dict['dist_loss'].mean()
      }
      self.log_dict(loss_dict)
      return loss

    def validation_step(self, batch, batch_idx, dataloader_idx=0):
      output_dict, loss_dict = self.model(batch, mode='evaluate')

      loss = self.hparams.lambda_x * loss_dict['ts_loss'].mean() +\
        self.hparams.lambda_z * loss_dict['vq_loss'].mean() + \
        self.hparams.lambda_s * loss_dict['shape_loss'].mean() + \
        self.hparams.lambda_dist * loss_dict['dist_loss'].mean()
      
      loss_dict = {
        f"VAL{dataloader_idx}/total": loss, 
        f"VAL{dataloader_idx}/x": loss_dict['ts_loss'].mean(), 
        f"VAL{dataloader_idx}/z": loss_dict['vq_loss'].mean(),
        f"VAL{dataloader_idx}/s": loss_dict['shape_loss'].mean(), 
        f'VAL{dataloader_idx}/dist': loss_dict['dist_loss'].mean()
      }
      self.log_dict(loss_dict, sync_dist=True)
      self.validation_step_outputs[dataloader_idx]['z'].append(output_dict['code_idx'])
      self.validation_step_outputs[dataloader_idx]['t'].append(output_dict['t_pred'])
      self.validation_step_outputs[dataloader_idx]['l'].append(output_dict['l_pred'])
      if batch_idx == 0 and self.global_rank == 0:
        fig, s_fig = visualize_shapes(output_dict)
        self.logger.experiment.log({
            f"VAL{dataloader_idx}/x_fig": wandb.Image(fig),
            f"VAL{dataloader_idx}/s_fig": wandb.Image(s_fig)
        }, self.global_step)
        plt.close(fig)
        plt.close(s_fig)

    def on_validation_epoch_end(self):
      for i in self.validation_step_outputs.keys():
        z_idx = torch.cat(self.validation_step_outputs[i]['z'], dim=0)
        t_hat = torch.cat(self.validation_step_outputs[i]['t'], dim=0)
        l_hat = torch.cat(self.validation_step_outputs[i]['l'], dim=0)
        fig = plot_code_heatmap(z_idx, self.hparams.num_code, title=f"{self.global_step}")
        self.logger.experiment.log({
            f"VAL{i}/z_dist": wandb.Image(fig),
            f"VAL{i}/t_dist": wandb.Histogram(t_hat.float().cpu().numpy()),
            f"VAL{i}/l_dist": wandb.Histogram(l_hat.float().cpu().numpy())
        }, self.global_step)
        for key in self.validation_step_outputs[i].keys():
            self.validation_step_outputs[i][key].clear()
        plt.close(fig)

    def configure_optimizers(self):
      optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)
      stepping_batches = self.trainer.estimated_stepping_batches - self.hparams.warmup_step

      # Cosine annealing with linear warmup learning rate schedule
      scheduler = torch.optim.lr_scheduler.SequentialLR(
        optimizer, 
        schedulers=[torch.optim.lr_scheduler.LinearLR(optimizer, 0.0001, 1, self.hparams.warmup_step), 
                torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=stepping_batches, eta_min=1e-5)], 
        milestones=[self.hparams.warmup_step]
      )
      return {
        "optimizer": optimizer,
        "lr_scheduler": {
            "scheduler": scheduler,
            "interval": "step",
        }
      }
```




## LSA 논문 읽어보기.
### Introduction
- 서론
    - Wearble Sensor Data는 다양한 real-world Setting에서 수집되고 있다.
    - 각자 환경마다, noise의 형태가 다르고, 이런걸 잘 처리하고 싶음.
- 문제점
    - Users는 다른 physical charateristics를 가진다. (몸무게, 팔길이, 같은 행동에 대한 패턴)
    - 실제 인간의 행동은 매우 복잡하고, 연속적이지만, 데이터셋을 구축할 때에는 한 동작을 하나의 고정된 카테고리로 붙여버린다. → 행동 중간의 전이 상태(걷기 → 뛰기), 행동의 변형 (느리게 걷기, 절뚝이며 걷기) 같은 미묘한 차이를 무시하게 됨.
    - 데이터 불균형 문제가 존재함. → 자주 나오는 행동(걷기, 앉기)는 잘 맞추지만, 드문 행동(점프, 설거지하기)는 무시하게 되는 경우
    - 데이터 Distributional shift(분포 차이) 문제 → 데이터 수집 환경, 센서 위치, 대상 집단(population)의 차이 때문에 분포가 바뀐다.
        - 스마트워치를 왼손에 차기 vs 오른손에 찬 경우.
        - 어린이 vs 청년 vs 노인의 걷기 패턴의 차이가 존재함.
- 기존의 해결 방법 및 한계점
    - Data Augmentation : 전형적으로, Gaussian noise나, temporal shifting, temporal misalignment 등을 통해 distortions(방해)에 강인한 모델을 만들고자 함.
    - 이 Data Augmentation 방법은, 입력 자체의 왜곡, 불일치에 대한 강인성은 키워주지만, 활동 고유의 의미적 구조(peak, periodic patterns)를 보존, 강조하지 못해서 서로 비슷한 파형을 가진 활동을 구분하는 데에 한계가 있다.
- LLM을 사용해야 하는 이유
    - 단순한 raw signal perturbation만으로는 의미 있는 행동 단서를 잘 반영하지 못하고, Semantics(의미 있는 구조)를 활용하는 증강 기법이 필요함.
    - 그런 Semantic prior를 줄 수 있는 도구가 바로 LLM임. → LLM은 자연어로 행동을 묘사할 수 있고, prompt를 통해 얻어진 Text는 고수준의 의미 지식(high-level semantic guidance)이므로, 모델 학습에 추가적인 감독 신호를 제공해줄 수 있다.
- 기존 LLM을 이용한 연구들
    - End to end predictor 방식 : Time-series를 Text형태로 바꿔 LLM으로 직접 분류 → 모델의 크기가 커져서 wearable 환경에 부적합.
    - Language 활용 방식 : 활동 추론, 대화형 reasoning, 멀티모달 사전학습 등에 사용 → 이 또한 inference 시에 추가 입력이나, 무거운 생성 모델을 요구함.
    - 근본적인 한계 : 인간의 행동은 Keyward의 수준부터, Sentence의 수준까지 다양하게 표현이 가능한데, 어떤 것이 더 좋은건지 모르겠음.
- 이 논문의 주요 포인트
    1. LLM의 high-level semantic knowledge가 Inference complexity를 높이지 않고 모델의 generalization 과 robustness를 향상시킬 수 있나?
    2. 어떤 형태의 linguistic supervision이 가장 효과적인지, 이 선택이 HAR 성능에 어떤 영향을 미치는지.
- 이 주요 포인트(question)를 해소하기 위해서 어떻게 할 것인지.
    - LLM을 end-to-end 예측기(입력 → 출력까지 다하는)로 사용하지 않는다.
    - LLM을 Semantic augmentation의 제공자로서 역할에 집중하게 함.
    - 즉, LLM이 생성하는 행동에 대한 Text 설명을 훈련 단계에서 주입해줌으로써, Sensor data만으로는 담기 힘든 맥락, 행동, 의미를 보강하고, HAR 모델의 잠재 표현(latent feature space)를 의미적으로 정규화(semantic regularizer)한다.
        - LLM은 Inference 단계에서는 사용하지 않음. (추론 시에는 Time-series만 받는 Unimodal Model로 동작) → wearable device에 적합.
    - 제안 방법 : LLM이 생성한 Text Prompt를 활용하여 HAR 모델의 Latent Space를 의미적으로 정규화하여 input perturbation만 하던 기존 방식에 Semantic 수준의 증강을 결합함.

### Proposed Method
- LLM이 Text Embedding을 어떻게 만드는가?
    - 각 Sample마다, Label을 가지고, LLM을 통해 그 Activity에 대한 (1) Keyword, (2) Sentence, (3) Negative Paraphrases, (4) Affirmative Paraphrases 등의 정보를 각각 추출하고, 이것들을 LLM Text Encoder를 통해서 Text Embedding을 만든다.
    - 전에 생각했던 것 처럼, 같은 Class Label인 Sample마다 같은 Text Embedding이 만들어 지는 것으로 보임 → ("LLM generates natural language prompts from class labels")
- 둘의 Text Embedding과, Signal Embedding을 공통의 Projection Layer를 통해서 Dimension을 매칭해준다.??? → 말이 안되는게 공통의 Projection Layer에 넣을거면 애초에 차원이 같아야 함.
    - 이 때 아마 Projection Layer는 공통의 네트워크를 사용하는 것으로 봐서, 공통된 의미 공간으로 만들어 주는 역할도 수행하는 것 같음.
    - 차원만 맞춰줄거면 개별로 Projection을 하던가 그냥 numpy연산으로 했을것임.
- Fusion Layer를 통한 정보 혼합
    - Fusion Layer는 두 번 나뉘어져서 수행되는 것으로 보인다.
    1. Z_s를 Query로 두고, Z_t를 Key/Value로 두는 것.
    2. Z_t를 Query로 두고, Z_s를 Key/Value로 두는 것.
    - 어떤 정보를 Main으로 두고, 다른 정보를 Sub로 두는지에 따라서 얻을 수 있는 장점이 다르기 때문에 각각을 수행하여 attn(Z)를 만드는 것으로 생각됨.
    - 마지막으로, 이렇게 정보가 합쳐진 attn(Z)와 Query로 사용한 Embedding Z를 Residual Connection처럼 합친 후, Proj_out(아마 Classifier?)를 통과시켜 Main Modality에 따른 확률 분포를 얻는다. → Fused(Z)
- Z_s fused를 이용한 Loss : Signal을 메인 Modality로 했을 때의 Label값과의 Cross Entropy Loss
- Z_t fused를 이용한 Loss : Text를 메인 Modality로 했을 때의 Label값과의 Cross Entropy Loss
    - 이 두 Cross Entropy에서는 Class Imbalance 문제를 해결해보기 위해서 label-dependent weight w_yi를 이용해서 클래스의 frequency를 어떻게 처리했다고 함. 
- Supervised Contrastive Loss : 의미적으로 두 모달리티가 더 유사해지도록 하는 Loss.
    - Signal과 Text가 서로를 Query-Anchor로 삼아, Signal을 기준(Anchor)으로 했을 때 같은 Class의 Text들을 끌어당기고,
    - Text를 기준(Anchor)으로 했을 때, 같은 class의 signal들을 끌어당기게 함. → 끌어당긴다는게, 각 샘플들의 Embedding Vector가 비슷해지게 한다는 의미임.

### 그외
- 해야 할 남은것
    - 논문 한 파트 작성 완 (10분컷내버리기) ㅇ 
    - 영어 강의 대충 듣고, 과제 (아직 안올라옴 왜지?) ㅇ 
    - 취업뭐시기 과제
        1. (지원 회사)지원한 동기와 입사 후 포부에 대해 작성하여 주십시오. (700자)
        2. 특정 영역의 전문성을 키우기 위해 꾸준히 노력한 경험에 대해 서술해 주십시오 (700자)
        3. 본인의 성장과정을 간략히 기술하되 현재의 자신에게 가장 큰 영향을 끼친 사건, 인물 등을 포함하여 기술하시기  바랍니다. (700자) 
    - VQShape 돌려보기.
    - 왜 Cross Attention을 사용했을 때 Modality 차이를 해소할 수 있을까? ㅇ
        - 즉, 왜 두 Modality를 활용해서 Cross Attention으로 하나의 정보로 합치고, 그것을 이용해서 Codebook을 업데이트 했을 때, Prediction 과정에서는 하나의 Modality만으로도 둘을 모두 이용한 급의 성능을 달성할 수 있나?
        - 내 생각으로는, 두 정보를 CA해서 어어진 Codebook인데, 하나의 Modality로만 비슷한 Codebook Code를 찾는다면, 당연히 이상해질거라고 생각했는데, 아닐 수 있는 이유.
        - LSA Papaer를 읽어보면, LLM을 이용한 Text와 Time-series를 CrossAttention을 이용해서 정보를 합쳤는데, Prediction에서는 Time-series만 사용해서 성능이 높아진 사례가 있음.
    - 위에서 이야기한 방법론을 그림으로 그려보자. ㄴ
    - LSA논문의 방법론이 우리가 하려는 것의 어떤 것과 그렇게 유사한건지 알아보기.