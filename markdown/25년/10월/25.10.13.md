1. codebook → z, mu, sigma, l, t : Attribute Decoder를 통해서 나눠주는 역할만임.
2. hugging face text embedding을 사용해서 각자 decoder를 둬서 뒤에서 합쳐도 됨.
3. LLM을 통해서 time-series를 text로 출력할 수도 있음. text라는게 sequence vector로 표현할 수도 있음.
4. 각 text Tokenizer가 서로 다름 ( , 포함/비포함)
5. LSA paper? 에서도 이렇게 했는데, CrossAttention이 이것 또한 해소가 된다.
6. 왜 cross modality가 잘 되냐? 
      - 둘을 사용해서 codebook을 만들어도, 하나만 사용하더라도 잘 동작할것임.
7. parallel하게 signal을 위한 loss쪽 하나, text쪽 loss하나로 둬서 
      - 같은 Token으로 출발하는데, 말이 안됨.
      - 그러려면, Text embedding 따로(hugging face) → 샘플마다 PI 만들어놓듯이, Time-series Encoder 따로 두고 각자의 token을 통한 loss를 구현하고, 뒤에서 하나의 token으로 맞추는 느낌으로 함.
8. Text는 Tokenizer를 통해서 굳이 mu, sigma, t, l을 할 필요 없으니, z만 가진다고 생각하면 됨.
      - 이거로, 중앙에 Codebook을 업데이트하는 Operation을 포함하게 함.
      - Tokenizer Reverse를 통해서 다시 Time-series로 돌린 다음에, Recon Loss를 얻으면 된다.
9. Cross Attention을 통해서 다른 Modality 차이를 해소해줄 수 있다 + 같은 time-series로부터 출발했다는 것 이것에 집중해서 이번주 공부를 해야할듯.
10. 좋은 논문이 되기 위함이란? → 방법론까지는 거의 다 똑같음. 그렇지만 여기에 어떤 재미난 것을 더 추가할 수 있을 것인가? 어떻게 말을 표현할 수 있을 것인가?
11. toy model : GLW에 codebook 바꾸고, pi를 text로 바꾸는거. 
      - loss부분은?
      - 아마 Codebook 부분 바꾸는게 어려울듯.??
      - Problem : 어떻게 Time-series데이터를 interpretable 하게, subsequence로 분할할 수 있을까? → 이것을 time-series만으로는 할 수 없기 때문에, Text Modality를 가져오게 되었다.
→ 어떻게 Text를 넣을건데? : huggingface tokenizer를 통해서 같은 shape의 token으로 만듦.

GLW 논문, LLM Guided semantic augmentation, Frequency-Semantic Enhanced Variational Autoencoder for zeroshot skeleton based action ... 이런얘들 읽어봐야함.
   - Skeleton : Motivation이 되어야 할 논문. 
      - LLM을 어떻게 Text에 사용할지. → Prompt engineering : 어떻게 더 자세히 text를 잘 뽑아낼 수 있을까.


## GLW코드와 VQShape 코드를 돌려보면서, 어떻게 위의 일을 할 수 있을 지 생각해봐야할듯.
### VQShape 코드 따라가보기. (Main함수부터 쭉)
- 

## LSA 논문 읽어보기.
### Introduction
- 서론
      - Wearble Sensor Data는 다양한 real-world Setting에서 수집되고 있다.
      - 각자 환경마다, noise의 형태가 다르고, 이런걸 잘 처리하고 싶음.
- 문제점
      - Users는 다른 physical charateristics를 가진다. (몸무게, 팔길이, 같은 행동에 대한 패턴)
      - 실제 인간의 행동은 매우 복잡하고, 연속적이지만, 데이터셋을 구축할 때에는 한 동작을 하나의 고정된 카테고리로 붙여버린다. → 행동 중간의 전이 상태(걷기 → 뛰기), 행동의 변형 (느리게 걷기, 절뚝이며 걷기) 같은 미묘한 차이를 무시하게 됨.
      - 데이터 불균형 문제가 존재함. → 자주 나오는 행동(걷기, 앉기)는 잘 맞추지만, 드문 행동(점프, 설거지하기)는 무시하게 되는 경우
      - 데이터 Distributional shift(분포 차이) 문제 → 데이터 수집 환경, 센서 위치, 대상 집단(population)의 차이 때문에 분포가 바뀐다.
            - 스마트워치를 왼손에 차기 vs 오른손에 찬 경우.
            - 어린이 vs 청년 vs 노인의 걷기 패턴의 차이가 존재함.
- 기존의 해결 방법 및 한계점
      - Data Augmentation : 전형적으로, Gaussian noise나, temporal shifting, temporal misalignment 등을 통해 distortions(방해)에 강인한 모델을 만들고자 함.
      - 이 Data Augmentation 방법은, 입력 자체의 왜곡, 불일치에 대한 강인성은 키워주지만, 활동 고유의 의미적 구조(peak, periodic patterns)를 보존, 강조하지 못해서 서로 비슷한 파형을 가진 활동을 구분하는 데에 한계가 있다.
- LLM을 사용해야 하는 이유
      - 단순한 raw signal perturbation만으로는 의미 있는 행동 단서를 잘 반영하지 못하고, Semantics(의미 있는 구조)를 활용하는 증강 기법이 필요함.
      - 그런 Semantic prior를 줄 수 있는 도구가 바로 LLM임. → LLM은 자연어로 행동을 묘사할 수 있고, prompt를 통해 얻어진 Text는 고수준의 의미 지식(high-level semantic guidance)이므로, 모델 학습에 추가적인 감독 신호를 제공해줄 수 있다.
- 기존 LLM을 이용한 연구들
      - End to end predictor 방식 : Time-series를 Text형태로 바꿔 LLM으로 직접 분류 → 모델의 크기가 커져서 wearable 환경에 부적합.
      - Language 활용 방식 : 활동 추론, 대화형 reasoning, 멀티모달 사전학습 등에 사용 → 이 또한 inference 시에 추가 입력이나, 무거운 생성 모델을 요구함.
      - 근본적인 한계 : 인간의 행동은 Keyward의 수준부터, Sentence의 수준까지 다양하게 표현이 가능한데, 어떤 것이 더 좋은건지 모르겠음.
- 이 논문의 주요 포인트
      1. LLM의 high-level semantic knowledge가 Inference complexity를 높이지 않고 모델의 generalization 과 robustness를 향상시킬 수 있나?
      2. 어떤 형태의 linguistic supervision이 가장 효과적인지, 이 선택이 HAR 성능에 어떤 영향을 미치는지.
- 이 주요 포인트(question)를 해소하기 위해서 어떻게 할 것인지.
      - LLM을 end-to-end 예측기(입력 → 출력까지 다하는)로 사용하지 않는다.
      - LLM을 Semantic augmentation의 제공자로서 역할에 집중하게 함.
      - 즉, LLM이 생성하는 행동에 대한 Text 설명을 훈련 단계에서 주입해줌으로써, Sensor data만으로는 담기 힘든 맥락, 행동, 의미를 보강하고, HAR 모델의 잠재 표현(latent feature space)를 의미적으로 정규화(semantic regularizer)한다.
            - LLM은 Inference 단계에서는 사용하지 않음. (추론 시에는 Time-series만 받는 Unimodal Model로 동작) → wearable device에 적합.
      - 제안 방법 : LLM이 생성한 Text Prompt를 활용하여 HAR 모델의 Latent Space를 의미적으로 정규화하여 input perturbation만 하던 기존 방식에 Semantic 수준의 증강을 결합함.

### Proposed Method
- LLM이 Text Embedding을 어떻게 만드는가?
      - 각 Sample마다, Label을 가지고, LLM을 통해 그 Activity에 대한 (1) Keyword, (2) Sentence, (3) Negative Paraphrases, (4) Affirmative Paraphrases 등의 정보를 각각 추출하고, 이것들을 LLM Text Encoder를 통해서 Text Embedding을 만든다.
      - 전에 생각했던 것 처럼, 같은 Class Label인 Sample마다 같은 Text Embedding이 만들어 지는 것으로 보임 → ("LLM generates natural language prompts from class labels")
- 둘의 Text Embedding과, Signal Embedding을 공통의 Projection Layer를 통해서 Dimension을 매칭해준다.
      - 이 때 아마 Projection Layer는 공통의 네트워크를 사용하는 것으로 봐서, 공통된 의미 공간으로 만들어 주는 역할도 수행하는 것 같음.
      - 차원만 맞춰줄거면 개별로 Projection을 하던가 그냥 numpy연산으로 했을것임.
- Fusion Layer를 통한 정보 혼합
      - Fusion Layer는 두 번 나뉘어져서 수행되는 것으로 보인다.
      1. Z_s를 Query로 두고, Z_t를 Key/Value로 두는 것.
      2. Z_t를 Query로 두고, Z_s를 Key/Value로 두는 것.
      - 어떤 정보를 Main으로 두고, 다른 정보를 Sub로 두는지에 따라서 얻을 수 있는 장점이 다르기 때문에 각각을 수행하여 attn(Z)를 만드는 것으로 생각됨.
      - 마지막으로, 이렇게 정보가 합쳐진 attn(Z)와 Query로 사용한 Embedding Z를 Residual Connection처럼 합친 후, Proj_out(아마 Classifier?)를 통과시켜 Main Modality에 따른 확률 분포를 얻는다. → Fused(Z)
- Z_s fused를 이용한 Loss : Signal을 메인 Modality로 했을 때의 Label값과의 Cross Entropy Loss
- Z_t fused를 이용한 Loss : Text를 메인 Modality로 했을 때의 Label값과의 Cross Entropy Loss
      - 이 두 Cross Entropy에서는 Class Imbalance 문제를 해결해보기 위해서 label-dependent weight w_yi를 이용해서 클래스의 frequency를 어떻게 처리했다고 함. 
- Supervised Contrastive Loss : 

### 그외

