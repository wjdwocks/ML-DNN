### VQShape쪽 공부해가야 할 것.
1. 일단, Text Guidance는 Label을 이용해서 얻은 Sub-Actions에 대한 Vector들만 들어감.
2. 그렇다는 것은, Text Guidance는 단지, 이 Time-series에는 Action1, Action2, Action3 ... 의 Sub-Action이 들어간다는 정보만 줄 수 있음.
3. 원본 Signal에서는 이 부분 SubSequence가 어떤 Sub-Action인지는 당연히 모름. Text-Guidance에서도 이 Time-series에 이런 Sub-Action이 포함될 거라고 알려주기만 함.
4. 어떻게 이 Sub-Sequence가 어떤 Sub-Action인지 매핑이 가능할까? → 아직 모르겠다.
5. Attribute Decoder에서 Sub-Action을 추출하기 전에 어떤 행위를 더 해주면 이게 가능하기는 한 것일까?


### VQShape 공부하고있음...
1. Tokenizer까지 한 후에 그 Token과 Label을 이용한 Sub Action이라는 Text Guidance를 이용해서 Fusion Layer와 함께 Contrastive Loss를 하면, 두 모달리티의 의미정렬이 이루어지고, Time-series Encoder는 Text Guidance가 없이도 이 Sample안에 어떤 Sub-Actions들이 포함될 수 있는지를 알게 됨. 
2. 또한, 학습이 진행됨에 따라서 Token들과 Sub-Actions들 사이에 Attention Weight를 통해서 이 Token이 어떤 Sub-Action을 의미하는지를 알 수 있게 된다. 즉, Pretrain 과정에서 각 토큰이 어떤 모양의 Shape인지는 VQShape의 원래 목적과 같이 알 수 있게 되고, 이 Attention Weight를 통해서 어떤 Sub-Action일지까지도 확률로서 알 수 있게 된다. 
3. 그리고, 그 Token이 어떤 Code로 연결되는지도 알게 되기 때문에 이 Code가 어떤 Sub-Action을 많이 선택했는지를 알 수 있기 때문에 각 Code가 어떤 Sub-Action을 의미하게 될지도 알 수 있게 되긴 함. 
    - 이 때, Attention Weight를 가지고 이 Label의 Sub-Action만이 아닌 전체 Sub-Action에 대한 Attention Weight(확률 분포)로 만들어서 Attribute Decoder의 Action Head가 예측하게 해야 Prediction 과정에서 Time-series Input만 가지고도 예측을 할 수 있다.
4. 또한, Attribute Decoder에서 Action Head를 통해서 어떤 Sub-Action인지예측하게 하고, Loss 설계를 통해 하나의 Sub-Action으로 치우치도록 Sharp하게 만들어줌.

- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/VQShape.png" alt="results" width="700">




### 25.10.20 미팅 내용
- 어떻게 Time-series Token하나에 Sub Action하나를 매핑시킬 수 있냐??
- Label을 이용한 Text Encoder를 사용하면 그 Text Guidance에서는 모든 Label이 합쳐져서 정보가 생김.
    - 그렇다는 것은 우리가 Label을 이용한 Sub Actions백터들에 대한 정보가 혼합되어버림?
- 어떤 특정 Loss를 통해서 하나의 Token에 하나의 Sub Action이 되게 해야 함.
- Text Guidance는 Sub Actions를 줄여주는 역할
- Text 를 이용해서 Time-series Codebook을 만들면, Time-series만 사용한 VQShape과 Code의 모양이 다를 것이고 이것이 Novelty가 될 수 있다.

### 이 둘에 대해서 자세히 생각해보자.
- Sub Action에 대한 유니크한 Representation으로 하는 것이 좀 더 명확히. (Sub Actions들을 구성할 때 뭔가 더 필요해보임)
- Token(Code)에 하나하나씩 Sub-Action이 어떻게 매핑이 잘 될 것인가?
- 근거를 좀 더 명확히 미리 준비해가자. (reference)
- 좀 더 문과적인 마인드로 설명을 하자.



## 생각한 방법.
### 공통
- 각 Class별로 약 5개의 Sub-Action 후보를 LLM 등을 활용해 추출한다.
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/subactions.png" alt="results" width="650">
- 전체 데이터셋에 대해, 각 Class에서 추출된 Sub-Action들을 Similarity 기반으로 중복 제거하여 최종 약 12개의 Sub-Action 집합을 구성한다. (개수의미 x)
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/subactions2.png" alt="results" width="650">
- 입력 시계열 샘플 x가 (1, C, 512) 형태라면, 이를 Signal Encoder → (1, 64, 8) → Signal Tokenizer → (1, 32, 256) 형태로 변환한다.
- 각 Class의 Sub-Action 텍스트는 Text Encoder (예: LLM 기반) 를 통해 (1, 5, 256) 형태의 임베딩으로 변환하며, Text Encoder는 Freeze한다.
- 두 모달리티의 차원을 일치시키기 위해 필요 시 Projection Layer를 적용한다.
- LSA와 유사하게 Signal Token을 Query로 하는 Cross-Attention과 Text Token을 Query로 하는 Cross-Attention을 각각 한 번씩 수행하여, 두 모달리티의 semantic space를 align시킨다.
    - 이 과정을 통해 Signal Encoder와 Tokenizer만으로도 샘플 내 포함된 Sub-Action의 의미를 부분적으로 추론할 수 있게 된다.
    - 또한 Signal을 Query로 한 Cross-Attention의 Attention Weight은 각 Token이 어떤 Sub-Action에 주목하는지를 나타내는 확률 분포(Soft Label) 로 활용할 수 있다.
- 한 샘플에서 얻은 5개의 Sub-Action 확률 분포를 전체 Sub-Action 집합(예: 12개)에 맞춰 확장하고, 나머지 7개 항목은 0으로 채워 전체적인 확률 분포 형태로 구성한다.
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/method0.png" alt="results" width="700">


### 1번 방식 : Attribute Decoder에 Action Head를 추가.
- 기존 Attribute Decoder에 Action Head를 추가하여 전체 Sub-Action에 대한 확률 분포를 직접 학습한다.
- Action Head는 KL-Divergence Loss를 통해 Attention Weight 기반의 soft target을 모방하도록 학습한다.
- KL Loss를 사용하는 이유: 유사한 Sub-Action 간의 semantic 관계를 유지하면서 soft한 분포를 학습하기 위함이다. (예: walk ↔ jog ↔ run 간의 관계를 자연스럽게 보존)
- 장점 : Downstream Task에서 각 Token의 Attribute representation을 Sub-Action 정보와 함께 전달할 수 있어, Classification 등 후속 작업의 성능 향상을 기대할 수 있다. (Token-level semantic richness 강화)
- 단점 : Main Motivation인 “각 Code를 통해 Shape과 Sub-Action을 함께 이해하는 구조”에는 부합하지 않는다. Action Head는 Token 단위로 학습되므로, Codebook의 각 Code가 어떤 Sub-Action과 대응되는지 직접적으로 알 수 없다. 따라서 Code–SubAction 매핑을 위해 별도의 후처리가 필요하다.
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/method1.png" alt="results" width="700">

### 2번 방식.
- Token에서 Quantization을 통해 얻은 Code z를 기존 Shape Decoder에 입력하여 shape reconstruction을 수행하는 동시에, Action Decoder를 추가하여 Sub-Action 분포를 예측하게 한다.
- Action Decoder 또한 KL-Divergence Loss로 학습하며, Attention Weight 기반 확률 분포를 target으로 사용한다.
- 장점 : Codebook이 Shape 정보 + Sub-Action semantic 정보를 동시에 학습하게 된다.
    - Shape Decoder로부터의 reconstruction loss (shape-level supervision)
    - Action Decoder로부터의 Sub-Action alignment loss (semantic supervision)를 동시에 받으며, Codebook embedding이 더욱 풍부한 의미 공간을 형성한다.
    - 완성된 Codebook을 사용하면, 각 Code를 Shape Decoder에 통과시켜 “어떤 모양인지”를 알 수 있을 뿐 아니라, Action Decoder를 통해 “어떤 Sub-Action에 해당하는지”까지 즉시 파악할 수 있다. → 논문의 Motivation 충족 가능
- 단점 : Codebook이 두 가지 목적(Shape & Semantic)을 동시에 학습해야 하므로, 두 손실 간의 경쟁(trade-off) 가능성이 있다. 
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/method2.png" alt="results" width="700">


## 3Teacher 논문 작성 및 실험 (거의 못함..)
- WRN 다른 조합 2Teacher 그림 Robustness쪽 논문 참고해서 히스토그램으로 처리. ## 어떻게 그릴 수 있을지 걱정이 됩니다

- Tolerance Noise : 모든 Teacher에 대해서 다 그려보기 # Teacher의 크기가 달라도 되고, 같더라도 다른 모델로
- Confusion Matrix : 다른 Student로 더 좋은게 있나 보기 # 다른 Model이면 F1-score가 달라질 수 있으니까 이걸 노리고.
- 실험 중 학습하는 / 학습한 SP Maps들 시각화 및 비교 : 2Teacher꺼들 지워서도 그려보고 KD(TS)가 없던가 그랬음. # 이건 어차피 지우기만해도 되서 나중에
    - 생각해보니 Teacher1이랑 Student랑 다를게 없다..
    - <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/sp_map1.png" alt="results" width="700">
    - <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/25년/10월/25.10.24/sp_map2.png" alt="results" width="700">

- Hyper parameter : 3Teacher만 표로 Alpha 비교해서 한다. # 하나 고정 나머지 변화 이런식으로 3개 Section으로 표를 구성하면 좋을듯.
- Parametric Plot : 하던대로 하는데, SPKD포함되어서 그려보면 좋을듯.

