## 24.10.30 공부한 내용
<li> 여러 네트워크에 대해 공부하고, ???에 대한 논문 2편과 동영상 내용에 대해 공부하고, 이해하자. </li>
<li> VGG, Resnet - series, Wide Resnet, Mobile net v1/v2, Inception Net, Shuffle Net, DenseNet, MLP, FPNet </li>

## VGG Net
<li> 3x3 크기의 필터를 사용한 Convolution 층을 여러 개 쌓아 깊이 있는 네트워크를 구성함. </li>
<li> 이미지 처리에서 input특성을 3x3 kernel을 이용해 2~3개의 convolution 층을 통과하게 한 후, 2x2 Maxpooling을 통해서 크기를 줄여주고, 이를 VGG-16, 19등에 따라 여러 번 반복해준다.</li>
<li> 마지막으로 몇 개의 linear층을 통과시켜 주는 아주 간단한 모델. </li>

## ResNet
<li> Skip Connection : 신경망이 깊어질 수록 기울기 소실 문제 때문에, 학습이 재데로 이루어지지 않는다. 이를 위해 단차를 줄여주는 방식을 사용. </li>
<li> 기울기 소실 문제란, backward과정에서 기울기가 점점 작아져서, optimizer.step단계에서 가중치를 업데이트 하는데, 큰 의미를 주지 못하는 것을 의미함. </li>
<li> 각 Residual Block을 통과할 때 변환된 출력 값에 원래의 입력 값 x를 더해줌으로써 단차를 줄여줄 수 있다. (네트워크가 깊어져도 중요한 정보가 손실되지 않고, 각 층에 전달됨.) </li>
<li> 단차 : 이전 입력과 최종 출력 간의 차이 (정보의 차이.) </li>
<li> Batch Normalization : 각 층의 출력을 정규화하여, 데이터 분포가 일정하게 유지되게 함. - Exploding Gradient 문제를 해결할 수 있다 (역전파 과정에서 기울기가 너무 커져버리는 현상.) </li>

## ResNet - 18
<li>원본 사진의 크기가 224 x 224 라고 가정한다.</li>
<ul>
<li> 처음에는, Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3)을 통해 이미지의 크기 는 절반으로 줄어들고, 깊이는 64가 됨. </li>
<li> MaxPooling(kernel_size=(3, 3), stride=2, padding=1)로 깊이는 그대로, 이미지 크기는 절반으로 더 줄여줌. </li>
<li> 아레의 4개의 Residual Stage를 통과함. </li>
<li> 첫 convolution층을 보내기 전에 maxpooling을 통해  </li>
<li> 다음에 4개의 Residual Stage를 지난다. </li>
<li> 각각의 Residual Stage에는 2개의 Residual Block이 있다. </li>
<li> 각각의 Residual Block은 다음과 같이 구성된다. </li>
<ol>
<li> out = conv(x) # x를 유지해주기 위해 out이라는 새로운 변수 사용 </li>
<li> out = bn(out) </li>
<li> out = relu(out) </li>
<li> out = conv(out) </li>
<li> out = bn(out) </li>
<li> out = out + x # 잔차를 해결해주기 위해 x를 더해줌. </li>
<li> out = relu(out) </li>
</ol>
<li> 모든 Residual Stage의 첫 번째 블록, 첫 번째 Convolution 층에서는 Strides=2, out_channels=in_channels*2를 해주어, 각 Stage 당 특성 맵의 크기는 반, 깊이는 2배가 되도록 조정한다. </li>
<li> Fully Connected 층과 연결되어 최종 출력을 한다. </li>
<li>Resnet-18 동작 예시.</li>
</ul>

![Resnet-18](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_18.png)



## ResNet - 34
<li>원본 사진의 크기가 224 x 224 라고 가정한다.</li>
<ul>
<li> 첫 Convolution 층 : - Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3)을 통해 이미지의 크기는 절반으로 줄어들고, 깊이는 64로 바꾼다. </li> 
<li> Max Pooling : - MaxPooling(kernel_size=3x3, stride=2, padding=1)을 통해 깊이는 그대로 유지하고, 이미지의 가로 세로 크기만 절반으로 줄입니다. </li>
<li> 4개의 Residual Stage를 통과합니다. </li> 
<li> 각 Residual Stage 구성: - Stage는 총 4개로 이루어지며, 각 Stage는 서로 다른 수의 Residual Block을 포함한다.</li>
<li> Stage 1: 3개의 Residual Block (채널 수 64) - 6개의 Conv층 </li>
<li> Stage 2: 4개의 Residual Block (채널 수 128) - 8개의 Conv층 </li>
<li> Stage 3: 6개의 Residual Block (채널 수 256) - 12개의 Conv층 </li>
<li> Stage 4: 3개의 Residual Block (채널 수 512) - 6개의 Conv층 </li> 
<li> 각 Stage마다 특성 맵의 크기를 절반으로 줄이고 깊이를 두 배로 늘리기 위해, 첫 번째 Residual Block의 첫 번째 Convolution 층에서 stride=2와 out_channels=in_channels*2를 적용한다.. </li> 
<li> 각 Residual Block 구성: - 각각의 Residual Block은 두 개의 Convolution 레이어로 구성되며, 다음과 같은 구조를 가진다.
<ol> 
<li> out = conv(x) </li> 
<li> out = bn(out) </li> 
<li> out = relu(out) </li> 
<li> out = conv(out) </li> 
<li> out = bn(out) </li> 
<li> out = out + x </li> 
<li> out = relu(out) </li> 
</ol> 
</li> 
<li> 각 Stage에서 다운샘플링: - 모든 Residual Stage의 첫 번째 Block의 첫 번째 Convolution 층에서만 `stride=2`와 `out_channels=in_channels*2`로 설정하여 특성 맵 크기를 줄이고 깊이를 증가시킵니다. </li>
<li> Fully Connected 층: - 마지막에 Global Average Pooling을 사용하여 특성 맵을 축소한다. (512, ?, ?)의 크기를 (512, 1, 1)의 크기로 바꾸어준다. (?, ?)는 원래 이미지의 크기에 비례함. 이 ? x ? 영역을 평균하여 바꾸주는 것임. </li>
<li> Fully Connected 층과 연결하여 최종 출력을 생성한다.</li> 
</ul>

![Resnet-34](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_34.png)

---

## ResNet - 50
<li> 원본 사진의 크기가 224 x 224 라고 가정한다.</li>
<li> Resnet 50부터는 Residual Block의 구조가 달라진다. </li>
<ul> 
<li> 초기 Convolution 층: - Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3) 으로 똑같다. </li> 
<li> Max Pooling: - MaxPooling(kernel_size=3x3, stride=2, padding=1)을 통해 깊이는 그대로 유지하고, 이미지의 가로 세로 크기만 절반으로 줄인다. (똑같음) </li>
<li> 4개의 Residual Stage를 통과한다. </li> 
<li> 각 Residual Stage 구성: - ResNet-50에서는 Bottleneck 구조를 사용하여 Residual Block이 3개의 Convolution 레이어로 구성됩니다. </li> 
<li> Stage 1: 3개의 Bottleneck Residual Block (채널 수 256) - 9개의 conv층 (이미지 : 56, 56) </li>
<li> Stage 2: 4개의 Bottleneck Residual Block (채널 수 512) - 12개의 conv층 (이미지 : 28, 28)</li>
<li> Stage 3: 6개의 Bottleneck Residual Block (채널 수 1024) - 18개의 conv층 (이미지 : 14, 14)</li>
<li> Stage 4: 3개의 Bottleneck Residual Block (채널 수 2048) - 9개의 conv층 (이미지 : 7, 7)</li> 
<li> 각 Stage마다 특성 맵의 크기를 절반으로 줄이고 깊이를 두 배로 늘리기 위해, 첫 번째 Bottleneck Block의 첫 번째 Convolution 층에서 stride=2와 out_channels=in_channels*2를 적용합니다. </li> 
<li> Bottleneck Residual Block 구성: - 각각의 Bottleneck Residual Block은 다음과 같은 순서로 3개의 Convolution 레이어와 Batch Normalization으로 구성됩니다: 
<ol> 
<li> out = conv(x, output=1/4 * out_channels, kernel_size=(1,1)) # 첫 번째 1x1 Conv로 채널 수를 줄임</li> 
<li> out = bn(out)</li> 
<li> out = relu(out)</li> 
<li> out = conv(out, output=1/4 * out_channels, kernel_size=(3,3)) # 3x3 Conv로 특징 학습</li> 
<li> out = bn(out)</li> 
<li> out = relu(out)</li> 
<li> out = conv(out, output=out_channels, , kernel_size=(1,1)) # 마지막 1x1 Conv로 채널 수를 원래대로 복원</li> 
<li> out = bn(out)</li> 
<li> out = out + x # 잔차 연결을 위해 입력 x를 더해줌</li> 
<li> out = relu(out)</li> 
</ol> 
</li> 
<li> 첫 번째 Residual Stage의 첫 번째 Residual Block은 이전 출력을 유지. </li>
<li> 각 Stage에서 다운샘플링 : 모든 Residual Stage의 첫 번째 Block의 첫 번째 Convolution 층에서만 `stride=2`와 `out_channels=in_channels*2`로 설정하여 특성 맵 크기를 줄이고 깊이를 증가시킵니다. 두 번째 스테이지부터 적용. </li> 
<li> Global Average Pooling: - 마지막 Residual Stage의 출력을 받아 Global Average Pooling을 통해 (2048, 1, 1) 크기의 출력을 얻습니다. </li> 
<li> Fully Connected 층: - GAP 층의 출력을 평탄화하여 1차원 벡터로 변환한 후, 마지막 Fully Connected 층에 연결되어 최종 클래스 확률을 예측합니다. </li> 
</ul>

![Resnet-50](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_50.png)

## ResNet - 101, 152
<ul>
<li> Resnet 50에서 같은 Bottleneck Residual Block을 사용하고, 각 스테이지 당 블록의 개수만 달라진다.
</ul>

---

## Wide ResNet
<li> 32x32 이미지의 예시 </li>
<li> Resnet-101, 152와 같은 깊은 네트워크는 학습과 연산량이 매우 큰 것에 비해 별로 효율적이지 않기 때문에 Wide Resnet이라는게 등장함. </li>
<li> 무조건적으로 깊이를 늘리기보다, 네트워크의 너비(채널 수)를 늘리는 것이 연산 효율성과 성능 향상 측면에서 더 효과적이라는 것. </li>
<ul>
<li> 처음에 Convolution(input, out_channels=16, kernel_size=3x3, padding=1)을 통해 초기 특징을 추출함. (이미지 크기는 유지. (32x32) ), 원본 이미지가 커질 수록 out_channels는 늘어남. </li>
<li> 다음에 Resnet과 다르게 Maxpooling이 적용되지 않는다. </li>
<li> 3개의 Residual Stage를 통과한다. </li>
<li> Wide Residual Network는 각 Stage에 포함된 채널의 수가 확장 배수를 곱하여 늘어난다. ex) Wide Resnet-28-10 에서 10이 확장배수. </li>
<li> Stage 1: 4개의 Residual Block (out_channels = 160) - 8개의 Conv층 이미지 : 32x32</li>
<li> Stage 2: 4개의 Residual Block (out_channels = 320) - 8개의 Conv층 이미지 : 16x16</li>
<li> Stage 3: 4개의 Residual Block (out_channels = 640) - 8개의 Conv층 이미지 : 8x8</li>
<li> 각 Residual Block은 아레와 같이 구성되어 있음. (기본적으로 Resnet-18의 Residual Block과 똑같이 생김.)</li>
<ol> 
<li> out = conv(x, output=확장된 채널 수, kernel_size=3x3)</li> 
<li> out = bn(out)</li> 
<li> out = relu(out)</li> 
<li> out = dropout(out) </li>
<li> out = conv(out, output=확장된 채널 수, kernel_size = 3x3)</li> 
<li> out = bn(out)</li> 
<li> out = out + x # 잔차 연결을 위해 입력 x를 더해줌</li> 
<li> out = relu(out)</li> 
<li> out = dropout(out) </li>
</ol>
<li> 이것도 마찬가지로, 각 Residual Stage의 첫 번째 Block의 첫 번째 층의 Stride = 2, out_channels=2*in_channels로 하여 특성맵은 반, 깊이는 두배로 늘려준다.(첫 번째 Stage 제외.) </li>
<li>GAP를 이용하여 (640, 1, 1)의 크기로 변환한 후 FC층과 연결해 최종 출력을 받는다.</li>
</ul>

	요약 : 그냥 Stage가 3개로 줄고, 
	첫 Conv층에서 Residual Stage로 넘어갈 때 Maxpooling이 사라지고, 
	out_channels를 확장배수 만큼 늘어났다고 생각하자. 
	나머지는 Resnet-18과 똑같음. 내부 구조는.

---

## Mobile Net
<li> 모바일 및 임베디드 장치에서 효율적으로 작동하도록 설계된 경량화 CNN 아키텍처 </li>
<li> 연산량과 파라미터 수를 줄여 성능을 유지하면서, 모바일 환경에서 실시간에 가까운 속도를 내는 것이 목표이다. </li>

### Mobile Net V1
<li> Depthwise Separable Convolution </li>
<ul>
<li> 일반 Convolution을 두 단계로 나누어서 연산량과 파라미터의 수를 줄인다. </li>
<li> Depthwise Convolution : 각 채널에 대해 독립적으로 Convolution을 수행하여 채널별로 특성을 추출한다. </li>
<li> Pointwise Convolution : 1x1 Convolution을 통해 각 채널의 특성을 결합하여 출력 채널을 생성한다. </li>
<li> 이 방식을 이용하면 일반 Convolution 대비 약 1/8까지 연산량이 줄어들 수 있다고 한다. </li>

- Depthwise Convolution층.
``` py
self.dconv = nn.Sequential(
    nn.Conv2d(in_channels, in_channels, kernel_size = 3, stride, padding = 1, groups=in_channels),
    nn.BatchNorm2d(in_channels),
    nn.ReLU()
) 
# Depthwise Separable Convolution을 구현한 모습.
# Conv2d에 out_channels = in_channels로 함으로써 각 채널별로 하나의 필터만 적용된 값으로 그대로 나옴.
# groups = in_channels로 지정해줌으로써, 각 입력 채널에 독립적인 필터를 적용할 수 있다고 함.
```

- Pointwise Convolution층.
``` py
self.conv = nn.Sequential(
    nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = 1),
    nn.BatchNorm2d(out_channels),
    nn.ReLU()
)
# 왜 (1, 1) Convolution을 하면 독립되었던 위의 특성맵들이 결합이 되는가? (생각해보면 당연하다)
# 위에서 나온 독립적인 특성맵이 예를 들어 10개가 있다.
# 그러면, 1x1 Convolution을 하면 각 특성맵을 모두 훑으며 총 10개의 특성맵 정보를 이용해서 하나의 특성맵을 또 만듦.
# 그렇기 때문에, Depthwise로 개별 연산을 하고, pointwise로 합친다는 의미임.
```
<li> 원래 Convolution은 H(이미지 높이) x W(가로) x D_in(입력 채널) x D_out(출력 채널) x K(커널 크기) x K 의 연산을 하는데,  </li>
<li> Depthwise Separable Convolution은 H x W x D_in x K x K + H x W x D_in x D_out 만큼 하기에 (분리한 두 연산의 합) 계산 시간이 줄어드는 것임. </li>
</ul>

### Mobile Net V2
<li> Inverted Residual Block과 Linear Bottleneck이라는 새로운 개념을 도입하여 연산 효율을 유지하면서, 특성 추출 능력을 높였다. </li>
<li> 1. Inverted Residual Block </li>
<ul>
<li> 일반적인 Residual Block과 반대로 채널 수를 먼저 확장한 다음 Depthwise Convolution을 적용한 후, 다시 채널 수를 줄인다. </li>
<li> 위의 Bottleneck Residual Block은 각 스테이지 내의 각 블록마다 (축소 -> 유지 -> 확장 -> 축소 -> 유지 -> 확장)을 반복했음. </li>
<li> 좁은 채널에서 연산이 일어나도록 하지 않고, 중간에 채널을 확장해 더 많은 정보를 학습하게 한다. </li>
<li> Mobile net v2의 기본 블록은 '확장 -> Depthwise Convolution -> 축소' 로 구성됨. </li>
</ul>
<li> 2. Linear Bottlenect </li>
<ul>
<li> Inverted Residual Block의 마지막에 ReLU대신, Linear 활성화 함수를 사용한다. </li>
<li> ReLU는 max(0, x)이기 때문에, 0보다 작은 경우, 필연적으로 정보의 손실이 발생함.(근데, 기존보다 낮은 차원으로 변환 후에 적용하면 더 많이 발생한다고 함. 근데 이게 지금까지 우리가 해온 방식임.) </li>
<li> Linear 활성화 함수는 특성 표현이 더 잘 유지되고, 정보 손실을 줄일 수 있기 때문. </li>
</ul>
<li> 요약하자면, Resnet처럼 여러 Stage로 구성되고, 각 Block이 Inverted Residual 구조이고, 그 내부의 Convolution 연산이 Depthwise Separable Convolution으로 분리된다. +(Skip Connection을 할 때 입력 x와 출력의 채널의 수가 같을 때만 적용. ReLU대신 Linear()를 사용함.) </li>
<li> Depthwise Convolution을 유지하지만, PointWise Convolution이 아닌, Bottleneck 구조로 채널을 관리한다. </li>

## ShuffleNet
<li> 모바일 및 임베디드 장치에서의 효율적인 신경망 설계를 위한 경량화 모델임. </li>
<li> MobileNet에서 대부분의 연산량이 1x1 Conv(PointWise Convolution)에서 나오게 되는데, 이 부분의 계산 비용을 줄이기 위한 방식. </li>
<li> ShuffleNet의 특징 </li>
<ul>
<li> Group Convolution : 입력 채널을 여러 그룹으로 나누고, 각 그룹에 대해 Convolution을 수행함. 일반적인 Convolution과 달리, 각 그룹은 다른 필터를 사용하므로, 연산량을 줄이고, 계산 효율성이 높아짐.</li>
<li> Channel Shuffle : 각 그룹의 출력을 섞어서 다음 그룹의 입력으로 전달하여 서로 다른 그룹 간의 정보를 공유할 수 있도록 한다. (다양한 특성을 학습할 수 있음.) </li>
</ul>
<li>Group Shuffle의 의미</li>
<ul>
<li> Group Convolution에서는 입력 채널이 여러 그룹으로 나뉘어져서 각 그룹에 대해 독립적으로 Convolution을 수행함. </li>
<li> 그렇기에 각 그룹 내에서만 필터가 적용되므로, 그룹 간의 정보 교환이 없다. 이로 인해 정보가 제한된 범위 내에서만 전파된다. </li>
<li> Channel Shuffle은 이러한 제한된 정보 흐름을 개선하기 위해 각 그룹의 출력을 서로 섞어 다음 그룹의 입력으로 전달해주는 과정을 뜻한다. </li>
<li> 이 과정에서 서로 다른 그룹의 출력이 서로 연결되므로, 각 그룹 간에 정보를 효과적으로 공유한다.</li>
<li> 이로 인해 약간의 정보 손실이 있을 수 있지만, 채날 간에 정보 교루가 이뤄져서 더 다양한 특징을 학습할 수 있게 된다. </li>
</ul>



## Inception Net
<li> Inception Net은 1x1, 3x3, 5x5 Convolution 필터와 3x3 MaxPooling을 병렬로 사용한다. (여러 필터가 동시에 적용된다.) </li>
<li> 위와 같은 구조를 Inception Module이라고 하는데, 각각은 아레와 같이 구성되어 있음. </li>
<li> 1x1은 작은 패턴 (색상), 3x3은 중간 크기의 패턴 (지역적), 5x5는 넓은 영역의 큰 패턴을 감지함. </li>
<li> 그런 후에, 3x3 MaxPooling으로 중요한 정보를 추출한다. </li>
<li>학습 과정</li>
<ol>
<li> 입력 채널 수를 줄이기 위해, 먼저 1x1 Convolution으로 채널을 줄인다. </li>
<li> 축소된 채널로 3x3 Convolution을 적용, 특징 추출 </li>
<li> 축소된 채널에 대해 5x5 Convolution을 적용. </li>
<li> 3x3 MaxPooling을 적용하여 특정 영역에서 가장 중요한 정보들만 추출한다. </li>
</ol>


## DenseNet
<li> Resnet에서는 이전 레이어의 out과, 이번 layer의 output을 통으로 합쳐버리기 때문에 정보들이 온전히 흘러가지 않음. </li>
<li> 각 레이어가 이전 모든 레이어의 출력을 입력으로 받는 구조. </li>
<li> 층 간 연결을 최대화하여 효율적인 특징 학습을 가능하게 만듦. </li>
<li> Residual Connection을 확장한 개념이다. </li>
<li> DenseNet의 구조 </li>
<ul>
<li> Dense Block 1 : 6개의 Dense Layer를 통과함. 각 Layer는 입력에 대해 BatchNorm -> ReLU -> 1x1 Conv(Bottleneck) -> BatchNorm -> ReLU -> 3x3 Conv 순서로 이루어짐. </li>
<li> Transition Layer를 통과함 (특성 맵의 크기를 줄이고, 채널 수를 조절해준다.) : 1x1 Convolution층을 사용하여 다음 Dense Block에 도달하기 전에 채널 수를 줄여줌, 2x2 Average Pooling(stride=2, padding=same)을 통해 특성 맵의 크기를 절반으로 줄여준다. (이미지가 반반으로 줄어듦.) </li>
<li> Dense Block 2에 도달하여 위의 과정을 반복한다. </li>
<li> 마지막 Dense Block 통과 후 Global Average Pooling을 통해 최종 출력 크기를 (?, 1, 1)로 변환함. 이 후 Fully Connected층을 통해 최종 예측을 함. </li>
</ul>
<li> 각 층이 Batch Normalization으로 시작하는 이유 : Pre-activation이라고도 불림. 모델의 일반화 성능이 향상되고, 학습 안정성이 개선됨.
</li>
<li> DenseNet에서도 각 DenseBlock의 Layer를 bottleneck구조로 하여 더 효율적으로 연산이 가능하다. </li>
<br>

![DenseNet](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/DenseNet.png)



## FPNet
<li> 가중치 프루닝 (Pruning)을 사용하여 모델을 더 경량화 한 방식이다. </li>
<li> MobileNet v1 -> ShuffleNet -> MobileNet v2 -> FPNet의 순서로 발전함. </li>
<li> Weight Pruning : 모델의 가중치 중에서 불필요하거나 중요하지 않은 값을 제거하여 모델 크기와 연산량을 줄이는 방법이다. </li>
<li> Weight Pruning의 기본 개념 </li>
<ul>
<li> 딥러닝 모델에서는 모든 가중치가 동일한 중요성을 가지지 않는다. 그렇기에 작은 값의 가중치는 모델의 출력에 미치는 영향이 적다고 가정하고, 제거할 수 있다. </li>
<li>가중치의 절댓값이 Threshold 이하인 가중치를 제거하고 뉴런을 끊어버림.</li>
<li> 가중치를 제거한 후(Pruning 후) 남은 가중치로 모델을 재학습시킨다.</li>
<li> 위의 과정을 통해 모델이 중요한 특징을 잃지 않으면서 성능을 회복할 수 있다. </li>
<li> 여기서 중요한 것은 Pruning 과정을 통해 필요하지 않은 가중치를 제거함으로써 모델의 저장 용량이 줄어들고, 메모리 사용량이 감소하게 된다. </li>
</ul>

## MLP란?
<li> 피드 포워드 구조라고 해서, 입력층 -> 은닉층 -> 출력층으로 구성된 기본적인 구조. 근데 주로 완전 연결 구조로 되어 있다. </li>
<li> 입력 -> 은닉 -> 출력으로 진행되는 단방향 구조를 가진다. </li>
<li> 각 뉴런의 출력에 비선형성을 부여하기 위해 ReLU, Sigmoid, Tanh등의 활성화 함수를 사용함. </li>



## 정리
<li> 딥러닝 성능 향상을 위한 네트워크 </li>

| 네트워크         | 주요 특징                                                | 주로 사용하는 활성화 함수 |   주로 사용하는 Optimizer      |
|------------------|--------------------------------------------------------|-----------------------|-----------------------------|
| VGG              | 심층 신경망 구조, 깊이를 증가시켜 성능 향상           | ReLU                  | SGD(그때는 SGD가 대세였다.)                  |
| ResNet           | Residual Connection으로 기울기 소실 문제 완화         | ReLU                  | SGD, Adam                  |
| Wide ResNet      | 각 layer의 너비(출력 채널 수)를 늘려 성능 향상                                   | ReLU                  | SGD, Adam                  |
| InceptionNet     | 다양한 크기의 필터를 병렬로 사용하여 특징 학습       | ReLU                  | SGD, Adam                  |
| DenseNet         | 블록 내 모든 레이어가 이전 레이어의 출력을 입력으로 사용      | ReLU, Linear(블록 간)     | SGD, Adam                  |


<li> 모델 경량화를 위한 네트워크 </li>

| 네트워크         | 주요 특징                                                | 주로 사용하는 활성화 함수 |   주로 사용하는 Optimizer      |
|------------------|--------------------------------------------------------|-----------------------|-----------------------------|
| MobileNet V1     | Depthwise Separable Convolution으로 경량화            | ReLU                  | SGD, Adam                  |
| ShuffleNet       | Group Convolution과 Channel Shuffle로 정보 교환        | ReLU                  | SGD, Adam                  |
| MobileNet V2     | Inverted Residual Block과 Linear Bottleneck으로 경량화 | ReLU, Linear(블록 간)    | SGD, Adam                  |
| FPNet            | 가중치 프루닝을 통해 경량화 및 연산량 감소            | ReLU                  | SGD, Adam                  |






## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = Intel(R) Iris(R) Xe Graphics </li>
<li> CPU = 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz, 2419Mhz, 4 코어, 8 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.001 </li>
<li> optimizer = Adam (Adagrad, RMSprop, SGD, SGD+Momentum 도 사용.) </li>



## Evaluation


## Results
<li> </li>
