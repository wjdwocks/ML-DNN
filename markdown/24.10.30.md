## 24.10.30 공부한 내용
<li> 여러 네트워크에 대해 공부하고, ???에 대한 논문 2편과 동영상 내용에 대해 공부하고, 이해하자. </li>
<li> VGG, Resnet - series, Wide Resnet, Mobile net v1/v2, Inception Net, Shuffle Net, DenseNet, MLP, FPNet </li>

## VGG Net
<li> 3x3 크기의 필터를 사용한 Convolution 층을 여러 개 쌓아 깊이 있는 네트워크를 구성함. </li>
<li> 이미지 처리에서 input특성을 3x3 kernel을 이용해 2~3개의 convolution 층을 통과하게 한 후, 2x2 Maxpooling을 통해서 크기를 줄여주고, 이를 VGG-16, 19등에 따라 여러 번 반복해준다.</li>
<li> 마지막으로 몇 개의 linear층을 통과시켜 주는 아주 간단한 모델. </li>

## ResNet
<li> Convolution 층을 아주 깊게 하는데, 이때 발생하는 문제를, Residual block을 통해 해결함.</li>
<li> 아무런 테크닉 없이 깊이만 엄청 늘린다면, 기울기 소실문제나 과적합이 발생해서 오히려 성능이 나빠짐.</li>
<li> 기울기 소실 문제란, backward과정에서 기울기가 점점 작아져서, optimizer.step단계에서 가중치를 업데이트 하는데, 큰 의미를 주지 못하는 것을 의미함. </li>
<li> 해결 방법 : 2개층 마다 원래의 input값인 x를 더해준다. </li>
<li> 즉, x -> conv(x) -> conv(x) + x -> Relu -> x' 를 여러 번 반복한다. </li>
<li> 다음 번에는 x' -> conv(x') -> conv(x') + x' -> Relu 이런식으로 각각의 Residual Block을 통과한다. </li>

## ResNet - 18
<li> 주로 (2개의 3x3필터를 사용한 Convolution 층으로 구성된) Residual Block을 사용함. </li>
<li> 즉, x -> conv(x) -> conv(x) + x -> Relu -> x' 를 여러 번 반복한다. </li>
<li>Resnet-18 동작 예시.</li>
<ul>
<li> 1. 7x7 크기의 커널 64개로 Convolution해줌.</li>
<li> 2. Residual Block을 두개 통과 후 ReLU 적용.</li>
<li> 3. Residual Block을 두개 통과 후 Maxpooling으로 입력크기 반, 깊이 2배로 변경</li>
</ul>
<li> 위의 2,3 번 과정을 4번 반복한다. </li>
<li> 마지막으로, Fully Connected 층에 연결하여 출력을 한다. </li>

![Cifar-10 Adagrad](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_18.png)



## ResNet - 34
<li> </li>
<li> </li>
<li> </li>


## ResNet - 50
<li> </li>
<li> </li>
<li> </li>
<li> </li>


## ResNet - 101, 152
<li> </li>
<li> </li>
<li> </li>
<li> </li>


## 각 Optimizer 비교
<li>Adam : SGD와, Momentum, RMSprop, Adagrad의 개념을 결합한 옵티마이저임.</li>
<li>Adagrad : 각 파라미터마다 학습률을 개별적으로 조정하여 학습 속도를 최적화한다.</li>
<li>RMSprop : 기울기 제곱에 대한 지수 이동 평균(EMA)를 기울기의 2차 모멘트를 추적하여 학습률 조정 (뭔소린지 모르겠음)</li>
<li>SGD : batch 또는 데이터 샘플을 사용하여 기울기를 계산하고, 그 기울기로 파라미터 업데이트</li>
<li>SGD+momentum : 모멘텀을 추가하면, 이전 업데이트 값의 일정 비율을 가중치로 더하면서 업데이트를 한다.</li>
<br>

| Optimizer  | 학습률 조정 방식        | Momentum | 적합한 문제                         |
|-------------|-------------------------|-------------|-------------------------------------|
| SGD         | 고정 학습률             | 없음        | 단순한 데이터셋                    |
| Momentum    | 고정 학습률             | 사용        | 빠른 수렴이 필요한 경우            |
| Adagrad     | 파라미터마다 다른 학습률 | 없음        | 드문드문한(sparse) 특징 학습        |
| RMSprop     | 지수 이동 평균 사용     | 없음        | 비정규화된 데이터셋, RNN 등        |
| Adam        | RMSprop + Momentum      | 사용        | 대부분의 딥러닝 모델               |

## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = Intel(R) Iris(R) Xe Graphics </li>
<li> CPU = 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz, 2419Mhz, 4 코어, 8 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.001 </li>
<li> optimizer = Adam (Adagrad, RMSprop, SGD, SGD+Momentum 도 사용.) </li>



## MNIST데이터셋, Adam에서 lr = 0.01, 0.001, 0.0001 로 바꿔가며 돌려본 결과
<li>patience = 2로 주었더니, lr = 0.01에서와 lr = 0.001에서는 조기종료가 되었고, lr = 0.0001에서는 20epoch까지 모두 채웠다. val_accuracy는 lr = 0.0001일 때가 가장 높았다.</li>




## Evaluation
| Optimizer      | MNIST  | Cifar-10 |
|----------------|--------|----------|
| Adam           | 0.989  | 0.696    |
| Adagrad        | 0.879  | 0.495    |
| RMSprop        | 0.873  | 0.522    |
| SGD            | 0.806  | 0.404    |
| SGD+momentum   | 0.879  | 0.648    |

## Results
<li> 같은 구조를 가진 모델인데, MNIST와 Cifar-10에 Accuracy 차이가 너무 많이 났다. 그래서 Cifar-10에 대한 나의 모델이 잘못되었나 고민을 해보다가 VGG 라는 모델을 알게 되었는데, 이 VGG모델은 3x3 크기의 작은 커널을 엄청 사용하고, Convolution층도 매우 많고, Linear층을 3개정도 통과하는 아주 복잡한 모델이었다. 이런 모델로 Cifar-10을 학습했을 때 결과가 90%이상 나온다는 글을 보게 되었습니다. 그래서 내가 MNIST에서 얻은 그 Acc들은 이 Channel의 깊이가 1인 간단한 데이터셋이라서 높은 Acc를 얻은 것이었고, Cifar-10은 생각보다 어려운 데이터셋이라서 나의 간단한 모델로는 Acc가 낮게 나온다는 것을 알게 되었습니다. 또한 그런 글들을 보니 저는 Epoch 20을 기준으로 했는데 너무 낮은 수였다는것을 늦게 알았습니다.</li>
<img src='markdown/images/VGG_Cifar10.png'/>