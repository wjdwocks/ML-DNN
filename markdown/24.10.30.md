## 24.10.30 공부한 내용
<li> 여러 네트워크에 대해 공부하고, ???에 대한 논문 2편과 동영상 내용에 대해 공부하고, 이해하자. </li>
<li> VGG, Resnet - series, Wide Resnet, Mobile net v1/v2, Inception Net, Shuffle Net, DenseNet, MLP, FPNet </li>

## VGG Net
<li> 3x3 크기의 필터를 사용한 Convolution 층을 여러 개 쌓아 깊이 있는 네트워크를 구성함. </li>
<li> 이미지 처리에서 input특성을 3x3 kernel을 이용해 2~3개의 convolution 층을 통과하게 한 후, 2x2 Maxpooling을 통해서 크기를 줄여주고, 이를 VGG-16, 19등에 따라 여러 번 반복해준다.</li>
<li> 마지막으로 몇 개의 linear층을 통과시켜 주는 아주 간단한 모델. </li>

## ResNet
<li> Skip Connection : 신경망이 깊어질 수록 기울기 소실 문제 때문에, 학습이 재데로 이루어지지 않는다. 이를 위해 단차를 줄여주는 방식을 사용. </li>
<li> 기울기 소실 문제란, backward과정에서 기울기가 점점 작아져서, optimizer.step단계에서 가중치를 업데이트 하는데, 큰 의미를 주지 못하는 것을 의미함. </li>
<li> 각 Residual Block을 통과할 때 변환된 출력 값에 원래의 입력 값 x를 더해줌으로써 단차를 줄여줄 수 있다. (네트워크가 깊어져도 중요한 정보가 손실되지 않고, 각 층에 전달됨.) </li>
<li> 단차 : 이전 입력과 최종 출력 간의 차이 (정보의 차이.) </li>
<li> Batch Normalization : 각 층의 출력을 정규화하여, 데이터 분포가 일정하게 유지되게 함. - Exploding Gradient 문제를 해결할 수 있다 (역전파 과정에서 기울기가 너무 커져버리는 현상.) </li>

## ResNet - 18
<li>원본 사진의 크기가 224 x 224 라고 가정한다.</li>
<ul>
<li> 처음에는, Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3)을 통해 이미지의 크기 는 절반으로 줄어들고, 깊이는 64가 됨. </li>
<li> MaxPooling(kernel_size=(3, 3), stride=2, padding=1)로 깊이는 그대로, 이미지 크기는 절반으로 더 줄여줌. </li>
<li> 아레의 4개의 Residual Stage를 통과함. </li>
<li> 첫 convolution층을 보내기 전에 maxpooling을 통해  </li>
<li> 다음에 4개의 Residual Stage를 지난다. </li>
<li> 각각의 Residual Stage에는 2개의 Residual Block이 있다. </li>
<li> 각각의 Residual Block은 다음과 같이 구성된다. </li>
<ol>
<li> out = conv(x) # x를 유지해주기 위해 out이라는 새로운 변수 사용 </li>
<li> out = bn(out) </li>
<li> out = relu(out) </li>
<li> out = conv(out) </li>
<li> out = bn(out) </li>
<li> out = out + x # 잔차를 해결해주기 위해 x를 더해줌. </li>
<li> out = relu(out) </li>
</ol>
<li> 모든 Residual Stage의 첫 번째 블록, 첫 번째 Convolution 층에서는 Strides=2, out_channels=in_channels*2를 해주어, 각 Stage 당 특성 맵의 크기는 반, 깊이는 2배가 되도록 조정한다. </li>
<li> Fully Connected 층과 연결되어 최종 출력을 한다. </li>
<li>Resnet-18 동작 예시.</li>
</ul>

![Resnet-18](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_18.png)



## ResNet - 34
<li>원본 사진의 크기가 224 x 224 라고 가정한다.</li>
<ul>
<li> 첫 Convolution 층 : - Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3)을 통해 이미지의 크기는 절반으로 줄어들고, 깊이는 64로 바꾼다. </li> 
<li> Max Pooling : - MaxPooling(kernel_size=3x3, stride=2, padding=1)을 통해 깊이는 그대로 유지하고, 이미지의 가로 세로 크기만 절반으로 줄입니다. </li>
<li> 4개의 Residual Stage를 통과합니다. </li> 
<li> 각 Residual Stage 구성: - Stage는 총 4개로 이루어지며, 각 Stage는 서로 다른 수의 Residual Block을 포함한다.</li>
<li> Stage 1: 3개의 Residual Block (채널 수 64) - 6개의 Conv층 </li>
<li> Stage 2: 4개의 Residual Block (채널 수 128) - 8개의 Conv층 </li>
<li> Stage 3: 6개의 Residual Block (채널 수 256) - 12개의 Conv층 </li>
<li> Stage 4: 3개의 Residual Block (채널 수 512) - 6개의 Conv층 </li> 
<li> 각 Stage마다 특성 맵의 크기를 절반으로 줄이고 깊이를 두 배로 늘리기 위해, 첫 번째 Residual Block의 첫 번째 Convolution 층에서 stride=2와 out_channels=in_channels*2를 적용한다.. </li> 
<li> 각 Residual Block 구성: - 각각의 Residual Block은 두 개의 Convolution 레이어로 구성되며, 다음과 같은 구조를 가진다.
<ol> 
<li> out = conv(x) </li> 
<li> out = bn(out) </li> 
<li> out = relu(out) </li> 
<li> out = conv(out) </li> 
<li> out = bn(out) </li> 
<li> out = out + x </li> 
<li> out = relu(out) </li> 
</ol> 
</li> 
<li> 각 Stage에서 다운샘플링: - 모든 Residual Stage의 첫 번째 Block의 첫 번째 Convolution 층에서만 `stride=2`와 `out_channels=in_channels*2`로 설정하여 특성 맵 크기를 줄이고 깊이를 증가시킵니다. </li>
<li> Fully Connected 층: - 마지막에 Global Average Pooling을 사용하여 특성 맵을 축소한다. (512, ?, ?)의 크기를 (512, 1, 1)의 크기로 바꾸어준다. (?, ?)는 원래 이미지의 크기에 비례함. 이 ? x ? 영역을 평균하여 바꾸주는 것임. </li>
<li> Fully Connected 층과 연결하여 최종 출력을 생성한다.</li> 
</ul>

![Resnet-34](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_34.png)

## ResNet - 50
<li> 원본 사진의 크기가 224 x 224 라고 가정한다.</li>
<li> Resnet 50부터는 Residual Block의 구조가 달라진다. </li>
<ul> 
<li> 초기 Convolution 층: - Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3) 으로 똑같다. </li> 
<li> Max Pooling: - MaxPooling(kernel_size=3x3, stride=2, padding=1)을 통해 깊이는 그대로 유지하고, 이미지의 가로 세로 크기만 절반으로 줄인다. (똑같음) </li>
<li> 4개의 Residual Stage를 통과한다. </li> 
<li> 각 Residual Stage 구성: - ResNet-50에서는 Bottleneck 구조를 사용하여 Residual Block이 3개의 Convolution 레이어로 구성됩니다. </li> 
<li> Stage 1: 3개의 Bottleneck Residual Block (채널 수 256) - 9개의 conv층 (이미지 : 56, 56) </li>
<li> Stage 2: 4개의 Bottleneck Residual Block (채널 수 512) - 12개의 conv층 (이미지 : 28, 28)</li>
<li> Stage 3: 6개의 Bottleneck Residual Block (채널 수 1024) - 18개의 conv층 (이미지 : 14, 14)</li>
<li> Stage 4: 3개의 Bottleneck Residual Block (채널 수 2048) - 9개의 conv층 (이미지 : 7, 7)</li> 
<li> 각 Stage마다 특성 맵의 크기를 절반으로 줄이고 깊이를 두 배로 늘리기 위해, 첫 번째 Bottleneck Block의 첫 번째 Convolution 층에서 stride=2와 out_channels=in_channels*2를 적용합니다. </li> 
<li> Bottleneck Residual Block 구성: - 각각의 Bottleneck Residual Block은 다음과 같은 순서로 3개의 Convolution 레이어와 Batch Normalization으로 구성됩니다: 
<ol> 
<li> out = conv(x, output=1/4 * out_channels, kernel_size=(1,1)) # 첫 번째 1x1 Conv로 채널 수를 줄임</li> 
<li> out = bn(out)</li> 
<li> out = relu(out)</li> 
<li> out = conv(out, output=1/4 * out_channels, kernel_size=(3,3)) # 3x3 Conv로 특징 학습</li> 
<li> out = bn(out)</li> 
<li> out = relu(out)</li> 
<li> out = conv(out, output=out_channels, , kernel_size=(1,1)) # 마지막 1x1 Conv로 채널 수를 원래대로 복원</li> 
<li> out = bn(out)</li> 
<li> out = out + x # 잔차 연결을 위해 입력 x를 더해줌</li> 
<li> out = relu(out)</li> 
</ol> 
</li> 
<li> 첫 번째 Residual Stage의 첫 번째 Residual Block은 이전 출력을 유지. </li>
<li> 각 Stage에서 다운샘플링 : 모든 Residual Stage의 첫 번째 Block의 첫 번째 Convolution 층에서만 `stride=2`와 `out_channels=in_channels*2`로 설정하여 특성 맵 크기를 줄이고 깊이를 증가시킵니다. 두 번째 스테이지부터 적용. </li> 
<li> Global Average Pooling: - 마지막 Residual Stage의 출력을 받아 Global Average Pooling을 통해 (2048, 1, 1) 크기의 출력을 얻습니다. </li> 
<li> Fully Connected 층: - GAP 층의 출력을 평탄화하여 1차원 벡터로 변환한 후, 마지막 Fully Connected 층에 연결되어 최종 클래스 확률을 예측합니다. </li> 
</ul>

![Resnet-50](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_50.png)

## ResNet - 101, 152
<ul>
<li> Resnet 50에서 같은 Bottleneck Residual Block을 사용하고, 각 스테이지 당 블록의 개수만 달라진다.
</ul>

## Wide ResNet
<li> 32x32 이미지의 예시 </li>
<li> Resnet-101, 152와 같은 깊은 네트워크는 학습과 연산량이 매우 큰 것에 비해 별로 효율적이지 않기 때문에 Wide Resnet이라는게 등장함. </li>
<li> 무조건적으로 깊이를 늘리기보다, 네트워크의 너비(채널 수)를 늘리는 것이 연산 효율성과 성능 향상 측면에서 더 효과적이라는 것. </li>
<ul>
<li> 처음에 Convolution(input, out_channels=16, kernel_size=3x3, padding=1)을 통해 초기 특징을 추출함. (이미지 크기는 유지. (32x32) ), 원본 이미지가 커질 수록 out_channels는 늘어남. </li>
<li> 다음에 Resnet과 다르게 Maxpooling이 적용되지 않는다. </li>
<li> 3개의 Residual Stage를 통과한다. </li>
<li> Wide Residual Network는 각 Stage에 포함된 채널의 수가 확장 배수를 곱하여 늘어난다. ex) Wide Resnet-28-10 에서 10이 확장배수. </li>
<li> Stage 1: 4개의 Residual Block (out_channels = 160) - 8개의 Conv층 이미지 : 32x32</li>
<li> Stage 2: 4개의 Residual Block (out_channels = 320) - 8개의 Conv층 이미지 : 16x16</li>
<li> Stage 3: 4개의 Residual Block (out_channels = 640) - 8개의 Conv층 이미지 : 8x8</li>
<li> 각 Residual Block은 아레와 같이 구성되어 있음. (기본적으로 Resnet-18의 Residual Block과 똑같이 생김.)</li>
<ol> 
<li> out = conv(x, output=확장된 채널 수, kernel_size=3x3)</li> 
<li> out = bn(out)</li> 
<li> out = relu(out)</li> 
<li> out = conv(out, output=확장된 채널 수, kernel_size = 3x3)</li> 
<li> out = bn(out)</li> 
<li> out = out + x # 잔차 연결을 위해 입력 x를 더해줌</li> 
<li> out = relu(out)</li> 
</ol>
<li> 이것도 마찬가지로, 각 Residual Stage의 첫 번째 Block의 첫 번째 층의 Stride = 2, out_channels=2*in_channels로 하여 특성맵은 반, 깊이는 두배로 늘려준다.(첫 번째 Stage 제외.) </li>
<li>GAP를 이용하여 (640, 1, 1)의 크기로 변환한 후 FC층과 연결해 최종 출력을 받는다.</li>
</ul>

	요약 : 그냥 Stage가 3개로 줄고, 첫 Conv층에서 Residual Stage로 넘어갈 때 Maxpooling이 사라지고, out_channels를 확장배수 만큼 늘어났다고 생각하자. 나머지는 Resnet-18과 똑같음. 내부 구조는.


## 각 Optimizer 비교
<li>Adam : SGD와, Momentum, RMSprop, Adagrad의 개념을 결합한 옵티마이저임.</li>
<li>Adagrad : 각 파라미터마다 학습률을 개별적으로 조정하여 학습 속도를 최적화한다.</li>
<li>RMSprop : 기울기 제곱에 대한 지수 이동 평균(EMA)를 기울기의 2차 모멘트를 추적하여 학습률 조정 (뭔소린지 모르겠음)</li>
<li>SGD : batch 또는 데이터 샘플을 사용하여 기울기를 계산하고, 그 기울기로 파라미터 업데이트</li>
<li>SGD+momentum : 모멘텀을 추가하면, 이전 업데이트 값의 일정 비율을 가중치로 더하면서 업데이트를 한다.</li>
<br>

| Optimizer  | 학습률 조정 방식        | Momentum | 적합한 문제                         |
|-------------|-------------------------|-------------|-------------------------------------|
| SGD         | 고정 학습률             | 없음        | 단순한 데이터셋                    |
| Momentum    | 고정 학습률             | 사용        | 빠른 수렴이 필요한 경우            |
| Adagrad     | 파라미터마다 다른 학습률 | 없음        | 드문드문한(sparse) 특징 학습        |
| RMSprop     | 지수 이동 평균 사용     | 없음        | 비정규화된 데이터셋, RNN 등        |
| Adam        | RMSprop + Momentum      | 사용        | 대부분의 딥러닝 모델               |

## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = Intel(R) Iris(R) Xe Graphics </li>
<li> CPU = 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz, 2419Mhz, 4 코어, 8 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.001 </li>
<li> optimizer = Adam (Adagrad, RMSprop, SGD, SGD+Momentum 도 사용.) </li>



## MNIST데이터셋, Adam에서 lr = 0.01, 0.001, 0.0001 로 바꿔가며 돌려본 결과
<li>patience = 2로 주었더니, lr = 0.01에서와 lr = 0.001에서는 조기종료가 되었고, lr = 0.0001에서는 20epoch까지 모두 채웠다. val_accuracy는 lr = 0.0001일 때가 가장 높았다.</li>




## Evaluation
| Optimizer      | MNIST  | Cifar-10 |
|----------------|--------|----------|
| Adam           | 0.989  | 0.696    |
| Adagrad        | 0.879  | 0.495    |
| RMSprop        | 0.873  | 0.522    |
| SGD            | 0.806  | 0.404    |
| SGD+momentum   | 0.879  | 0.648    |

## Results
<li> 같은 구조를 가진 모델인데, MNIST와 Cifar-10에 Accuracy 차이가 너무 많이 났다. 그래서 Cifar-10에 대한 나의 모델이 잘못되었나 고민을 해보다가 VGG 라는 모델을 알게 되었는데, 이 VGG모델은 3x3 크기의 작은 커널을 엄청 사용하고, Convolution층도 매우 많고, Linear층을 3개정도 통과하는 아주 복잡한 모델이었다. 이런 모델로 Cifar-10을 학습했을 때 결과가 90%이상 나온다는 글을 보게 되었습니다. 그래서 내가 MNIST에서 얻은 그 Acc들은 이 Channel의 깊이가 1인 간단한 데이터셋이라서 높은 Acc를 얻은 것이었고, Cifar-10은 생각보다 어려운 데이터셋이라서 나의 간단한 모델로는 Acc가 낮게 나온다는 것을 알게 되었습니다. 또한 그런 글들을 보니 저는 Epoch 20을 기준으로 했는데 너무 낮은 수였다는것을 늦게 알았습니다.</li>
<img src='markdown/images/VGG_Cifar10.png'/>