## 24.10.30 공부한 내용
<li> 여러 네트워크에 대해 공부하고, ???에 대한 논문 2편과 동영상 내용에 대해 공부하고, 이해하자. </li>
<li> VGG, Resnet - series, Wide Resnet, Mobile net v1/v2, Inception Net, Shuffle Net, DenseNet, MLP, FPNet </li>

## VGG Net
<li> 3x3 크기의 필터를 사용한 Convolution 층을 여러 개 쌓아 깊이 있는 네트워크를 구성함. </li>
<li> 이미지 처리에서 input특성을 3x3 kernel을 이용해 2~3개의 convolution 층을 통과하게 한 후, 2x2 Maxpooling을 통해서 크기를 줄여주고, 이를 VGG-16, 19등에 따라 여러 번 반복해준다.</li>
<li> 마지막으로 몇 개의 linear층을 통과시켜 주는 아주 간단한 모델. </li>

## ResNet
<li> Convolution 층을 아주 깊게 하는데, 이때 발생하는 문제를, Residual block을 통해 해결함.</li>
<li> 아무런 테크닉 없이 깊이만 엄청 늘린다면, 기울기 소실문제나 과적합이 발생해서 오히려 성능이 나빠짐.</li>
<li> 기울기 소실 문제란, backward과정에서 기울기가 점점 작아져서, optimizer.step단계에서 가중치를 업데이트 하는데, 큰 의미를 주지 못하는 것을 의미함. </li>
<li> 해결 방법 : 2개층 마다 원래의 input값인 x를 더해준다. </li>
<li> 즉, x -> conv(x) -> conv(x) + x -> Relu -> x' 를 여러 번 반복한다. </li>
<li> 다음 번에는 x' -> conv(x') -> conv(x') + x' -> Relu 이런식으로 각각의 Residual Block을 통과한다. </li>

## ResNet - 18
<ul>
<li> 처음에는, Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3)을 통해 이미지의 크기 는 절반으로 줄어들고, 깊이는 64가 됨. </li>
<li> MaxPooling(kernel_size=(3, 3), stride=2, padding=1)로 깊이는 그대로, 이미지 크기는 절반으로 더 줄여줌. </li>
<li> 아레의 4개의 Residual Stage를 통과함. </li>
<li> 첫 convolution층을 보내기 전에 maxpooling을 통해  </li>
<li> 다음에 4개의 Residual Stage를 지난다. </li>
<li> 각각의 Residual Stage에는 2개의 Residual Block이 있다. </li>
<li> 각각의 Residual Block은 다음과 같이 구성된다. </li>
<ol>
<li> out = conv(x) # x를 유지해주기 위해 out이라는 새로운 변수 사용 </li>
<li> out = bn(out) </li>
<li> out = relu(out) </li>
<li> out = conv(out) </li>
<li> out = bn(out) </li>
<li> out = out + x # 잔차를 해결해주기 위해 x를 더해줌. </li>
<li> out = relu(out) </li>
</ol>
<li> 모든 Residual Stage의 첫 번째 블록, 첫 번째 Convolution 층에서는 Strides=2, out_channels=in_channels*2를 해주어, 각 Stage 당 특성 맵의 크기는 반, 깊이는 2배가 되도록 조정한다. </li>
<li> Fully Connected 층과 연결되어 최종 출력을 한다. </li>
<li>Resnet-18 동작 예시.</li>
</ul>

![Resnet-18](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_18.png)



## ResNet - 34
<ul>
<li> 첫 Convolution 층 : - Convolution(input, output=64, kernel_size=7x7, stride=2, padding=3)을 통해 이미지의 크기는 절반으로 줄어들고, 깊이는 64로 바꾼다. </li> 
<li> Max Pooling : - MaxPooling(kernel_size=3x3, stride=2, padding=1)을 통해 깊이는 그대로 유지하고, 이미지의 가로 세로 크기만 절반으로 줄입니다. </li>
<li> 4개의 Residual Stage를 통과합니다. </li> 
<li> 각 Residual Stage 구성: - Stage는 총 4개로 이루어지며, 각 Stage는 서로 다른 수의 Residual Block을 포함한다.</li>
<li> Stage 1: 3개의 Residual Block (채널 수 64) - 6개의 Conv층 </li>
<li> Stage 2: 4개의 Residual Block (채널 수 128) - 8개의 Conv층 </li>
<li> Stage 3: 6개의 Residual Block (채널 수 256) - 12개의 Conv층 </li>
<li> Stage 4: 3개의 Residual Block (채널 수 512) - 6개의 Conv층 </li> 
<li> 각 Stage마다 특성 맵의 크기를 절반으로 줄이고 깊이를 두 배로 늘리기 위해, 첫 번째 Residual Block의 첫 번째 Convolution 층에서 stride=2와 out_channels=in_channels*2를 적용한다.. </li> 
<li> 각 Residual Block 구성: - 각각의 Residual Block은 두 개의 Convolution 레이어로 구성되며, 다음과 같은 구조를 가진다.
<ol> 
<li> out = conv(x) </li> 
<li> out = bn(out) </li> 
<li> out = relu(out) </li> 
<li> out = conv(out) </li> 
<li> out = bn(out) </li> 
<li> out = out + x </li> 
<li> out = relu(out) </li> 
</ol> 
</li> 
<li> 각 Stage에서 다운샘플링: - 모든 Residual Stage의 첫 번째 Block의 첫 번째 Convolution 층에서만 `stride=2`와 `out_channels=in_channels*2`로 설정하여 특성 맵 크기를 줄이고 깊이를 증가시킵니다. </li>
<li> Fully Connected 층: - 마지막에 Global Average Pooling을 사용하여 특성 맵을 축소한다. (512, ?, ?)의 크기를 (512, 1, 1)의 크기로 바꾸어준다. (?, ?)는 원래 이미지의 크기에 비례함. 이 ? x ? 영역을 평균하여 바꾸주는 것임. </li>
<li> Fully Connected 층과 연결하여 최종 출력을 생성한다.</li> 
</ul>

![Resnet-34](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_34.png)

## ResNet - 50
<ul>
<li> </li>
<li> </li>
<li> </li>
<li> </li>
</ul>

![Resnet-50](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.10.30/Resnet_50.png)

## ResNet - 101, 152
<li> </li>
<li> </li>
<li> </li>
<li> </li>


## 각 Optimizer 비교
<li>Adam : SGD와, Momentum, RMSprop, Adagrad의 개념을 결합한 옵티마이저임.</li>
<li>Adagrad : 각 파라미터마다 학습률을 개별적으로 조정하여 학습 속도를 최적화한다.</li>
<li>RMSprop : 기울기 제곱에 대한 지수 이동 평균(EMA)를 기울기의 2차 모멘트를 추적하여 학습률 조정 (뭔소린지 모르겠음)</li>
<li>SGD : batch 또는 데이터 샘플을 사용하여 기울기를 계산하고, 그 기울기로 파라미터 업데이트</li>
<li>SGD+momentum : 모멘텀을 추가하면, 이전 업데이트 값의 일정 비율을 가중치로 더하면서 업데이트를 한다.</li>
<br>

| Optimizer  | 학습률 조정 방식        | Momentum | 적합한 문제                         |
|-------------|-------------------------|-------------|-------------------------------------|
| SGD         | 고정 학습률             | 없음        | 단순한 데이터셋                    |
| Momentum    | 고정 학습률             | 사용        | 빠른 수렴이 필요한 경우            |
| Adagrad     | 파라미터마다 다른 학습률 | 없음        | 드문드문한(sparse) 특징 학습        |
| RMSprop     | 지수 이동 평균 사용     | 없음        | 비정규화된 데이터셋, RNN 등        |
| Adam        | RMSprop + Momentum      | 사용        | 대부분의 딥러닝 모델               |

## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = Intel(R) Iris(R) Xe Graphics </li>
<li> CPU = 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz, 2419Mhz, 4 코어, 8 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.001 </li>
<li> optimizer = Adam (Adagrad, RMSprop, SGD, SGD+Momentum 도 사용.) </li>



## MNIST데이터셋, Adam에서 lr = 0.01, 0.001, 0.0001 로 바꿔가며 돌려본 결과
<li>patience = 2로 주었더니, lr = 0.01에서와 lr = 0.001에서는 조기종료가 되었고, lr = 0.0001에서는 20epoch까지 모두 채웠다. val_accuracy는 lr = 0.0001일 때가 가장 높았다.</li>




## Evaluation
| Optimizer      | MNIST  | Cifar-10 |
|----------------|--------|----------|
| Adam           | 0.989  | 0.696    |
| Adagrad        | 0.879  | 0.495    |
| RMSprop        | 0.873  | 0.522    |
| SGD            | 0.806  | 0.404    |
| SGD+momentum   | 0.879  | 0.648    |

## Results
<li> 같은 구조를 가진 모델인데, MNIST와 Cifar-10에 Accuracy 차이가 너무 많이 났다. 그래서 Cifar-10에 대한 나의 모델이 잘못되었나 고민을 해보다가 VGG 라는 모델을 알게 되었는데, 이 VGG모델은 3x3 크기의 작은 커널을 엄청 사용하고, Convolution층도 매우 많고, Linear층을 3개정도 통과하는 아주 복잡한 모델이었다. 이런 모델로 Cifar-10을 학습했을 때 결과가 90%이상 나온다는 글을 보게 되었습니다. 그래서 내가 MNIST에서 얻은 그 Acc들은 이 Channel의 깊이가 1인 간단한 데이터셋이라서 높은 Acc를 얻은 것이었고, Cifar-10은 생각보다 어려운 데이터셋이라서 나의 간단한 모델로는 Acc가 낮게 나온다는 것을 알게 되었습니다. 또한 그런 글들을 보니 저는 Epoch 20을 기준으로 했는데 너무 낮은 수였다는것을 늦게 알았습니다.</li>
<img src='markdown/images/VGG_Cifar10.png'/>