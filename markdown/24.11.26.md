## 24.11.13 공부할 내용
<li> 첫 번째 논문 : Leveraging Topological Guidance for Improved Knowledge Distillation </li>
<li> 두 번째 논문 : Topological Persistence Guided Knowledge Distillation for Wearable Sensor Data </li>

## 첫 번째 논문
### Abstract 정리
<li> 이미지 분류 작업에서 복잡하고, 노이즈가 많은 데이터에서는 TDA를 활용해서 위상적 종보를 통해 성능과 견고성(robustness)을 개선할 수 있다. </li>
<li> 근데 소형 기기에서는 TDA의 계산량을 버틸 수가 없음. </li>
<li> 이 논문에서는 TGD(Topological Guidance-based Knowledge Distillation)이라는 프레임워크를 제안한다. </li>
<li> TGD는 TDA 기반의 topological information을 Knowledge Distillation에 확용하여 경량 모델을 학습시킨다. </li>
<li> 1. 경량 모델 학습 : KD를 통해 성능이 우수한 경량 모델 학습. </li>
<li> 2. 다수의 교사 모델 활용 : 여러 교사 모델로부터 위상 정보를 동시에 학습. </li>
<li> 3. 교사-학생 간 지식 격차 해소 : 교사 간 및 교사-학생 간의 지식 격차를 줄이기 위한 통합 메커니즘 제안. </li>

### Introduction 정리
<li> 연구 배경 및 문제점 </li>
<ul>
<li> 딥러닝은 컴퓨터 비전 작업에서 유용한 특징을 추출하고 복잡한 문제를 해결하는 데 있어 큰 성과를 보여줌. </li>
<li> 실제 데이터는 구조적으로 복잡하고, 노이즈가 많아 학습 및 일반화 성능을 떨어뜨리는 경우가 많다. </li>
<li> TDA는 데이터의 위상적 구조를 분석하여 복잡한 데이터를 효과적으로 이해할 수 있도록 도와준다. (TDA를 기반으로 데이터의 위상적 특징을 PI로 표현을 한다.) </li>
<li> TDA는 계산량이 많아 소형 디바이스나 자원 제한이 있는 환경에서 적용하기 어렵다. </li>
</ul>
<li> 연구 동기 </li>
<ul>
<li> TDA의 강점 활용 : 위상적 특징은 데이터의 구조를 더 잘 표현하고, 기존 딥러닝 모델이 놓칠 수 있는 정보를 보완할 가능성이 있다. </li>
<li> 소형 디바이스에 적용 : 직접적으로 TDA를 계산하는 것이 불가능하기에 이를 간접적으로 학습시키는 방법이 필요하다. </li>
<li> Knowledge Distillation(KD)의 가능성 : 복잡한 Teacher 모델의 지식을 경량화된 Student모델에 전달하는 기법을 통해 위의 일들을 해결할 수 있을 것이다. </li>
</ul>
<li> 제안 방법 </li>
<ul>
<li> TGD라는 것을 제시한다. </li>
<li> TDA를 활용해 추출된 위상적 특징(Persistence Image)를 Teacher 모델이 학습함. </li>
<li> Student 모델은 Teacher 모델의 지식을 KD로 학습하고, 위상적 정보를 간접적으로 학습을 한 것이 됨. </li>
<li> Teacher1은 원본 이미지로 학습, Teacher2는 TDA로 얻은 PI를 기반으로 학습함. </li>
<li> 위 두 Teacher를 통해 소형 디바이스에서 위상적 특징의 장점을 활용하면서, 경량 모델의 성능을 극대화할 수 있다. </li>
</ul>
<li> 주요 기여 </li>
<ul>
<li> TDA 기반 정보 활용의 간소화 : TDA의 높은 계산량 문제를 해결하고, 위상적 특징을 효과적으로 전달할 수 있다. </li>
<li> 다중 Teacher를 활용하여 서로 다른 데이터 표현(PI, 원본)을 학습시킨다. </li>
<li> 경량 모델의 실용성 증대 : 소형 디바이스에서도 높은 성능을 유지하며, TDA의 장점을 활용 가능. </li>
</ul>
<li> TGD의 Loss </li>
<ul>
<li> 1. Student Logits Loss(CE) </li>
<li> 2. KL-Divergence Loss </li>
<li> 3. Similarity Loss : Teacher1과 Teacher2 간의 유사성 정보를 계산하여 두 Teacher로부터 유용한 정보를 모두 증류받는 것이 목표인듯. </li>
<li>  </li>
</ul>



## Trainning Environment
<li> Dataset = Cifar10, CINIC10, Tiny ImageNet </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1 + CUDA ??? </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results