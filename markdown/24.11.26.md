## 24.11.26 공부할 내용
<li> 첫 번째 논문 : Leveraging Topological Guidance for Improved Knowledge Distillation </li>
<li> 두 번째 논문 : Topological Persistence Guided Knowledge Distillation for Wearable Sensor Data </li>

## 첫 번째 논문
### Abstract 정리
<li> 이미지 분류 작업에서 복잡하고, 노이즈가 많은 데이터에서는 TDA를 활용해서 위상적 종보를 통해 성능과 견고성(robustness)을 개선할 수 있다. </li>
<li> 근데 소형 기기에서는 TDA의 계산량을 버틸 수가 없음. </li>
<li> 이 논문에서는 TGD(Topological Guidance-based Knowledge Distillation)이라는 프레임워크를 제안한다. </li>
<li> TGD는 TDA 기반의 topological information을 Knowledge Distillation에 확용하여 경량 모델을 학습시킨다. </li>
<li> 1. 경량 모델 학습 : KD를 통해 성능이 우수한 경량 모델 학습. </li>
<li> 2. 다수의 교사 모델 활용 : 여러 교사 모델로부터 위상 정보를 동시에 학습. </li>
<li> 3. 교사-학생 간 지식 격차 해소 : 교사 간 및 교사-학생 간의 지식 격차를 줄이기 위한 통합 메커니즘 제안. </li>

### Introduction 정리
<li> 연구 배경 및 문제점 </li>
<ul>
<li> 딥러닝은 컴퓨터 비전 작업에서 유용한 특징을 추출하고 복잡한 문제를 해결하는 데 있어 큰 성과를 보여줌. </li>
<li> 실제 데이터는 구조적으로 복잡하고, 노이즈가 많아 학습 및 일반화 성능을 떨어뜨리는 경우가 많다. </li>
<li> TDA는 데이터의 위상적 구조를 분석하여 복잡한 데이터를 효과적으로 이해할 수 있도록 도와준다. (TDA를 기반으로 데이터의 위상적 특징을 PI로 표현을 한다.) </li>
<li> TDA는 계산량이 많아 소형 디바이스나 자원 제한이 있는 환경에서 적용하기 어렵다. </li>
</ul>
<li> 연구 동기 </li>
<ul>
<li> TDA의 강점 활용 : 위상적 특징은 데이터의 구조를 더 잘 표현하고, 기존 딥러닝 모델이 놓칠 수 있는 정보를 보완할 가능성이 있다. </li>
<li> 소형 디바이스에 적용 : 직접적으로 TDA를 계산하는 것이 불가능하기에 이를 간접적으로 학습시키는 방법이 필요하다. </li>
<li> Knowledge Distillation(KD)의 가능성 : 복잡한 Teacher 모델의 지식을 경량화된 Student모델에 전달하는 기법을 통해 위의 일들을 해결할 수 있을 것이다. </li>
</ul>
<li> 제안 방법 </li>
<ul>
<li> TGD라는 것을 제시한다. </li>
<li> TDA를 활용해 추출된 위상적 특징(Persistence Image)를 Teacher 모델이 학습함. </li>
<li> Student 모델은 Teacher 모델의 지식을 KD로 학습하고, 위상적 정보를 간접적으로 학습을 한 것이 됨. </li>
<li> Teacher1은 원본 이미지로 학습, Teacher2는 TDA로 얻은 PI를 기반으로 학습함. </li>
<li> 위 두 Teacher를 통해 소형 디바이스에서 위상적 특징의 장점을 활용하면서, 경량 모델의 성능을 극대화할 수 있다. </li>
</ul>
<li> 주요 기여 </li>
<ul>
<li> TDA 기반 정보 활용의 간소화 : TDA의 높은 계산량 문제를 해결하고, 위상적 특징을 효과적으로 전달할 수 있다. </li>
<li> 다중 Teacher를 활용하여 서로 다른 데이터 표현(PI, 원본)을 학습시킨다. </li>
<li> 경량 모델의 실용성 증대 : 소형 디바이스에서도 높은 성능을 유지하며, TDA의 장점을 활용 가능. </li>
</ul>
<li> TGD의 Loss </li>
<ul>
<li> 1. Student Logits Loss(CE) </li>
<li> 2. KL-Divergence Loss </li>
<ul>
<li> 이 논문에서의 KL Loss는 다중 교사를 사용하기 때문에 a(KL(T1, S)) + (1-a)KL(T2, S)를 사용함. </li>
</ul>
<li> 3. Similarity Loss : Teacher1과 Teacher2 간의 유사성 정보를 계산하여 두 Teacher로부터 유용한 정보를 모두 증류받는 것이 목표인듯. </li>
</ul>
<li> Similarity Loss에 관하여 </li>
<ul>
<li> 이 Similarity Loss는 다중 교사를 사용한 지식 증류 기법에서 여러 교사의 지식을 모두 전달하기 위한 방법이다. </li>
<li> 이 논문에서 Similarity Loss는 중간층에서 얻은 Feature map을 이용하여 계산하는 것 같다. </li>
<li> 우선 두 교사 모델의 중간층(같은 부분)의 Feature map을 얻어와서 가중합을 통한 새로운 Feature Map을 생성한다. </li>
<li> 이후, 학생 모델의 중간층에서도 Feature Map을 얻어와서 두 Feature Map을 행렬로 표현함. </li>
<li> |두 행렬의 차|^2을 통해 두 중간층 feature map이 최대한 비슷해지도록 유도한다. </li>
</ul>

## 두 번째 논문
### Abstract 정리
<li> 센서 데이터에서 TDA를 통한 Robust한 Feature를 사용하기 힘든 이유 </li>
<ol>
<li> TDA를 계산하는데 드는 엄청난 비용 </li>
<li> 딥러닝과 TDA에서 얻는 신호 표현 방식이 달라서 이들을 융합하는 것이 어렵다. </li>
</ol>
<li> 이 논문에서 제안하는 방법 </li>
<ol>
<li> 첫 번째 교사 네트워크 : row time-series 데이터를 학습함. </li>
<li> 두 번째 교사 네트워크 : TDA를 통해 얻어진 PI를 학습함. </li>
</ol>
<li> 이 방법의 이점(기대 효과?) </li>
<ul>
<li> 상호 보완정 정보 활용 : 여러 교사로부터 얻은 정보를 결합하여 풍부한 표현력을 가진 compact한 모델을 생성함. </li>
<li> 새로운 제약 조건 도입 : feature correlation map에 대한 직교성 제약을 적용하여 특징 표현력을 향상시키고, 학생 모델이 교사로부터 쉽게 학습할 수 있게 함. </li>
<li> 어닐링 전략 적용 : KD과정에서 Annealing(하이퍼 파라미터 점진적 조정)을 통해 빠른 수렴과 안정적인 학습을 가능하게 한다. </li>
</ul>

### Introduction ~ Proposed Method 정리
<li> Sensor Data의 Robustness를 위하여 여기서도 두 교사 모델을 이용한 TGD를 제안한다. </li>
<li> 여기서도 마찬가지로, 두 교사 모델에서 중간층의 Feature map을 추출하여 합친 것과 학생 모델의 중간층에서 나온 Feature Map을 비교하려고 했다. </li>
<li> 그런데 Seonsor Data는 시계열 신호로, 개별 Feature Map의 값보다는 Feature 간의 관계가 더 중요한 경우가 많기 때문에, 중간 Layer의 Feature Correlation Map을 사용한다. </li>
<li> Sensor Data의 세 번째 Loss항은 Image와 애초부터 다르다. </li>
<li> Sensor Data는 각 특성 맵 자체가 중요한 것이 아닌, Feature들 간의 관계가 중요하기 때문에 Similarity Map(내부 특징들 간의 유사성)을 얻어온다. </li>
<li> 이렇게 얻은 두 교사의 Similarity Map을 합친다. </li>
<li> 합쳐진 Similarity Map을 k개의 map으로 쪼갠다.(고차원의 정보를 더 작은 단위로 나누어 처리하기 위함?) </li>
<li> K개의 Map을 더 작은 단위(Patch)로 분할한다. </li>
<li> Patch별로 직교성 제약(OF)를 적용하여 각 Patch가 독립적이고, 명확한 정보를 갖도록 변환한다. </li>
<li> 그래서 Orthogonality Features(GT)와 학생 모델의 Similarity Map을 Patch한 후 만들어진 Orthogonality Features(GS)를 비교하는 loss항을 만들게 됨. </li>

### Background - Topological Feature Extraction
<li> 데이터를 구성하는 '모양'에 대한 새로운 통찰을 제공함. </li>
<li> 데이터의 형상(Structure)를 파악하는 데 유용한 특징들을 제공한다. </li>
<li> 주요 알고리즘 - Persistent Homology </li>
<li> 고차원 공간에서 나타나는 데이터의 점, 간선, 삼각형 등 구조를 동적 threshold 과정을 통해 분석함 - Filtration </li>
<li> 이 과정에서 점들의 생성과 죽음의 시점을 Diagram형태로 만들면 그것이 PD가 된다. </li>
<li> PI는 PD의 점들을 Persistence Surface(R -> R^2)로 투영한다.(PD의 각 점에서 Weighted sum of Gaussian Function을 통해) </li>
<li> 그렇게 만들어진 PS를 Grid로 나누어서 각 그리드 내의 function을 적분하여 픽셀값으로 나타낼 수 있게 Discretize(이산화)한다. </li>
<li> 이렇게 만들어진 값이 각 grid당의 pixel value를 하여 PI가 만들어진다. </li>

### Background - Simulated Annealing
<li> 기존의 Simulated Annealing : 최적화 문제를 해결하기 위해 개발된 기법으로, 특정 문제에서 탐색 공간을 점차 줄여가며 빠르게 최적해를 찾는 기법. </li>
<li> KD에서의 SA : 지식 증류에서 교사 모델과 학생 모델 사이의 지식 격차를 줄이기 위해 적용된다. </li>
<li> SA의 두 단계 </li>
<li> 초기 단계 </li>
<ol>
<li> 초기 epoch에서 학생 모델은 교사 모델의 출력(logits)에 의존해 학습한다.  </li>
<li> KD의 온도가 점차 감소하면서, 교사와 학생 모델의 출력 간의 차이가 줄어들도록 유도한다. </li>
<li> 초기 단계에서는 학생 모델은 교사 모델의 출력 logit을 보고, 그것을 학습하는게 주요 목표인듯 (KL-loss에 비중을 더 둠.) </li>
</ol>
<li> 후반 단계 </li>
<ol>
<li> 온도 매개변수를 낮추어 logits을 날카롭게 하고, 학생의 Hard Label간의 학습에 더 큰 비중을 둔다. </li>
<li> CE-loss에 더 큰 비중을 두어 문제(데이터)에 더 최적화되도록 한다. </li>
</ol>

### Proposed Method - TGD Loss for Sensor Data
<li> TDA 방식을 통해 PI를 생성한다. </li>
<li> 원본 데이터로 학습한 T1과 PI로 학습한 T2를 이용해 KD를 할 것이다. </li>
<li> KL-loss 항이 그래서 aKL(T1, S) + (1-a)KL(T2, S)로 나뉘어짐. </li>
<li> 여기서부터가 문제다. </li>
<li> 두 교사 모델에서 대응하는 중간 층의 Activation Similarity Matrices를 추출함. </li>
<li> 그냥 이 Activation Similarity Matrices를 합치면 학생 모델이 더 많은 정보를 학습할 수 있다. </li>
<li> 그렇지만 위의 방법은 두 교사의 Similarity Map이 중복되거나 상충되는 정보가 표함될 가능성이 높다. </li>
<li> 그래서 Orthogonality(직교성)을 부여해주어야 함. (위의 Merged Similarity Map을) </li>
<li> 직교성을 부여해주면, 특징 간의 독립성이 강화되고, 중복된 정보를 줄이며, 서로 상충되는 간섭도 최소화할 수 있다. </li>
<li> Orthogonality Features로 변환하는 과정 </li>
<ol>
<li> 두 교사의 Similarity Map을 병합한다.(두 교사의 공통 정보와 상호 보완적인 정보가 포함되게 됨.) </li>
<li> 병합된 맵 G_T를 k개의 작은 Patch로 나눈다.(각각의 patch는 특정 영역의 특징 간 관계를 나타내며, 이를 통해 local(국소적) 특징과 전역적(global) 특징을 모두 포착할 수 있다.) </li>
<li> 직교성 제약(Orthogonality Constraint)를 적용함. (각 Patch를 정규화 및 직교성 변환하여 서로 독립적인 특징이 되도록 만든다.) (G_T를 정규화한 후, 단위행렬 I를 빼서 직교성을 유지한다?) </li>
<li> 학생 모델의 Orthogonal Feature와 비교하여 두 G_T와 G_S를 비교하여 loss를 계산한다.(똑같이 같은 방식으로 Orthogonal Feature를 만듦.) </li>
</ol>
<li> 즉, 첫 번째 논문과 두 번째 논문의 방법론적 차이는 첫 번째 Image데이터의 경우 intermediate Layer에서의 Similarity Feature Map을 단순히 Weighted Merge를 해서 학생 모델의 Intermediate Layer의 Similarity Feature Map과 비교하여 Loss항을 비교하고, 두 번째 논문에서 Time-Series Data의 경우 똑같이 T1과 T2에서 Intermediate Layer에서의 Similarity Feature Map을 추출한 뒤, Merge하고, Orthogonality Feature로 변환을 한 뒤 Student Model의 Orthogonality Feature로 변환을 하여 비교한다. </li>

### Proposed Method - Annealing Strategy for Multiple Teachers.
<li> Knowledge Gap을 줄이고, 학생 모델이 교사 모델의 정보를 효율적으로 학습할 수 있게 돕기 위한 전략 </li>
<li> 1. 초기화 단계 </li>
<ul>
<li> 학생 모델의 가중치 초기화 : 학생 모델을 학습하기 전에, 학생 모델과 동일한 구조를 가진 작은 모델을 처음부터 학습한다. </li>
<li> 이렇게 학습된 모델의 가중치를 학생 모델의 초기 가중치 값으로 지정해준다. </li>
<li> 이렇게 하면 랜덤초기화의 불안정한 방식보다 사전 학습된 가중치를 통해 더 안정적인 수렴에 도달할 수 있다. </li>
<li> 또한, 교사와 학생 간 초기 Knowledge Gap을 완화할 수 있기에, 교사 모델의 정보에 더 잘 적응할 수 있다. </li>
<li> 탐색할 공간이 줄어들어서 좋다고 하는데, ??? </li>
</ul>





## Trainning Environment
<li> Dataset = Cifar10, CINIC10, Tiny ImageNet </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1 + CUDA ??? </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results