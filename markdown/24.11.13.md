## 24.11.13 공부할 내용
<li> Resnet의 Bottleneck 구조에 대해 더 깊은 공부 + Resnet 51을 구현한 코드를 보고 이해. </li>
<li> 교수님이 읽으라고 하신 논문 두 부에 대해 충분한 이해를 하고 가자. </li>
<li> 저번 주 논문 중 18번 Reference의 ESKD에 대한 논문을 읽고 이해해보자. </li>


## 논문 주요 내용 (배경지식)
### 기존의 KD의 loss
<ol>
<li> logits이란? </li>
<ul>
<li> 모델의 마지막 층에서 나오는 출력으로, 각 클래스에 대해 예측한 '점수'를 의미한다. </li>
<li> 이 점수는 아직 정규화되지 않은 상태이고, softmax를 적용하면 확률 분포로 변환된다. </li>
</ul>
<li> logit을 이용한 지식 전달 </li>
<ul>
<li> 지식 증류에서 교사 모델의 logit은 단순한 정답이 아닌, 클래스 간의 "미묘한 관계"를 나타내는 정보로 간주된다. </li>
<li> 예를 들어, 어떤 이미지가 고양이일 확률이 0.8이고, 개일 확률이 0.15, 새일 확률이 0.05라면, 교사 모델은 "이 이미지는 고양이에 가깝지만, 개일 가능성도 약간 있다"고 예측한 것임. </li>
<li> 학생 모델은 이러한 클래스 간의 유사성 정보를 배우게 되고, 단순히 정답을 맞추는 것보다도 더 깊은 클래스 간의 관계도 학습하게 된다. </li>
</ul>
<li> 지식 증류에서의 logit 전달 방식 </li>
<ul>
<li> 교사 모델이 생성한 logit을 student 모델의 학습 손실에 포함시켜, 학생 모델이 이 값을 따라가도록 유도한다. </li>
<li> logit 손실(KL Divergence loss) = (a)KL(교사 모델의 logit, 학생 모델의 logit) 으로 표현하는데 이는 두 모델의 출력 분포 간의 차이를 계산하는 식이다. </li>
</ul>
<li> 학생 모델은 두 가지 손실을 통해 학습한다. </li>
<ul>
<li> 일반적인 손실 : 학생 모델이 정답 라벨과 가까운 예측을 하도록 유도하는 손실 (Cross-entropy loss) </li>
<li> 지식 증류 손실 : 교사 모델의 logits을 따라가도록 유도하는 손실 (Distillation loss) </li>
</ul>
</ol>

![KD_loss](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.11.13/KD_lossfunc_KL.png)

### AMD loss
<li> 양성 특징(타겟 객체)와 음성 특징(background) 간의 각 거리(Angular Distance)를 사용하여 특징을 학습한다. </li>
<li> 이 때 특징들을 고차원 구(hypersphere)에 투영하여, 기존의 많은 특징 추출기에서 볼 수 있는 유사한 각 분포를 활용하는 방식이다. </li>
<li> AMD Loss의 작동 원리와 주요 개념 </li>
<ol>
<li> 고차원 구 투영(embedding) </li>
<ul>
<li> 특징 벡터를 고차원 구에 투영하여 학습함. 이를 통해 모델이 각도를 기준으로 양성과 음성 특징을 구분하게 하며, 특징의 방향성에 기반한 학습이 가능해짐. </li>
<li> 구에 투영된 특징 벡터는 방향성에 따라 가까운 특징(양성 특징)은 비슷한 각도로 유지하고, 먼 특징(음성 특징)은 더 큰 각도로 떨어지도록 유도한다. </li>
</ul>
<li> 양성 특징에 각 마진(angular margin) 추가 </li>
<ul>
<li> 양성 특징에 각 마진을 추가하여 더 집중된 특징 벡터를 만든다. 이 마진은 양성 특징 벡터를 구상에서 특정 각도로 모이게 하여, 모델이 중요한 특징을 더 뚜렷하게 구분할 수 있도록 한다. </li>
<li> 이로 인해 양성 특징 벡터는 구 내부에서 밀집된 각도 분포를 가지게 되며, 음성 특징과의 거리가 더 커져 모델이 쉽게 구분할 수 있다. </li>
</ul>
<li> 특징의 각 분포(angular distribution) </li>
<ul>
<li> 많은 특징 추출기에서 양성 특징은 특정 각도에 모여 있는 경향이 있다. AMD Loss는 이러한 특징을 활용하여 양성과 음성 특징 간의 각도를 명확하게 분리하며, 양성 특징은 좁은 각도 분포에 집중시키고 음성 특징은 더 넓은 각도 분포로 분리되도록 한다. </li>
</ul>
</ol>


### Spherical Embedding을 활용하여 KD를 개선하는 방안
<ol>
<li> 활성화된 특징을 얻기 위해 공간 주의(spacial attention) map을 계산하고, 양성 맵과 음성 맵으로 분리함. </li>
<ul>
<li> spacial attention map : 모델이 주목해야 할 영역 </li>
<li> 목적 : 모델이 학습하는 동안 더 중요한 특징에 집중하도록 유도하기 위함. </li>
<li> 방법 : 이미지나 데이터에서 spatial attention map을 계산함. </li>
<li> 이 spatial attention map을 positive map과 negative map으로 나눈다. </li>
<li> 모델이 주목해야 하는 활성화된 영역이 positive map, negative map은 덜 중요한 정보를 의미함. </li>
</ul>
<li> 특징을 고차원 구(hypershpere)에 투영하여 각도 거리(angular distance)를 반영하게 함. </li>
<ul>
<li> angular distance :  </li>
<li> 목적 : 지식 증류 과정에서 모델이 특징 간의 각도 차이를 인식하게 하여 더 나은 표현 학습을 유도하는 것. </li>
<li> 방법 : spatial attention map에서 얻은 positive map(특징)을 고차원 구에 투영한다. </li>
<li> 이 과정을 통해 특징 간의 관계를 각도(angular distance)로 나타낼 수 있다. </li>
<li> 효과 : 고차원 구 임베딩을 통해 특징 간의 각도 차이가 반영되므로, 특징 간의 유사성과 차이가 명확하게 나타난다.  </li>
</ul>
<li> 양성 특징에 각 마진(angular margin)을 추가하여 더 집중된(attentive) 특징 표현 생성 </li>
<ul>
<li> 목적 : 학생 모델이 교사 모델이 주목하는 중요한 특징에 더 집중하게 유도하는 것. </li>
<li> 방법 : 양성 특징에 각 마진(angular margin)을 도입하여, 특징들이 특정 각도로 더 밀집되도록 만든다. 이로 인해 양성 특징은 더 집중된 형태로 표현된다. (같은 클래스의 특징들이 더 비슷한 방향을 가지도록 유도.) </li>
<li> 효과 : 양성 특징이 더 집중된 분포를 가지게 되어, 모델이 중요한 정보에 더 주목하게 된다. 이렇게 특징 표현을 해서, 모델이 특정 객체나 중요한 정보를 더 잘 인식하고 구별할 수 있음. </li>
</ul>
<li> 증류 과정에서 학생 모델이 교사 모델의 더 분리된 결정 영역(decision regions)을 모방하여 분류 성능이 향상됨. </li>
<ul>
<li> 목적 : 학생 모델이 교사 모델의 결정 경계(deicision boundary)를 모방하여 더 좋은 분류 성능을 가지게 한다. (특정 지점을 기준으로 각도 차이가 작다면, 같은 클래스에 속하고, 각도가 크다면 서로 다른 클래스에 속한다. 이 각도 차이를 기준으로 결정 경계를 설정함.) </li>
<li> 방법 : 지식 증류 과정에서 학생 모델이 교사 모델의 more separated된 decision boundary를 모방하도록 학습시킨다. </li>
<li> 이렇게 하여 교사 모델이 특징 간의 구분을 명확히 하고, 더 높은 정확도로 클래스를 예측하는 것을 따라갈 수 있게 한다. </li>
<li> 효과 : 학생 모델은 교사 모델의 Decision Boundary를 모방함으로써, 더 나은 분류 성능을 발휘하게 된다. </li>
<li> 학생 모델이 교사 모델의 결정 경계를 모방한다는 것은, 학생 모델이 교사 모델이 설정한 클래스 간의 구분 방식과 결정 경계를 따라가도록 학습한다는 것임. </li>
<li> Angular Margin과 KL-divergence 손실을 함께 사용하여, 학생 모델이 교사 모델과 유사한 방향성과 각도를 가진 특징 벡터를 학습하도록 만든다. </li>
<li> 학생 모델이 교사 모델의 각도 분포와 특징 간의 거리(angular distance(두 벡터가 이루는 각도))를 학습함으로써, 교사 모델이 가진 결정 경계와 유사한 경계를 형성하게 됨. </li>
</ul>
<li> 이 Shperical Embedding방식에서의 특징 표현 규제(regularizatino) </li>
<ul>
<li> 목적 : 전체적으로 학생 모델이 교사 모델의 유용한 정보를 더 잘 학습하도록 만든다. </li>
<li> 방법 : 제안된 방법을 통해 학생 모델의 특징 표현이 교사 모델처럼 정보가 풍부하고 중요한 특징을 포함하도록 규제한다. </li>
<li> 효과 : 학생 모델이 교사 모델의 중요한 정보를 효과적으로 학습하여, 더 높은 성능을 발휘할 수 있다. 교사 모델의 표현력을 학생 모델이 닮아가도록 학습하는 과정이므로, 학생 모델이 더 좋은 특징을 학습하고, 최종 성능이 향상됨. </li>
<li> Angular Margin을 추가함으로써, 같은 클래스의 특징 벡터들이 서로 더 가까운 방향을 가리키도록 제약을 주는 것임. </li>
<li> 손실 함수에 Angular Margin을 추가하여, 같은 클래스의 벡터들이 일정한 각도 이상으로 더 가까워져야 손실이 줄어들도록 학습을 진행한다. </li>
</ul>
</ol>

## Abstract 정리
<ol>
<li> KD는 네트워크를 경량화하고, 메모리효율적으로 만들어 주었다. </li>
<li> 교사 모델은 pre-trained 모델을 주로 사용하고, Pre-Trained 모델은 이미 학습된 상태로 제공되는 모델을 의미함. (즉, 오픈소스 활용이 가능한 대규모 데이터셋에서 학습이 완료된 모델을 말하는 듯.), 즉, 이미 학습된 가중치를 가지고 나의 Domain에 맞춰서 미세하게 가중치를 조정해주면 됨. </li>
<li> Pre-Trained Model의 반대로는 From Scratch Model 이라고 해서 내가 어떤 특수한 도메인에서 사용하고자 할 때 처음부터 학습을 시켜서 만드는 모델을 말한다. </li>
<li> 이 논문에서는 기존의 KD와 다르게, 중간 층의 Feature Map을 Knowledge Distillation의 source로 사용하여 교사 네트워크의 지식을 학생 네트워크로 전달한다. </li>
<ul>
<li> 중간 층의 Feature Map은 입력 데이터의 저차원 특징(간단한 엣지, 패턴)부터, 고차원 특징(객체의 형태, 구조)까지 다양한 정보를 포함하고 있다. </li>
<li> 즉, 중간층의 특징 맵을 활용하면, 학생 네트워크가 교사 네트워크의 다층적인 특징 표현을 학습하게 되어서, 더 나은 일반화 성능을 기대할 수 있다. </li>
<li> 최종 출력만으로는 클래스에 대한 확률 분포 정도만 전달할 수 있지만, 중간층의 특징 맵을 사용하면 객체의 구체적인 세부 정보와 구조를 학생 모델이 학습할 수 있다. </li>
<li> 특히 객체의 위치, 모양, 텍스처 등의 세밀한 정보가 중간층의 특징 맵에 포함되어있다. </li>
<li> 즉, 중간 층의 Feature Map을 이용한 Distillation은, 학생 모델이 중간층의 Feature Map을 모방하도록 유도하여, 교사 모델의 내재된 표현 능력까지 학습하게 된다. </li>
</ul>
</ol>

## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results