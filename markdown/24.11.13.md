## 24.11.13 공부할 내용
<li> Resnet의 Bottleneck 구조에 대해 더 깊은 공부 + Resnet 51을 구현한 코드를 보고 이해. </li>
<li> 교수님이 읽으라고 하신 논문 두 부에 대해 충분한 이해를 하고 가자. </li>
<li> 저번 주 논문 중 18번 Reference의 ESKD에 대한 논문을 읽고 이해해보자. </li>


## 논문 주요 내용 (배경지식)
### 기존의 KD의 loss
<li> logits이란? </li>
<ul>
<li> 모델의 마지막 층에서 나오는 출력으로, 각 클래스에 대해 예측한 '점수'를 의미한다. </li>
<li> 이 점수는 아직 정규화되지 않은 상태이고, softmax를 적용하면 확률 분포로 변환된다. </li>
</ul>
<li> logit을 이용한 지식 전달 </li>
<ul>
<li> 지식 증류에서 교사 모델의 logit은 단순한 정답이 아닌, 클래스 간의 "미묘한 관계"를 나타내는 정보로 간주된다. </li>
<li> 예를 들어, 어떤 이미지가 고양이일 확률이 0.8이고, 개일 확률이 0.15, 새일 확률이 0.05라면, 교사 모델은 "이 이미지는 고양이에 가깝지만, 개일 가능성도 약간 있다"고 예측한 것임. </li>
<li> 학생 모델은 이러한 클래스 간의 유사성 정보를 배우게 되고, 단순히 정답을 맞추는 것보다도 더 깊은 클래스 간의 관계도 학습하게 된다. </li>
</ul>
<li> 지식 증류에서의 logit 전달 방식 </li>
<ul>
<li> 교사 모델이 생성한 logit을 student 모델의 학습 손실에 포함시켜, 학생 모델이 이 값을 따라가도록 유도한다. </li>
<li> logit 손실 = (a)KL(교사 모델의 logit, 학생 모델의 logit) 으로 표현하는데 이는 두 모델의 출력 분포 간의 차이를 계산하는 식이다. </li>
</ul>
<li> 학생 모델은 두 가지 손실을 통해 학습한다. </li>
<ol>
<li> 일반적인 손실 : 학생 모델이 정답 라벨과 가까운 예측을 하도록 유도하는 손실 (Cross-entropy loss) </li>
<li> 지식 증류 손실 : 교사 모델의 logits을 따라가도록 유도하는 손실 (Distillation loss) </li>
</ol>

### AMD loss
<li> 양성 특징(타겟 객체)와 음성 특징(background) 간의 각 거리(Angular Distance)를 사용하여 특징을 학습한다. </li>
<li> 이 때 특징들을 고차원 구(hypersphere)에 투영하여, 기존의 많은 특징 추출기에서 볼 수 있는 유사한 각 분포를 활용하는 방식이다. </li>
<li> AMD Loss의 작동 원리와 주요 개념 </li>
<ol>
<li> 고차원 구 투영 </li>
<ul>
<li> 특징 벡터를 고차원 구에 투영하여 학습함. 이를 통해 모델이 각도를 기준으로 양성과 음성 특징을 구분하게 하며, 특징의 방향성에 기반한 학습이 가능해짐. </li>
<li> 구에 투영된 특징 벡터는 방향성에 따라 가까운 특징(양성 특징)은 비슷한 각도로 유지하고, 먼 특징(음성 특징)은 더 큰 각도로 떨어지도록 유도한다. </li>
</ul>
<li> 양성 특징에 각 마진(angular margin) 추가 </li>
<ul>
<li> 양성 특징에 각 마진을 추가하여 더 집중된 특징 벡터를 만든다. 이 마진은 양성 특징 벡터를 구상에서 특정 각도로 모이게 하여, 모델이 중요한 특징을 더 뚜렷하게 구분할 수 있도록 한다. </li>
<li> 이로 인해 양성 특징 벡터는 구 내부에서 밀집된 각도 분포를 가지게 되며, 음성 특징과의 거리가 더 커져 모델이 쉽게 구분할 수 있다. </li>
</ul>
<li> 특징의 각 분포(angular distribution) </li>
<ul>
<li> 많은 특징 추출기에서 양성 특징은 특정 각도에 모여 있는 경향이 있다. AMD Loss는 이러한 특징을 활용하여 양성과 음성 특징 간의 각도를 명확하게 분리하며, 양성 특징은 좁은 각도 분포에 집중시키고 음성 특징은 더 넓은 각도 분포로 분리되도록 한다. </li>
</ul>
</ol>

### Early Stopping Knowledge Distillation
<li> Knowledge Distillation의 한 변형으로, 교사 모델이 최종 학습 완료된 모델이 아닌, 훈련 초기에 조기종료된 모델을 사용하여 학생 모델을 학습시키는 방식이다. </li>
<li> 조기 종료 : 즉, ESKD는 일반화 성능이 가장 높은 시점에서 학습을 멈추어서 학습 후반에 과적합될 가능성을 낮추기 위한 방식이다. </li>
<li> 일반화 성능 강화 : 위와 같이 일반화 성능이 강화된 교사 모델을 사용해서 학생 모델에 지식을 전달하면, 학생 모델 또한 더 다양한 상황에 적응할 수 있게 된다. </li>
<li> ESKD의 동작 방식 </li>
<ul>
<li> 교사 모델의 조기 종료 : 교사 모델을 학습시킬 때 교사 모델의 검증 성능이 최고치에 도달할 때 학습을 중단시켜 과적합이 발생하기 전에 학습을 끝마친다. </li>
<li> 조기 종료된 교사 모델의 지식 증류 : 조기 종료된 교사 모델을 학생 모델의 학습에 사용한다. 학생 모델은 교사 모델의 Soft Label을 통해 학습하면서, 교사 모델의 일반화된 지식을 흡수하게 된다. </li>
<li> 조기 종료와 데이터 증강의 결합 : ESKD에서는 종종 데이터 증강 기법을 조기 종료된 교사 모델과 함께 사용하여, 학생 모델이 더 다양한 데이터 변형에 적응하도록 돕는다. 이를 통해 학생 모델이 보다 다양한 시계열 패턴을 학습하고, 엣지 장비와 같은 제한된 환경에서도 높은 성능을 낼 수 있게 한다. </li>
</ul>

### Data Augmentation
<li> Data Augmentation(데이터 증강)은 모델이 더 다양항 데이터를 학습할 수 있도록, 기존 데이터에 변형을 가하여 새로운 데이터처럼 만드는 방법이다. </li>
<li> 이를 통해 모델이 과적합(overfitting)되지 않고, 다양한 상황에서도 잘 동작할 수 있게 한다. </li>
<li> 데이터 증강 기법의 종류 (논문에서 제시한) </li>
<ul>
<li> Removal : 시계열 데이터에서 일부 구간을 삭제하여, 일부 정보가 결여된 상황을 모델이 학습하게 함.<br>
ex) 특정 시간 구간의 데이터를 제거하고, 남은 데이터만으로 모델이 추론할 수 있도록 훈련한다. </li>
<li> Noise Injection (잡음 추가) : 시계열 데이터에 Gaussian Noise를 추가하여 데이터에 불확실성이 있는 상황을 모델이 학습하게 한다. 잡음이 섞인 데이터를 사용하여 모델이 노이즈에 강하게 되도록 훈련함. </li>
<li> Shifting (이동) : 시계열 데이터를 일정 시간만큼 앞으로 or 뒤로 이동시켜, 데이터의 위치가 약간 바뀌더라도 모델이 *일관된 패턴*을 학습할 수 있도록 돕는다. 데이터의 패턴이 시간에 따라 변해도 패턴 그 자체는 변하지 않기에 주요 특징을 잡아낼 수 있게 하는 데 유용하다. </li>
<li> Mix1 및 Mix2 : 여러 데이터 증강 기법을 결합하여 동시에 적용. Mix1은 Removal과 Shifting, Mix2는 Removal, Noise Injection, Shifting을 동시에 적용한다. </li>
</ul>
<li> Data Augmentation을 Knowledge Distillation에 적용하는 이유는 교사 모델과 학생 모델이 더 다양한 변형된 데이터를 학습할 수 있도록 하기 위함이다. </li>
<li> 교사 모델이 다양한 증강된 데이터를 학습하면, 교사 모델의 예측 분포가 더 일반화된 형태를 띄게 되어, 학생 모델이 더 잘 학습할 수 있는 정보를 제공한다. </li>
<li> 학생 모델 또한 이러한 증강된 데이터를 학습하여 실제 상황에서 다양한 변형에 더 강인하게 작동할 수 있다. </li>


### Trained from scratch와 KD 방식의 비교

| 특성                 | Trained from Scratch                            | Knowledge Distillation (KD)                              |
|----------------------|------------------------------------------------|---------------------------------------------------------|
| **학습 시작 상태**   | 무작위 초기화된 모델로 학습 시작               | 교사 모델이 사전 학습된 상태에서 학생 모델 학습 시작     |
| **라벨 사용 방식**   | 원본 데이터의 하드 라벨만 사용                  | 교사 모델의 소프트 라벨을 참고하여 학습                  |
| **주요 목표**        | 독립적인 학습과 최적화                         | 지식을 전달하여 소형 모델의 성능 최적화                  |
| **필요한 리소스**    | 많은 데이터와 긴 학습 시간                      | 사전 학습된 교사 모델 필요                               |
| **적용 대상**        | 새로운 데이터셋에 맞추어 모델 학습              | 모델 경량화 및 엣지 컴퓨팅 환경에서 사용                |



## 논문 내용 요약
### 제목 : Role of Data Augmentation Stragies in Knowledge Distillation for Wearable Sensor Data
### Abstract
<ul>
<li> 문제제기 : 딥러닝은 엄청난 수의 파라미터들을 학습해서, 많은 분류 문제를 해결할 수 있다. 하지만, Edge Device와 같이 작은 기계들에는 딥러닝 기술을 통합시키기 어렵다. 왜냐하면, 작기 때문에 연산량과 parameter의 개수가 부담이 됨.</li>
<li> 해결책 : 그렇기 때문에, KD가 널리 적용되왔다. 높은 성능을 보이는 네트워크에서 미리 학습을 수행하고, 그 지식을 작은 모델(소형 네트워크)에 전달하여 Edge Device에 적합한 모델을 만드는 데 사용된다. </li>
<li> 목표 : 이 논문에서는 처음으로 웨어러블 기기를 통해 얻어진 time-series 데이터에 적용할 수 있는지의 적용 가능성과 어려움을 연구함. </li>
<li> Data Augmentation의 필요성 : KD의 성공적인 적용은 적절한 Data Augmentation을 적용하는 것 부터가 시작이지만, KD에 적절한 Data Augmentation은 아직 발견되지 않았다.</li>
<li> 연구 방법 : 다양한 데이터 증강 기법을 비교하고, 혼합 데이터 증강 기법도 적용한 뒤 결과를 비교. </li>
<li> 주요 결과 : 여기에서 우리는 결론짓길, Databases에서 강한 Baseline performance를 보여줄 수 있는 추천할만한 General Set이 있었다. </li>
</ul>

### Introduction
<ol>
<li> 딥러닝 모델의 성능과 경량화 문제
<ul>
<li> 딥러닝은 컴퓨터 비전, 음성 인식, 웨어러블 센서 데이터 분석 등 다양한 분야에서 높은 성능을 보여주고 있지만, 더 많은 Layer와 Parameter를 쌓을수록 모델의 크기가 커지고, 더 많은 연산 및 전력이 필요하기에, Edge Device에서는 큰 제약으로 찾아온다. </li>
<li> 이 문제를 해결하기 위해 Network Pruning(불필요한 뉴런이나 연결 제거), Quantization(연산에 사용되는 숫자의 정밀도를 낮춤), low-rank Factorization(파라미터 행렬을 낮은 랭크 행렬들의 곱으로 분해), KD 등이 연구되어 왔다. </li>
</ul>
<li> 지식 증류(KD) </li>
<ul> 
<li> KD는 큰 모델에서 학습된 지식(가중치와 예측 분포)을 작은 모델(학생 모델)에 전달하여 작고 효율적인 모델을 만드는 기법이다. </li>
<li> 다른 모델 압축 기법과 달리, KD는 후처리나 Fine tuning 없이도 성능을 유지할 수 있다는 장점이 있다. </li>
<li> 최근에는 다양한 KD 변형 기법들이 제안되었고, 조기 종료된 교사 모델을 사용한 ESKD가 KD의 효율성을 높이는 데 기여한다는 연구가 있다. </li>
</ul>
<li> 이 연구의 주요 기여 </li>
<ul> 
<li> 인간 활동 인식을 위한 웨어러블 센서 기반 time-series Data에 KD를 적용한 최초의 연구이다. </li>
<li> 데이터 증강 기법이 KD에 미치는 영향을 분석한다. 다양한 시간 영역 데이터 증강 전략을 훈련 및 테스트에 적용하여 KD의 성능을 평가한다. </li>
</ul>
<li> 주요 연구 결과 </li>
<ul>
<li> 여러 KD 접근 방식을 비교하여 ESKD가 다른 기법들보다 우수한 성능을 보인다는 것을 확인 </li>
<li> 교사 및 학생 모델의 크기가 성능에 미치는 영향을 분석하여, 더 큰 교사 모델이 반드시 더 나은 성능을 제공하는 것은 아님을 증명.(Wide Resent 16-3이 가장 좋았다.) </li>
<li> 교사 및 학생 모델 모두에서 데이터 증강 기법의 효과를 평가하여, 분류 성능에 가장 유리한 증강 기법의 조합을 확인함. (mix1 데이터 증강 기법이 가장 일반화 성능을 증가시켜줌.) </li>
<li> 소규모와 대규모 공개 데이터셋을 활용하여 연구를 수행함으로써, 데이터셋 크기에 관계없이 연구 결과를 신뢰할 수 있음을 증명. </li>
</ul>

</ol>

### Knowledge Distillation에 Data Augmentation을 적용하는 전략들 (실험 내용 스포)
<ol>
<li> KD에서의 두 가지 증강 시나리오 </li>
<ul>
<li> 첫 번째 시나리오 : 교사 모델이 원본 데이터로 학습 후 학생 모델 학습 시에만 데이터 증강 기법을 적용한다. 즉, 교사는 원본 데이터를 기반으로 학습하고, 학생 모델은 증강된 데이터를 통해 학습한다. </li>
<li> 두 번째 시나리오 : 교사와 학생 모델 모두에 증강 기법을 적용한다. 이 경우, 교사 모델을 처음부터 학습할 때 증강 기법을 적용하고, 이 모델을 학생 모델 학습에 사용하는 사전 학습된 모델로 활용한다. 학생 모델은 교사와 동일하거나 다른 증강 기법을 통해 학습할 수 있다. </li>
</ul>
<li> 조기 종료된 교사 모델을 사용하는 ESKD </li>
<ul>
<li> ESKD는 교사 모델을 완전히 학습시키지 않고, 조기 종료된 상태에서 학생 모델을 학습시키는 방법이다. 이전 연구에서 ESKD가 완전히 학습된 교사 모델(Full KD)을 사용하는 것보다 더 나은 학생 모델을 생성한다는 결과가 있었다. </li>
<li> ESKD의 장점은, 학습 초기에 정확도가 향상되지만, 학습이 진행될수록 정확도가 감소하는 경향을 방지할 수 있다. 즉, ESKD는 초기에 높은 정확도를 보이는 조기 종료된 교사 모델과 증강 기법을 결합하여 실험을 진행한다. </li>
</ul>
<li> 3. 시간 영역 증강 기법 </li>
<ul>
<li> KD에 대한 증강 효과를 분석하기 위해 시간 영역 증강 기법을 사용한다. 여기에는 제거(Removal), 가우시안 노이즈 추가 (Noise Injection), 이동(Shifting)이 포함된다. </li>
<li> 이러한 변환은 데이터의 원래 패턴, 윈도우 길이, 주기적인 지점을 유지하면서도 다양한 데이터 변형을 제공한다. </li>
</ul>
<li> 증강 기법의 조합 </li>
<ul>
<li> 각 증강 기법뿐만 아니라, 여러 증강 기법을 조합하여 적용하여 교사와 학생 모델에 대한 데이터셋의 속성 간 관계를 분석한다. 예를 들어, 제거와 이동을 조합(Mix1)하거나, 모든 증강 기법을 조합(Mix2)하여 다양한 증강 효과를 분석한다. </li>
</ul>
</ol>

### Conclusion
<ol>
<li> 교사 모델의 크기와 성능 : 큰 용량의 교사 모델이 반드시 학생 모델의 성능을 높여주지는 않았다. </li>
<li> 조기 종료와 데이터 증강을 결합한 ESKD : 조기 종료와 데이터 증강을 결합한 ESKD방식이 시계열 데이터에 더 효과적이었다. </li>
<li> 증강 전략의 영향 : 증강 전략이 교사 모델보다는 학생 모델 훈련에 더 큰 영향을 미친다는 결론을 얻음. </li>
<li> Mix1 전략 : 학생 모델 훈련에서 Mix1 (Removal + Shifting) 전략이 대체로 강력한 성능을 보였다. </li>
</ol>


### 내가 이해한 내용
<ol>
<li> 여러 Resnet, Wide Resnet을 사용하여 네트워크의 깊이, 망의 너비(채널 수)를 비교해본 결과, 더 큰 교사모델(더 깊거나 너비가 넓은)이 항상 더 나은 학생 모델 성능을 보장하지는 않았다. (Wide Resnet 16-1, 16-3, 16-8로 뒤에는 고정) </li>
<li> 조기종료된 교사 모델(ESKD)이 완전 학습된 교사 모델 (Full KD)보다 더 좋은 성능을 보여주었다. (특히 time-series 데이터에 효과적이었음.) </li>
<li> ESKD에 데이터 증강을 결합한 방식이 가장 좋은 성능을 보였으며, 특히, 학생 모델에만 데이터 증강을 적용했을 때 성능이 더 좋았다. (라는건지 그냥 Student Model에 특히 Data Augmentation이 더 영향을 크게 주었다는건지 확실치 않음.) </li>
</ol>


### Resnet18로 Cifar10 돌려보기
<li> 확실히 층도 더 쌓고 단차 해결이 가능해서 그런가 Accuracy가 많이 올라왔다. </li>
<li> 4개의 Residual Stage를 만든 후 각각에 포함될 block수를 지정해줄 수 있었다. </li>
![Resnet-18](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.11.6/Cifar10_Resnet_Adam_lr0.0005.png)



## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results