```python
- 이번 주 미팅내용
Channels 방식으로 우선 진행
Training에서 CE_loss를 제거해야 함.
Tokenizer도 불러오지 말자. → 직접 모든 모듈을 학습하는 방식으로

codebook 개수를 줄여보자. 4개부터 시작 ~ 8 16 32 까지도 조금씩 맞춰서 늘려보는 느낌으로 가자.
Hyperparameter 여러번 수정해가면서 실행. → 여기서의 목적은 Codebook이 여러 sub-sequence를 잘 배울 수 있도록(recon/acc는 부수적인거)

원본과 recon된것의 MSE도 비교 → 이거또한 frozen된 classifier로 Acc를 측정하는 것과 비슷

즉, Classification CE_loss를 빼고, 하이퍼파라미터 및 codebook 크기를 잘 바꿔서 Codebook 학습이 되게 한 뒤, Epoch을 늘려서 결과를 확인해보자.
```

### MMVQShape
- 위의 것들을 반영해서 다시 실험을 돌려보자.
- Codebook = 4 부터, lambda를 조절해가며 학습이 되는지부터 확인. 

- 근데, 지금 GENE의 자체 문제가 어렵기 때문에, Patch 개수를 늘려서 더 조밀조밀하게 봐야하는 것 같다는 생각도 듬. 
- 지그제그가 너무 많아서 flat하게 예측을 하는거라면, 이거를 더 잘게 나누면 sub-sequence를 잘 포착하지 않을까라는 생각 
- 지금은 (50, 10)으로 나누었는데 (100, 5)로도 해볼까 생각중. → 이대로 진행.

- 실험 결과들 : pdf로 첨부. 생각보다 codebook이 의미있는 모양을 나타내는 경우가 늘었지만, 여러 모양을 학습한다는 느낌은 부족함.
- 여러 가지 setting을 바꿔가며 해봐야 할 듯.

- 현재 code dimension을 4 → 8 → 16 으로 바꿔가며 16에서 실험을 돌리고 있음.
- Acc를 통해서 모델의 x_recon을 보자면, codebook_size = 4, lambda_s = 0.5 일 때 epoch 500에서 63%로 가장 높았으며, 계속 증가하는 추세.
- codebook_size = 8, lambda들 1(UEA setting)인 경우 61.0%, codebook_size = 16, lambda들 1(UEA setting)인 경우 61.5% 였음.

### 오늘 할 일
1. 주간보고 작성
2. 내일 미팅할 자료 만들기
3. 논문 수정하기 (미완)