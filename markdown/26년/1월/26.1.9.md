```python
Channels 방식으로 ㅇ
Training에서 CE_loss를 제거. → 이거는 무조건 ㅇ
Tokenizer도 불러오지 말자.   → 이거도 무조건 ㅇ

codebook 개수를 줄여보자.   4개 ~ 8 16 32 까지도.. (하나씩 나아가자.) 
Hyperparameter 여러번 수정해가면서 실행. (Codebook 고도화가 잘 되도록 (Acc는 부수적))


Acc보다는 Codebook 및 Sub-Sequence를 잘 만드는것을 목표로 한다는 느낌.


원본과 recon된것의 MSE도 비교?


Classification CE_loss를 빼고, 하이퍼파라미터를 잘 바꿔서 Codebook 학습이 되게 한 뒤, Epoch을 늘려서 결과를 확인해보자.
```

### MMVQShape
- 위의 것들을 반영해서 다시 실험을 돌려보자.
- Codebook = 4 부터, lambda를 조절해가며 학습이 되는지부터 확인. 

- 근데, 지금 GENE의 자체 문제가 어렵기 때문에, Patch 개수를 늘려서 더 조밀조밀하게 봐야하는 것 같다는 생각도 듬. 
- 지그제그가 너무 많아서 flat하게 예측을 하는거라면, 이거를 더 잘게 나누면 sub-sequence를 잘 포착하지 않을까라는 생각 
- 지금은 (50, 10)으로 나누었는데 (100, 5)로도 해볼까 생각중.

### 