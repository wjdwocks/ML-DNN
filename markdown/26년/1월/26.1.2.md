### MMVQShape
- WRN1611에서 사용한 GENEActiv Dataset은 Training Set Data 전체를 사용해서 Normalize 되어 있는것임.(mean/std)
- 근데, 나는 VQShape으로 Normalize하고, batch 단위로 DeNormalize한 것을 WRN1611에 넣었기 때문에, classification 학습이 틀어졌을 수 있음.
- 그렇기 때문에, Training Set에 대한 Channel별로 (mean/std)를 npy로 저장해놓고, 그것을 불러와서 x_recon된 값을 다시 Normalize한 뒤에 Classification에 넣는 방식으로 학습이 이루어져야 할 듯 하다.
- mean = [ch1, ch2, ch3]
- std = [std1, std2, std3]

### MMVQShape 해야 할 것.
1. GENEActiv의 Training set에 대한 정규화 값을 얻어둔 뒤, npy로 저장해서, x_recon을 다시 정규화 하고, classification에 넣는다.
    - 이 때의 classification Acc를 확인해야 함. (65~67%는 넘어야 좋을듯.) → 하는중
2. 그렇게 log를 보고, Epoch을 늘릴 지 확인.
    - epoch 후반에도 계속 acc가 늘어난다면(과적합되지 않는다면,) epoch을 400까지도 늘려보자. (학습할게 많아서 괜찮을듯.) → 300으로..
3. Hyperparameter는 UEA와 동일하게 유지.
    - 그렇게 해야 base로서 사용하고, 하나씩 바꿔가기 편할듯. (잘 되기도 하고.) 

4. 만약 65%를 넘었다면, 말하고, Text 쪽을 추가해서 학습을 해보기. (아직..)
    - Text 추출해놓은 대로 사용하면서, Supcon loss추가 및 여러가지..
5.  그러면서 3Channel 나누어서 학습하는것도 고려해보면 좋을듯. (부수적)
    - 이것은 부수적으로 사용.
    - UEA의 TS transformer decoder/encoder를 pretrained된 것을 사용할 수 있도록 (500 → 512로 masking을 한 뒤)
    - VQShape의 것들을 수행하고, x_recon이 512로 나오면 다시 cut 해서 500으로 수정.
    - 이것들을 Normalization 하고, WRN1611에 넣어서 Classificatoin 수행

### Flatten 방식에서의 결과





### 이상한 점
- CE_loss를 0으로 뒀을 때까지는 Acc가 50%정도까지는 오른다. (Recon이 어느정도 된다는 느낌.)
- 그런데, CE_loss가 들어가고 나서 바로는 Acc가 쭉 떨어짐 ~20%정도까지
- 그러다가 다시 천천히 오르기 시작하는데, ~60%정도까지 오르는 듯.
- 그래서 Student(WRN1611)를 Freeze하고, CE_loss를 흘려보내야 할지도?? 



### 3Teacher 논문
- alpha Hyperparameter 0.7:0.3 이렇게 사용하지 말고, 논문에서 사용한 수식을 이용해서 2Teacher Setting에서는 alpha가 0.7 in GENE, 0.3 to PAMAP2 이런식으로. (완)
- alpha hyperparameter 3teacher는 alpha_1, alpha_2, alpha_3가 0.2, 0.3 and 0.5 이렇게로 사용했다는 서술방식으로 수정. (완)
- Proposed Method의 B, 2)를 {B, 2) 2T}, {B, 2) 3T}로 나누어서 서술하기. (짧아져도 ㄱㅊ을듯) (완)
- Figure 4 수정. (완)
- 이미지들 캡션 다 보면서 수정해야 함.

### VQShape 소개
- 26.1.7 수요일 1시에 VQShape 소개하는 발표 해야함.

