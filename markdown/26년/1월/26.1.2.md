### MMVQShape
- WRN1611에서 사용한 GENEActiv Dataset은 Training Set Data 전체를 사용해서 Normalize 되어 있는것임.(mean/std)
- 근데, 나는 VQShape으로 Normalize하고, batch 단위로 DeNormalize한 것을 WRN1611에 넣었기 때문에, classification 학습이 틀어졌을 수 있음.
- 그렇기 때문에, Training Set에 대한 Channel별로 (mean/std)를 npy로 저장해놓고, 그것을 불러와서 x_recon된 값을 다시 Normalize한 뒤에 Classification에 넣는 방식으로 학습이 이루어져야 할 듯 하다.
- mean = [ch1, ch2, ch3]
- std = [std1, std2, std3]

### MMVQShape 해야 할 것.
1. GENEActiv의 Training set에 대한 정규화 값을 얻어둔 뒤, npy로 저장해서, x_recon을 다시 정규화 하고, classification에 넣는다.
    - 이 때의 classification Acc를 확인해야 함. (65~67%는 넘어야 좋을듯.) → 하는중
2. 그렇게 log를 보고, Epoch을 늘릴 지 확인.
    - epoch 후반에도 계속 acc가 늘어난다면(과적합되지 않는다면,) epoch을 400까지도 늘려보자. (학습할게 많아서 괜찮을듯.) → 300으로..
3. Hyperparameter는 UEA와 동일하게 유지.
    - 그렇게 해야 base로서 사용하고, 하나씩 바꿔가기 편할듯. (잘 되기도 하고.) 

4. 만약 65%를 넘었다면, 말하고, Text 쪽을 추가해서 학습을 해보기. (아직..)
    - Text 추출해놓은 대로 사용하면서, Supcon loss추가 및 여러가지..
5.  그러면서 3Channel 나누어서 학습하는것도 고려해보면 좋을듯. (부수적)
    - 이것은 부수적으로 사용.
    - UEA의 TS transformer decoder/encoder를 pretrained된 것을 사용할 수 있도록 (500 → 512로 masking을 한 뒤)
    - VQShape의 것들을 수행하고, x_recon이 512로 나오면 다시 cut 해서 500으로 수정.
    - 이것들을 Normalization 하고, WRN1611에 넣어서 Classificatoin 수행

### Flatten 방식에서의 결과1
- 아레의 lambda 조합에서의 결과
``` python    
    --s_smooth_factor 1 \
    --lambda_x 1 \          x_hat과 x 사이의 MSE, x_loss
    --lambda_z 1 \          VQ-VAE loss 로 z_hat과 Codebook 비슷해지게 하던 loss
    --lambda_s 1 \          S_hat(예측한거)와 interpolation으로 가져온 s 와의 MSE, s_loss
    --lambda_dist 0.8 \     t_hat, l_hat의 consistnecy loss, Attribute Decoder가 t, l을 안겹치게 하던거
    --codebook-size 64 \
    --token_length 256
    --CE_loss 1 \
    --epoch 300 \
```
- epoch 50이전까지는 CE_loss의 가중치 0, 이후에 CE_loss(1)로
- epoch 50에서의 s, x
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_250_f.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_250_f.png" alt="results" width="700">
- epoch 300(끝났을 때)의 s, x
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_1500_f.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_1500_f.png" alt="results" width="700">
- Acc는 최대 61%대까지도 나왔다. 근데, epoch 300에서도 계속 오르는 것 처럼도 보였지만, x_recon이 망가지는 것 처럼 보여서 Codebook의 이점이 흐려지는 것 처럼 보임
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/acc_f.png" alt="results" width="700">
- CE_loss가 추가되기 전의 Acc (x_recon만 잘 된거로 wrn1611에 들어간 경우)
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/acc_ce1.png" alt="results" width="700">


### Flatten방식에서의 결과2
- 위에서 CE_loss를 1로 두고 epoch을 늘렸을 때 x_recon이 많이 망가지고, Acc만 높아지는것을 봐서 CE_loss를 0.5로 낮춰봄.
``` python    
    --s_smooth_factor 1 \
    --lambda_x 1 \          x_hat과 x 사이의 MSE, x_loss
    --lambda_z 1 \          VQ-VAE loss 로 z_hat과 Codebook 비슷해지게 하던 loss
    --lambda_s 1 \          S_hat(예측한거)와 interpolation으로 가져온 s 와의 MSE, s_loss
    --lambda_dist 0.8 \     t_hat, l_hat의 consistnecy loss, Attribute Decoder가 t, l을 안겹치게 하던거
    --codebook-size 64 \
    --token_length 256
    --CE_loss 0.5 \
    --epoch 300 \
```
- epoch 50에서의 s, x
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_250_f2.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_250_f2.png" alt="results" width="700">
- epoch 300(끝났을 때)의 s, x
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_1500_f2.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_1500_f2.png" alt="results" width="700">
- x_recon은 CE_loss가 높을 때 보다 더 나은 모습을 보이지만, Acc는 많이 떨어짐(56%정도).
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/acc_f2.png" alt="results" width="700">


### Channel로 나눈 방식에서의 결과
``` python    
    --s_smooth_factor 1 \
    --lambda_x 1 \          x_hat과 x 사이의 MSE, x_loss
    --lambda_z 1 \          VQ-VAE loss 로 z_hat과 Codebook 비슷해지게 하던 loss
    --lambda_s 1 \          S_hat(예측한거)와 interpolation으로 가져온 s 와의 MSE, s_loss
    --lambda_dist 0.8 \     t_hat, l_hat의 consistnecy loss, Attribute Decoder가 t, l을 안겹치게 하던거
    --codebook-size 64 \
    --token_length 256
    --CE_loss 1 \
    --epoch 300 \
```
- epoch 50에서의 s
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_300_channel0.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_300_channel1.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_300_channel2.png" alt="results" width="700">

- epoch 50에서의 x
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_300_channel0.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_300_channel1.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_300_channel2.png" alt="results" width="700">

- epoch 300(끝났을 때)의 s
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_1500_channel0.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_1500_channel1.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/s_recon_1500_channel2.png" alt="results" width="700">

- epoch 300(끝났을 때)의 x
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_1500_channel0.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_1500_channel1.png" alt="results" width="700">
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/x_recon_1500_channel2.png" alt="results" width="700">

- Acc는 오히려 낮아진 46%정도. 최대 49%대까지 나왔는데, 왜 Flatten방식보다 낮은지 모르겠다.. (차라리 코드문제였으면 좋겠다..)
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/acc_c.png" alt="results" width="700">
- CE_loss가 추가되기 전의 Acc(x_recon만 잘 된거로 wrn1611에 들어간 경우) 48%정도인데, 이상하게 CE_loss가 추가된 후에 잠깐동안 안줄어들음. (코드 점검해봐야할듯)
- <img src="https://github.com/wjdwocks/ML-DNN/raw/main/markdown/26년/1월/26.1.2/acc_ce2.png" alt="results" width="700">


### 3Teacher 논문
- alpha Hyperparameter 0.7:0.3 이렇게 사용하지 말고, 논문에서 사용한 수식을 이용해서 2Teacher Setting에서는 alpha가 0.7 in GENE, 0.3 to PAMAP2 이런식으로. (완)
- alpha hyperparameter 3teacher는 alpha_1, alpha_2, alpha_3가 0.2, 0.3 and 0.5 이렇게로 사용했다는 서술방식으로 수정. (완)
- Proposed Method의 B, 2)를 {B, 2) 2T}, {B, 2) 3T}로 나누어서 서술하기. (짧아져도 ㄱㅊ을듯) (완)
- Figure 4 수정. (완)
- 이미지들 캡션 다 보면서 수정해야 함.


### VQShape 소개
- 26.1.7 수요일 1시에 VQShape 소개하는 발표 해야함.

### 화요일 미팅임
- 25.1.6 12시