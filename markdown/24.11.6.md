## 24.11.06 공부할 내용
<li> Resnet을 직접 작게나마 구현해보고, 돌려보자. (이해는 당연히 필수.) </li>
<li> Residual Block, Bottleneck Residual Block 이런거에 대한 구분된 이해 </li>
<li> Knowledge Distillation 에 대한 논문을 읽고, 최대한 많이 이해해서 가자. </li>


## 논문 주요 내용 (배경지식)
### Knowledge Distillation
<li> 
대형 네트워크 모델(Teacher Model)의 성능을 작은 네트워크 모델(Student Model)로 전달하는 기술.
</li>
<li> 이렇게 함으로써, 작은 Student Model이 교사 모델의 성능을 최대한 모방할 수 있게 되고, Edge Device와 같은 리소스가 제한된 환경에서도 잘 동작할 수 있는 소형 모델(Student)을 만들 수 있다. </li>
<li> KD의 기본 개념 </li>
<ul>
<li> 교사 모델 : 대형 네트워크로 많은 파라미터와 높은 성능을 가지고 있다. 훈련이 잘 되어 있어 복잡한 데이터에 대한 높은 정확도를 보이지만, 모델이 커서 엣지 장비에 바로 적용하기 어렵다. </li>
<li> 학생 모델 : 파라미터 수가 적고, 구조가 간단한 작은 네트워크이다. 교사 모델의 성능을 최대한 학습하여 유사한 성능을 내는 것이 목표이다. </li>
<li> Soft Labels : KD에서 교사 모델이 예측하는 출력을 "소프트 라벨" 이라고 부른다. 
<br> ex) 분류 문제에서 단순히 정답 클래스만을 학습하는 것이 아닌, 교사 모델이 각 클래스에 대해 예측한 확률을 학습하게 된다.
<br>
이를 통해 학생 모델은 단순히 정답을 맞추는 것이 아니라, 교사 모델의 예측 분포를 모방하여 더 깊이 있는 학습을 할 수 있다.
 </li>
 <li> 손실 함수 : KD에서는 교사 모델과 학생 모델의 예측 분포 간의 차이를 줄이는 방향으로 학생 모델을 학습시킨다. 이를 위해 일반적으로 Cross Entropy loss와 Kullback-Leibler? 등을 결합하여 사용한다. </li>
</ul>
<li> KD가 시계열 데이터에 적용되는 방식 : 시계열 데이터는 시간이 흐름에 따라 변화하는 값들로 구성되어 있어, 패턴이 연속적으로 나타나기 때문에 모델이 연속적 패턴과 변동성을 학습해야 한다. KD를 시계열 데이터에 적용할 때는 교사 모델이 시계열의 다양한 패턴을 학습하고, 학생 모델이 이를 모방하여 일반화할 수 있도록 하는 것이 목표이다. </li>
<li> 주요 hyperparameter  <br>
    1. Temperature(τ) : 교사 모델의 출력 확률 분포를 부드럽게(smooth) 만드는 역할을 한다. 이 파라미터는 학생 모델이 교사 모델의 출력을 통해 더 많은 정보를 학습할 수 있도록 돕는다. 즉, 온도를 높이면 출력 분포가 부드러워짐 (peak가 줄어들어, 소프트맥스 후 다른 클래스들에도 유의미한 확률이 할당된다.) <br>
    τ = 1 : 온도를 조절하지 않은 상태<br>
    τ > 1 : 출력 분포가 부드러워지고, 학생 모델이 더 많은 정보를 학습할 수 있다.<br>
    2. Lambda (λ) : 교사 모델의 예측 분포와 원본 라벨 간의 중요도를 조정하는 파라미터이다. <br>
    즉, 교사 모델의 지식과 원본 데이터 라벨을 결합하는 비율을 조정하는 역할을 한다.
     </li>


### Early Stopping Knowledge Distillation
<li> Knowledge Distillation의 한 변형으로, 교사 모델이 최종 학습 완료된 모델이 아닌, 훈련 초기에 조기종료된 모델을 사용하여 학생 모델을 학습시키는 방식이다. </li>
<li> 조기 종료 : 즉, ESKD는 일반화 성능이 가장 높은 시점에서 학습을 멈추어서 학습 후반에 과적합될 가능성을 낮추기 위한 방식이다. </li>
<li> 일반화 성능 강화 : 위와 같이 일반화 성능이 강화된 교사 모델을 사용해서 학생 모델에 지식을 전달하면, 학생 모델 또한 더 다양한 상황에 적응할 수 있게 된다. </li>
<li> ESKD의 동작 방식 </li>
<ul>
<li> 교사 모델의 조기 종료 : 교사 모델을 학습시킬 때 교사 모델의 검증 성능이 최고치에 도달할 때 학습을 중단시켜 과적합이 발생하기 전에 학습을 끝마친다. </li>
<li> 조기 종료된 교사 모델의 지식 증류 : 조기 종료된 교사 모델을 학생 모델의 학습에 사용한다. 학생 모델은 교사 모델의 Soft Label을 통해 학습하면서, 교사 모델의 일반화된 지식을 흡수하게 된다. </li>
<li> 조기 종료와 데이터 증강의 결합 : ESKD에서는 종종 데이터 증강 기법을 조기 종료된 교사 모델과 함께 사용하여, 학생 모델이 더 다양한 데이터 변형에 적응하도록 돕는다. 이를 통해 학생 모델이 보다 다양한 시계열 패턴을 학습하고, 엣지 장비와 같은 제한된 환경에서도 높은 성능을 낼 수 있게 한다. </li>
</ul>

### Data Augmentation
<li> Data Augmentation(데이터 증강)은 모델이 더 다양항 데이터를 학습할 수 있도록, 기존 데이터에 변형을 가하여 새로운 데이터처럼 만드는 방법이다. </li>
<li> 이를 통해 모델이 과적합(overfitting)되지 않고, 다양한 상황에서도 잘 동작할 수 있게 한다. </li>
<li> 데이터 증강 기법의 종류 (논문에서 제시한) </li>
<ul>
<li> Removal : 시계열 데이터에서 일부 구간을 삭제하여, 일부 정보가 결여된 상황을 모델이 학습하게 함.<br>
ex) 특정 시간 구간의 데이터를 제거하고, 남은 데이터만으로 모델이 추론할 수 있도록 훈련한다. </li>
<li> Noise Injection (잡음 추가) : 시계열 데이터에 Gaussian Noise를 추가하여 데이터에 불확실성이 있는 상황을 모델이 학습하게 한다. 잡음이 섞인 데이터를 사용하여 모델이 노이즈에 강하게 되도록 훈련함. </li>
<li> Shifting (이동) : 시계열 데이터를 일정 시간만큼 앞으로 or 뒤로 이동시켜, 데이터의 위치가 약간 바뀌더라도 모델이 *일관된 패턴*을 학습할 수 있도록 돕는다. 데이터의 패턴이 시간에 따라 변해도 패턴 그 자체는 변하지 않기에 주요 특징을 잡아낼 수 있게 하는 데 유용하다. </li>
<li> Mix1 및 Mix2 : 여러 데이터 증강 기법을 결합하여 동시에 적용. Mix1은 Removal과 Shifting, Mix2는 Removal, Noise Injection, Shifting을 동시에 적용한다. </li>
</ul>
<li> Data Augmentation을 Knowledge Distillation에 적용하는 이유는 교사 모델과 학생 모델이 더 다양한 변형된 데이터를 학습할 수 있도록 하기 위함이다. </li>
<li> 교사 모델이 다양한 증강된 데이터를 학습하면, 교사 모델의 예측 분포가 더 일반화된 형태를 띄게 되어, 학생 모델이 더 잘 학습할 수 있는 정보를 제공한다. </li>
<li> 학생 모델 또한 이러한 증강된 데이터를 학습하여 실제 상황에서 다양한 변형에 더 강인하게 작동할 수 있다. </li>


## 논문 내용 요약
### 제목 : Role of Data Augmentation Stragies in Knowledge Distillation for Wearable Sensor Data
### Abstract
<ul>
<li> 문제제기 : 딥러닝은 엄청난 수의 파라미터들을 학습해서, 많은 분류 문제를 해결할 수 있다. 하지만, Edge Device와 같이 작은 기계들에는 딥러닝 기술을 통합시키기 어렵다. 왜냐하면, 작기 때문에 연산량과 parameter의 개수가 부담이 됨.</li>
<li> 해결책 : 그렇기 때문에, KD가 널리 적용되왔다. 높은 성능을 보이는 네트워크에서 미리 학습을 수행하고, 그 지식을 작은 모델(소형 네트워크)에 전달하여 Edge Device에 적합한 모델을 만드는 데 사용된다. </li>
<li> 목표 : 이 논문에서는 처음으로 웨어러블 기기를 통해 얻어진 time-series 데이터에 적용할 수 있는지의 적용 가능성과 어려움을 연구함. </li>
<li> Data Augmentation의 필요성 : KD의 성공적인 적용은 적절한 Data Augmentation을 적용하는 것 부터가 시작이지만, KD에 적절한 Data Augmentation은 아직 발견되지 않았다.</li>
<li> 연구 방법 : 다양한 데이터 증강 기법을 비교하고, 혼합 데이터  </li>
<li> 주요 결과 : 여기에서 우리는 결론짓길, Databases에서 강한 Baseline performance를 보여줄 수 있는 추천할만한 General Set이 있었다. </li>
</ul>

### Introduction
<ol>
<li> 딥러닝 모델의 성능과 경량화 문제
<ul>
<li> 딥러닝은 컴퓨터 비전, 음성 인식, 웨어러블 센서 데이터 분석 등 다양한 분야에서 높은 성능을 보여주고 있지만, 더 많은 Layer와 Parameter를 쌓을수록 모델의 크기가 커지고, 더 많은 연산 및 전력이 필요하기에, Edge Device에서는 큰 제약으로 찾아온다. </li>
<li> 이 문제를 해결하기 위해 Network Pruning(불필요한 뉴런이나 연결 제거), Quantization(연산에 사용되는 숫자의 정밀도를 낮춤), low-rank Factorization(파라미터 행렬을 낮은 랭크 행렬들의 곱으로 분해), KD 등이 연구되어 왔다. </li>
</ul>
<li> 지식 증류(KD) </li>
<ul> 
<li> KD는 큰 모델에서 학습된 지식(가중치와 예측 분포)을 작은 모델(학생 모델)에 전달하여 작고 효율적인 모델을 만드는 기법이다. </li>
<li> 다른 모델 압축 기법과 달리, KD는 후처리나 Fine tuning 없이도 성능을 유지할 수 있다는 장점이 있다. </li>
<li> 최근에는 다양한 KD 변형 기법들이 제안되었고, 조기 종료된 교사 모델을 사용한 ESKD가 KD의 효율성을 높이는 데 기여한다는 연구가 있다. </li>
</ul>
<li> 이 연구의 주요 기여 </li>
<ul> 
<li> 인간 활동 인식을 위한 웨어러블 센서 기반 time-series Data에 KD를 적용한 최초의 연구이다. </li>
<li> 데이터 증강 기법이 KD에 미치는 영향을 분석한다. 다양한 시간 영역 데이터 증강 전략을 훈련 및 테스트에 적용하여 KD의 성능을 평가한다. </li>
</ul>
<li> 주요 연구 결과 </li>
<ul>
<li> 여러 KD 접근 방식을 비교하여 ESKD가 다른 기법들보다 우수한 성능을 보인다는 것을 확인 </li>
<li> 교사 및 학생 모델의 크기가 성능에 미치는 영향을 분석하여, 더 큰 교사 모델이 반드시 더 나은 성능을 제공하는 것은 아님을 증명.(Wide Resent 16-3이 가장 좋았다.) </li>
<li> 교사 및 학생 모델 모두에서 데이터 증강 기법의 효과를 평가하여, 분류 성능에 가장 유리한 증강 기법의 조합을 확인함. (mix1 데이터 증강 기법이 가장 일반화 성능을 증가시켜줌.) </li>
<li> 소규모와 대규모 공개 데이터셋을 활용하여 연구를 수행함으로써, 데이터셋 크기에 관계없이 연구 결과를 신뢰할 수 있음을 증명. </li>
</ul>

</ol>

### Knowledge Distillation에 Data Augmentation을 적용하는 전략들 (실험 내용 스포)
<ol>
<li> KD에서의 두 가지 증강 시나리오 </li>
<ul>
<li> 첫 번째 시나리오 : 교사 모델이 원본 데이터로 학습 후 학생 모델 학습 시에만 데이터 증강 기법을 적용한다. 즉, 교사는 원본 데이터를 기반으로 학습하고, 학생 모델은 증강된 데이터를 통해 학습한다. </li>
<li> 두 번째 시나리오 : 교사와 학생 모델 모두에 증강 기법을 적용한다. 이 경우, 교사 모델을 처음부터 학습할 때 증강 기법을 적용하고, 이 모델을 학생 모델 학습에 사용하는 사전 학습된 모델로 활용한다. 학생 모델은 교사와 동일하거나 다른 증강 기법을 통해 학습할 수 있다. </li>
</ul>
<li> 조기 종료된 교사 모델을 사용하는 ESKD </li>
<ul>
<li> ESKD는 교사 모델을 완전히 학습시키지 않고, 조기 종료된 상태에서 학생 모델을 학습시키는 방법이다. 이전 연구에서 ESKD가 완전히 학습된 교사 모델(Full KD)을 사용하는 것보다 더 나은 학생 모델을 생성한다는 결과가 있었다. </li>
<li> ESKD의 장점은, 학습 초기에 정확도가 향상되지만, 학습이 진행될수록 정확도가 감소하는 경향을 방지할 수 있다. 즉, ESKD는 초기에 높은 정확도를 보이는 조기 종료된 교사 모델과 증강 기법을 결합하여 실험을 진행한다. </li>
</ul>
<li> 3. 시간 영역 증강 기법 </li>
<ul>
<li> KD에 대한 증강 효과를 분석하기 위해 시간 영역 증강 기법을 사용한다. 여기에는 제거(Removal), 가우시안 노이즈 추가 (Noise Injection), 이동(Shifting)이 포함된다. </li>
<li> 이러한 변환은 데이터의 원래 패턴, 윈도우 길이, 주기적인 지점을 유지하면서도 다양한 데이터 변형을 제공한다. </li>
</ul>
<li> 증강 기법의 조합 </li>
<ul>
<li> 각 증강 기법뿐만 아니라, 여러 증강 기법을 조합하여 적용하여 교사와 학생 모델에 대한 데이터셋의 속성 간 관계를 분석한다. 예를 들어, 제거와 이동을 조합(Mix1)하거나, 모든 증강 기법을 조합(Mix2)하여 다양한 증강 효과를 분석한다. </li>
</ul>
</ol>

### Conclusion
<ol>
<li> 교사 모델의 크기와 성능 : 큰 용량의 교사 모델이 반드시 학생 모델의 성능을 높여주지는 않았다. </li>
<li> 조기 종료와 데이터 증강을 결합한 ESKD : 조기 종료와 데이터 증강을 결합한 ESKD방식이 시계열 데이터에 더 효과적이었다. </li>
<li> 증강 전략의 영향 : 증강 전략이 교사 모델보다는 학생 모델 훈련에 더 큰 영향을 미친다는 결론을 얻음. </li>
<li> Mix1 전략 : 학생 모델 훈련에서 Mix1 (Removal + Shifting) 전략이 대체로 강력한 성능을 보였다. </li>
</ol>


### 내가 이해한 내용
<ol>
<li> 여러 Resnet, Wide Resnet을 사용하여 네트워크의 깊이, 망의 너비(채널 수)를 비교해본 결과, 더 큰 교사모델(더 깊거나 너비가 넓은)이 항상 더 나은 학생 모델 성능을 보장하지는 않았다. (Wide Resnet 16-1, 16-3, 16-8로 뒤에는 고정) </li>
<li> 조기종료된 교사 모델(ESKD)이 완전 학습된 교사 모델 (Full KD)보다 더 좋은 성능을 보여주었다. (특히 time-series 데이터에 효과적이었음.) </li>
<li> ESKD에 데이터 증강을 결합한 방식이 가장 좋은 성능을 보였으며, 특히, 학생 모델에만 데이터 증강을 적용했을 때 성능이 더 좋았다. (라는건지 그냥 Student Model에 특히 Data Augmentation이 더 영향을 크게 주었다는건지 확실치 않음.) </li>
</ol>


## Trainning Environment
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1+cpu </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.001 </li>
<li> optimizer = Adam (Adagrad, RMSprop, SGD, SGD+Momentum 도 사용.) </li>



## Evaluation


## Results