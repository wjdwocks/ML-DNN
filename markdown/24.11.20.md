## 24.11.13 공부할 내용
<li> 첫 번째 논문 : PI-Net - A Deep Learning Approach to Extract Topological Persistence Images </li>
<li> 두 번째 논문 :  </li>

## 첫 번째 논문
### Abstract 정리
<li> PD와 PI는 머신러닝과 컴퓨터 비전 응용에서 상당한 잠재력을 보여주고 있다. </li>
<li> 원래는 원본 데이터 → Persistence Diagram(PD) → Persistence Image(PI) 의 과정을 통해서 Topological Features를 얻어낼 수 있다. </li>
<li> 하지만, 이 방식은 PD를 생성하는데 복잡하고, 계산 비용이 매우 크다. </li>
<li> 또한, PD를 PI로 변환하는 과정에서도 설계자의 고정된 변환 방식(가우시안 필터링, 히스토그램 기반 변환)을 따른다. </li>
<li> 또한 그렇기 때문에 특정 데이터나 문제에 맞게 동적으로 최적화 되지 않음. </li>
<li> PI-Net은 입력 데이터를 받아 문제에 최적화된 PI를 학습해서 생성해준다. </li>
<li> 또한 이를 통해서 PI가 바로 학습 가능한 형태로 변환되어서 딥러닝 모델의 성능을 극대화해준다. </li>

### Introduction 정리
<li> CNN은 컴퓨터 비전에서 큰 호응을 얻고 있다. </li>
<li> CNN은 지역적 및 시공간적(temporal and spatial) 상관관계를 학습하는 데 뛰어난 성능을 보이고, 1D, 2D, 3D 데이터를 다룰 때 큰 관심을 받아옴. </li>
<li> CNN은 fully connected 네트워크보다 훨신 적은 파라미터의 수를 학습하여 과적합에도 강하다. </li>
<li> 하지만, 지금까지의 딥러닝 연구는 입력 데이터를 출력 데이터로 매핑하는데 초점을 맞췄지, 센서 노이즈 같은 저수준 물리적 요인에 대한 강건성을 확보하는 데는 상대적으로 관심이 적었다. </li>
<li> TDA(위상 데이터 분석)은 이러한 문제를 해결하기 위해 사용되는 접근법으로, 데이터의 형상을 분석하여 Persistence Diagram(PD)와 같은 표현을 통해 잡음에 강건한 특징을 제공한다. </li>
<li> TDA는 다양한 컴퓨터 비전 문제에 성공적으로 적용되어 컴퓨터 비전 커뮤니티가 관심을 가지고 있다. </li>
<li> PI-Net은 데이터의 형상(Shape of Data)으로부터 데이터의 위상적 속성(Topological Properties)을 추출하여 Topological Data(PI)로 변환하는 것이 목표이다. </li>
<ul>
<li> 데이터의 형상 : 데이터의 전체적인 공간적 배열과 구조. 늘리거나 구부리면 바뀔 수 있다. ex) 클러스터, 루프, 빈 공간 </li>
<li> 위상적 속성 : 데이터의 형상에서 변하지 않는 근본적인 특성. Stretch, 구부림, 회전에도 변하지 않는다. ex) 연결 성분의 수, 구멍의 개수, 고차원 구멍. </li>
<li> 데이터의 형상 속에서 위상적 속성을 얻어낼 수 있다. </li>
</ul>
<li> TDA 방법론은 원본 데이터를 Sparse Distance Matrix로 변환 → Distance Matrix를 사용해 Persistence Diagram 생성 → PD를 Persistence Image로 변환 해야 하는데, 이는 Sparse Distance Matrix와 PD를 계산하는 과정이 매우 복잡해서 계산 비용이 너무 많이든다는 문제(bottleneck)가 있다. </li>
<li> 그래서 이 논문에서는 PI-Net을 고안해 냈다. </li>
<li> PI-Net에서는 TDA의 복잡한 전처리 과정을 모두 제거하고, 원본 데이터를 바로 학습 가능한 PI(PI-Net은 미분 가능하므로, 딥러닝 네트워크와 쉽게 통합될 수 있다?)로 변환하는 것이 목표이다. </li>
<li> PI-Net은 원본 데이터를 입력으로 받으면 학습 가능한 PI를 출력으로 주는 그런 네트워크이다. </li>
<li> 이 논문에서는 두 가지 간단한 CNN 기반 아키텍처를 제공한다. Signal PI-Net (다변량 1D 시계열 데이터 처리), Image PI-Net (다채널 2D 이미지 데이터 처리) </li>
<li>뒤에 내용 정리</li>
<ul>
<li> 전이 학습 전략 탐구 : PI-Net을 소스 데이터셋에서 학습시키고, 이를 타겟 데이터셋에서 미세 조정(fine-Tuning)을 하거나 하지 않는 방식으로 사용하는 전이 학습 전략을 탐구함. (소스 데이터셋에서 학습되었을 때, 새로운 데이터셋에서도 유효하게 작동하는지 평가한다는 의미) </li>
<li> PI-Net이 생성한 PI와 전통적인 TDA방식에서 생성된 PI의 성능을 비교할 것이다. 어떻게?) PI데이터를 딥러닝에서 학습된 특징(CNN's Feature map)과 결합하여 성능 향상 가능성을 검토함. </li>
<li> 데이터에 Gaussian Noise를 추가하고, PI표현을 결합했을 때 노이즈에 대한 강건성이 향상되는지 검증함. </li>
</ul>


### Related Work
<li> Persistence Diagram(PD) </li>
<ul>
<li> 가장 널리 사용되는 위상적 요약이다(topological summary) </li>
<li> PD는 point cloud에 정의된 함수의 sub-level sets 또는 k차원 구멍과 같은 위상적 특징의 생성(birth)과 소멸(death)시점을 2D 평면에 나타낸 점들의 다중 집합이다. </li>
</ul>
<li> TDA의 한계 </li>
<ul>  
<li> PD를 추출하는 데 시간 비용이 매우 많이 든다. </li>
<li> PD는 점들의 다중 집합(multi-set)으로, 머신러닝이나 딥러닝 프레임워크에서 이를 직접 사용하는 것이 불가능하다. (비정형 데이터) </li>
</ul>
<li> 두 번째 문제를 해결하기 위해서 PD를 머신러닝 도구에 더 적합한 space로 mapping하려는 노력이 있었다. ex) PI로의 변환. </li>
<li> PI-Net의 기여 </li>
<ul>
<li> 첫 번째 문제를 완화하기 위해, 우리는 원하는 위상적 특징 표현을 계산하기 위한 간단한 단일 단계의 미분 가능한 아키텍처(PI-net)을 제안한다. </li>
<li> 즉, PI-Net은 기존의 복잡한 PD 생성 과정을 단일 단계로 단순화하고, 이를 학습 가능한 형태(딥러닝에서)로 변환해준다. </li>
</ul>
<li> 기존의 TDA + 딥러닝 방식과 이 논문에서 딥러닝을 사용한 방식의 차이 </li>
<ul>
<li> 기존에서는 딥러닝 모델의 위상 분석, 알고리즘 복잡성(딥러닝 모델의 계산 효율성과 복잡성을 TDA로 분석), 동작 및 선택(딥러닝 모델이 데이터에 방능하는 방식과 최적의 모델 선택 과정에 위상적 분석 적용)에 쓰였다. </li>
<li> 이 논문에서는 TDA와 딥러닝을 결합하여 PI를 생성하고, 이를 행동 인식과 이미지 분류에 적용한다. (TDA를 입력 데이터로 사용하는 것을 넘어, PI-Net이라는 학습 가능한 구조를 통해 위상적 특징을 직접 추출하는 방식으로 차별화됨.) </li>
</ul>

## BackGround
### Persistence Diagrams
<li> PD란? </li>
<ul>
<li> Persistence Diagram은 데이터의 위상적 특징 (연결성, 구멍 등)을 수학적으로 표현한 것이다. </li>
<li> 데이터는 고차원 point cloud로 모델링되고, 이를 기반으로 그래프 G를 만든다. (Node와 Node Relation(관계)) </li>
<li> 이 그래프 위에 단순 복합체 (simplicial complex)를 생성하여 위상적 정보를 추출한다. </li>
</ul>
<li> Persistent Homology와 Scalar Filed Topology를 통해 PD가 만들어짐. </li>
<ul>
<li> Persistent Homology : 데이터에서 나타나는 k-차원 위상적 구조(연결 성분, 루프, 고차원 구멍)등을 분석하는 방법 </li>
<li> simplex는 ϵ-이웃 규칙(두 점 사이의 거리가 ϵ이하일 때 연결)으로 구성된다. </li>
<li> Scalar Filed Topology : 그래프 정점에 정의된 실수 값 함수 g의 레벨 집합을 기반으로 위상적 구조를 분석한다. </li>
</ul>
<li> PD의 정보 </li>
<ul>
<li> Persistent Homology와 Scalar Filed Topology모두, PD는 관심 있는 위상적 특징의 생성 시점과 소멸 시점 정보를 요약하는 간단한 방법을 제공한다. </li>
<li> 이 논문에서 Persistent Homology는 이미지의 Ground-Truth PD를 계산하고, Scalar Filed Topology는 시계열 신호의 Ground-Truth PD를 계산한다. </li>
</ul>

### PI
<li> Persistence Image 정의 </li>
<ul>
<li> PI는 PD를 딥러닝 모델에서 사용할 수 있는 형태로 변환한 데이터이다. </li>
<li> 이를 위해 PD를 먼저 Persistence Surface라는 수학적 구조로 변환한다. </li>
<li> Persistence Surface 는 PD의 점들을 기반으로 연속적인 함수 형태로 표현한 것이다. </li>
</ul>
<li> Persistence Surface의 계산 </li>
<ul>
<li> PD에 존재하는 각 점(생성과 소멸 시점)은 2D 평면 위에 나타나고, 이 점들을 중심으로 가우시안 함수가 생성됨. </li>
<li> 이 가우시안 함수들의 가중 합(weighted sum)으로 Persistence Surface가 형성된다. </li>
<li> ex) 지속 시간이 길수록(구멍이 오래 유지될수록?) 더 큰 가중치를 부여할 수 있다. </li>
</ul>
<li> PI로의 변환 </li>
<ul>
<li> Persistence Surface는 연속적인 함수 형태이기 때문에, 이를 격자(grid)로 이산화(discretization) 하여 2D 이미지 형태로 변환한다. </li>
<li> 각 격자 셀에서 함수 값을 적분하여 최종적으로 픽셀 값(행렬)을 생성하고, 이것이 PI가 된다. </li>
</ul>
<li> 가중치 함수와 중요도 </li>
<ul>
<li> 가중치 함수는 PI를 생성할 때 중요한 요소인데, 지속 시간이 긴 점(구멍이나 연결성이 오래 유지되는 특성)은 데이터에서 더 중요한 의미를 가질 가능성이 크기 때문에, 가중치를 더 크게 설정할 수 있다. </li>
<li> 즉, PI를 계산할 때 가우시안 함수에 가중치를 부여하기 위해 다양한 가중치 함수를 선택할 수 있는데, 위와 같은 것들을 고려할 수 있다. </li>
</ul>


### Proposed Method

### 실험


## 두 번째 논문

## 주요 내용 요약

## Trainning Environment
<li> Dataset = Cifar10, CINIC10, Tiny ImageNet </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1 + CUDA ??? </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results