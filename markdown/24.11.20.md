## 24.11.13 공부할 내용
<li> 첫 번째 논문 : PI-Net - A Deep Learning Approach to Extract Topological Persistence Images </li>
<li> 두 번째 논문 : Understanding the Fole of Mixup in Knowledge Distillation: An Empirical Study </li>

## 첫 번째 논문
### Abstract 정리
<li> PD와 PI는 머신러닝과 컴퓨터 비전 응용에서 상당한 잠재력을 보여주고 있다. </li>
<li> 원래는 원본 데이터 → Persistence Diagram(PD) → Persistence Image(PI) 의 과정을 통해서 Topological Features를 얻어낼 수 있다. </li>
<li> 하지만, 이 방식은 PD를 생성하는데 복잡하고, 계산 비용이 매우 크다. </li>
<li> 또한, PD를 PI로 변환하는 과정에서도 설계자의 고정된 변환 방식(가우시안 필터링, 히스토그램 기반 변환)을 따른다. </li>
<li> 또한 그렇기 때문에 특정 데이터나 문제에 맞게 동적으로 최적화 되지 않음. </li>
<li> PI-Net은 입력 데이터를 받아 문제에 최적화된 PI를 학습해서 생성해준다. </li>
<li> 또한 이를 통해서 PI가 바로 학습 가능한 형태로 변환되어서 딥러닝 모델의 성능을 극대화해준다. </li>

### Introduction 정리
<li> CNN은 컴퓨터 비전에서 큰 호응을 얻고 있다. </li>
<li> CNN은 지역적 및 시공간적(temporal and spatial) 상관관계를 학습하는 데 뛰어난 성능을 보이고, 1D, 2D, 3D 데이터를 다룰 때 큰 관심을 받아옴. </li>
<li> CNN은 fully connected 네트워크보다 훨신 적은 파라미터의 수를 학습하여 과적합에도 강하다. </li>
<li> 하지만, 지금까지의 딥러닝 연구는 입력 데이터를 출력 데이터로 매핑하는데 초점을 맞췄지, 센서 노이즈 같은 저수준 물리적 요인에 대한 강건성을 확보하는 데는 상대적으로 관심이 적었다. </li>
<li> TDA(위상 데이터 분석)은 이러한 문제를 해결하기 위해 사용되는 접근법으로, 데이터의 형상을 분석하여 Persistence Diagram(PD)와 같은 표현을 통해 잡음에 강건한 특징을 제공한다. </li>
<li> TDA는 다양한 컴퓨터 비전 문제에 성공적으로 적용되어 컴퓨터 비전 커뮤니티가 관심을 가지고 있다. </li>
<li> PI-Net은 데이터의 형상(Shape of Data)으로부터 데이터의 위상적 속성(Topological Properties)을 추출하여 Topological Data(PI)로 변환하는 것이 목표이다. </li>
<ul>
<li> 데이터의 형상 : 데이터의 전체적인 공간적 배열과 구조. 늘리거나 구부리면 바뀔 수 있다. ex) 클러스터, 루프, 빈 공간 </li>
<li> 위상적 속성 : 데이터의 형상에서 변하지 않는 근본적인 특성. Stretch, 구부림, 회전에도 변하지 않는다. ex) 연결 성분의 수, 구멍의 개수, 고차원 구멍. </li>
<li> 데이터의 형상 속에서 위상적 속성을 얻어낼 수 있다. </li>
</ul>
<li> TDA 방법론은 원본 데이터를 Sparse Distance Matrix로 변환 → Distance Matrix를 사용해 Persistence Diagram 생성 → PD를 Persistence Image로 변환 해야 하는데, 이는 Sparse Distance Matrix와 PD를 계산하는 과정이 매우 복잡해서 계산 비용이 너무 많이든다는 문제(bottleneck)가 있다. </li>
<li> 그래서 이 논문에서는 PI-Net을 고안해 냈다. </li>
<li> PI-Net에서는 TDA의 복잡한 전처리 과정을 모두 제거하고, 원본 데이터를 바로 학습 가능한 PI(PI-Net은 미분 가능하므로, 딥러닝 네트워크와 쉽게 통합될 수 있다?)로 변환하는 것이 목표이다. </li>
<li> PI-Net은 원본 데이터를 입력으로 받으면 학습 가능한 PI를 출력으로 주는 그런 네트워크이다. </li>
<li> 이 논문에서는 두 가지 간단한 CNN 기반 아키텍처를 제공한다. Signal PI-Net (다변량 1D 시계열 데이터 처리), Image PI-Net (다채널 2D 이미지 데이터 처리) </li>
<li>뒤에 내용 정리</li>
<ul>
<li> 전이 학습 전략 탐구 : PI-Net을 소스 데이터셋에서 학습시키고, 이를 타겟 데이터셋에서 미세 조정(fine-Tuning)을 하거나 하지 않는 방식으로 사용하는 전이 학습 전략을 탐구함. (소스 데이터셋에서 학습되었을 때, 새로운 데이터셋에서도 유효하게 작동하는지 평가한다는 의미) </li>
<li> PI-Net이 생성한 PI와 전통적인 TDA방식에서 생성된 PI의 성능을 비교할 것이다. 어떻게?) PI데이터를 딥러닝에서 학습된 특징(CNN's Feature map)과 결합하여 성능 향상 가능성을 검토함. </li>
<li> 데이터에 Gaussian Noise를 추가하고, PI표현을 결합했을 때 노이즈에 대한 강건성이 향상되는지 검증함. </li>
</ul>


### Related Work
<li> Persistence Diagram(PD) </li>
<ul>
<li> 가장 널리 사용되는 위상적 요약이다(topological summary) </li>
<li> PD는 point cloud에 정의된 함수의 sub-level sets 또는 k차원 구멍과 같은 위상적 특징의 생성(birth)과 소멸(death)시점을 2D 평면에 나타낸 점들의 다중 집합이다. </li>
</ul>
<li> TDA의 한계 </li>
<ul>  
<li> PD를 추출하는 데 시간 비용이 매우 많이 든다. </li>
<li> PD는 점들의 다중 집합(multi-set)으로, 머신러닝이나 딥러닝 프레임워크에서 이를 직접 사용하는 것이 불가능하다. (비정형 데이터) </li>
</ul>
<li> 두 번째 문제를 해결하기 위해서 PD를 머신러닝 도구에 더 적합한 space로 mapping하려는 노력이 있었다. ex) PI로의 변환. </li>
<li> PI-Net의 기여 </li>
<ul>
<li> 첫 번째 문제를 완화하기 위해, 우리는 원하는 위상적 특징 표현을 계산하기 위한 간단한 단일 단계의 미분 가능한 아키텍처(PI-net)을 제안한다. </li>
<li> 즉, PI-Net은 기존의 복잡한 PD 생성 과정을 단일 단계로 단순화하고, 이를 학습 가능한 형태(딥러닝에서)로 변환해준다. </li>
</ul>
<li> 기존의 TDA + 딥러닝 방식과 이 논문에서 딥러닝을 사용한 방식의 차이 </li>
<ul>
<li> 기존에서는 딥러닝 모델의 위상 분석, 알고리즘 복잡성(딥러닝 모델의 계산 효율성과 복잡성을 TDA로 분석), 동작 및 선택(딥러닝 모델이 데이터에 방능하는 방식과 최적의 모델 선택 과정에 위상적 분석 적용)에 쓰였다. </li>
<li> 이 논문에서는 TDA와 딥러닝을 결합하여 PI를 생성하고, 이를 행동 인식과 이미지 분류에 적용한다. (TDA를 입력 데이터로 사용하는 것을 넘어, PI-Net이라는 학습 가능한 구조를 통해 위상적 특징을 직접 추출하는 방식으로 차별화됨.) </li>
</ul>

## BackGround
### Persistence Diagrams
<li> PD란? </li>
<ul>
<li> Persistence Diagram은 데이터의 위상적 특징 (연결성, 구멍 등)을 수학적으로 표현한 것이다. </li>
<li> 데이터는 고차원 point cloud로 모델링되고, 이를 기반으로 그래프 G를 만든다. (Node와 Node Relation(관계)) </li>
<li> 이 그래프 위에 단순 복합체 (simplicial complex)를 생성하여 위상적 정보를 추출한다. </li>
</ul>
<li> Persistent Homology와 Scalar Filed Topology를 통해 PD가 만들어짐. </li>
<ul>
<li> Persistent Homology : 데이터에서 나타나는 k-차원 위상적 구조(연결 성분, 루프, 고차원 구멍)등을 분석하는 방법 </li>
<li> simplex는 ϵ-이웃 규칙(두 점 사이의 거리가 ϵ이하일 때 연결)으로 구성된다. </li>
<li> Scalar Filed Topology : 그래프 정점에 정의된 실수 값 함수 g의 레벨 집합을 기반으로 위상적 구조를 분석한다. </li>
</ul>
<li> PD의 정보 </li>
<ul>
<li> Persistent Homology와 Scalar Filed Topology모두, PD는 관심 있는 위상적 특징의 생성 시점과 소멸 시점 정보를 요약하는 간단한 방법을 제공한다. </li>
<li> 이 논문에서 Persistent Homology는 이미지의 Ground-Truth PD를 계산하고, Scalar Filed Topology는 시계열 신호의 Ground-Truth PD를 계산한다. </li>
</ul>

### PI
<li> Persistence Image 정의 </li>
<ul>
<li> PI는 PD를 딥러닝 모델에서 사용할 수 있는 형태로 변환한 데이터이다. </li>
<li> 이를 위해 PD를 먼저 Persistence Surface라는 수학적 구조로 변환한다. </li>
<li> Persistence Surface 는 PD의 점들을 기반으로 연속적인 함수 형태로 표현한 것이다. </li>
</ul>
<li> Persistence Surface의 계산 </li>
<ul>
<li> PD에 존재하는 각 점(생성과 소멸 시점)은 2D 평면 위에 나타나고, 이 점들을 중심으로 가우시안 함수가 생성됨. </li>
<li> 이 가우시안 함수들의 가중 합(weighted sum)으로 Persistence Surface가 형성된다. </li>
<li> ex) 지속 시간이 길수록(구멍이 오래 유지될수록?) 더 큰 가중치를 부여할 수 있다. </li>
</ul>
<li> PI로의 변환 </li>
<ul>
<li> Persistence Surface는 연속적인 함수 형태이기 때문에, 이를 격자(grid)로 이산화(discretization) 하여 2D 이미지 형태로 변환한다. </li>
<li> 각 격자 셀에서 함수 값을 적분하여 최종적으로 픽셀 값(행렬)을 생성하고, 이것이 PI가 된다. </li>
</ul>
<li> 가중치 함수와 중요도 </li>
<ul>
<li> 가중치 함수는 PI를 생성할 때 중요한 요소인데, 지속 시간이 긴 점(구멍이나 연결성이 오래 유지되는 특성)은 데이터에서 더 중요한 의미를 가질 가능성이 크기 때문에, 가중치를 더 크게 설정할 수 있다. </li>
<li> 즉, PI를 계산할 때 가우시안 함수에 가중치를 부여하기 위해 다양한 가중치 함수를 선택할 수 있는데, 위와 같은 것들을 고려할 수 있다. </li>
</ul>

### Convolution Neural Network
<li> CNN의 특징과 강점 </li>
<ul>
<li> CNN은 인간의 시각 피질의 계층적 구조에 영향을 받아 생김 </li>
<li> CNN은 이미지를 통해 저수준 특징(가장자리, 색상, 등)과 고수준 특징(객체, 얼굴같은 복잡한 패턴)을 모두 학습하고, 추출할 수 있다. </li>
<li> CNN은 자연 이미지에 존재하는 공간적 상곤관계를 활용할 수 있고, FC layer보다 parameter가 적어 학습 시간이 빠르다. </li>
</ul>
<li> CNN의 활용 </li>
<ul>
<li> CNN은 분류 작업 말고도, 입력 데이터와 다른 특징 표현 간의 매핑을 학습하는 데에서도 널리 사용되었다. </li>
<li> 그렇기 때문에 우리는 데이터와 PI 표현 간의 매핑을 학습시키기 위해 간단한 CNN 모델을 설계했다. </li>
</ul>

### Learning Strategies
<li> 지도 학습 </li>
<ul>
<li> 입력 x와 출력 y간의 매핑을 학습한다. </li>
<li> 논문에서는 입력 데이터와 PI간의 매핑을 학습하며, 이는 회귀 문제로 다루어 진다. </li>
</ul>
<li> 전이 학습 </li>
<ul>
<li> 기존의 사전 학습된 모델(pre-trained model)을 활용하여 새로운 데이터셋에서 학습을 가속화하거나 개선한다. </li>
<li> 논문에서는 타겟 데이터셋이 제한적인 경우 전이 학습을 통해 학습 효율성을 높인다. </li>
<li> 즉, 소스 A에 대해 학습한 모델을 통해서 소스 B에 대한 학습을 할 것인데, 이 때 소스 A에 대해 학습된 모델이라도, fine-tuning의 과정을 통해 적은 데이터로도 빠른 학습이 가능하다. </li>
</ul>

## Proposed Method (PI-Net Framework)
### Ground Truth Persistence Image를 만드는 방법.
<ol>
<li> time-series Data에서 PI를 만드는 방법 </li>
<ul>
<li> 데이터 전처리 : 시계열 데이터의 고정된 길이(time-steps)를 정하고, 데이터의 평균을 0으로 설정(zero-centering)하여 학습을 안정화시킴. 또한, 표준화(Standardization)를 통해 데이터의 분포를 균일하게 만들어 모델 학습 효율을 높인다. </li>
<li> PD 계산 : 시계열 데이터를 기반으로, 각 데이터 포인트의 위상적 특성(Topological Feature)를 분석하여 PD를 생성한다. (PD는 신호의 생성-소멸 시점을 나타내며, 이 정보를 2D 이미지로 변환한다.) </li>
<li> PI 생성 : Gaussian Kernel을 사용하여 PD를 2D 이미지로 변환한다. 지속 시간이 긴 점은 더 큰 가중치를 부여받아 PI에서 더 중요한 특징으로 나타난다. </li>
<li> PI의 grid size를 50 x 50으로 설정하고, PD의 각 점에 Gaussian kernel 함수를 적용해서 합친 후 각 grid cell에서 적분하여 픽셀 값으로 표현한다. </li>
<li> PI는 각 픽셀을 maxima로 나누어서 [0, 1] 범위에 들게 한다. </li>
</ul>
<li> Multi-channel Image Data에서 PI를 만드는 방법. </li>
<ul>
<li> 데이터 전처리 : 각 이미지 채널을 x-좌표, y-좌표, 픽셀의 intensity(강도) 값을 타나내는 3D Point Cloud로 표현했다. 또한, x와 y좌표의 정보도 [0, 1] 사이로 정규화 됨. </li>
<li> PD 계산 : 앞에서 설명된 과정을 통해 각 이미지 채널에 대한 1차원 Persistent Homology PD를 계산한다. </li>
<li> Sub-level Set Filtration : 강도 값(intensity)을 기준으로 데이터를 스캔하며, 특정 값 이하의 점만 포함하는 하위 집합(Sub-level Set)을 생성한다. </li>
<li> 점들이 연결되거나 구멍이 생기는 순간을 기록하여 생성 과 소멸 정보를 계산한다. </li>
<li> PI 생성 : 이 PD를 2D 평면에 배치하고, 각 점에 Gaussian Kernel 함수를 적용하여 값을 구하고, 이 값들을 모두 더하여 Persistence Surface를 생성한다. </li>
<li> Persistence Surface를 고정된 크기의 격자(grid)로 변환한다. 이후, 각 격자 셀에서 함수 값을 적분하여 픽셀 값으로 표현한다. </li>
<li> PI의 모든 값을 [0, 1] 범위로 정규화하여 마무리한다. </li>
</ul>
<li> 여러 데이터셋에 대한 파라미터 정리 (굳이?) </li>
</ol>

### 네트워크 구조
<li> Signal PI-Net </li>
<ul>
<li> 처음 input은 b x t x n의 형태이다. </li>
<li> b는 batch size, t는 time-steps나 frame size의 개수?, n은 신호의 개수 </li>
<li> encoder block에서는 kernel_size=3이고, out_channels가 128, 256, 512, 1024로 커지는 4개의 층을 지난다. 각 층마다 maxpooling(size=3, stride=2)로 입력 크기를 반으로 줄여준다. </li>
<li> 마지막 층에서는 pooling이 아닌, GAP를 수행하여 1차원으로 변경해줌. </li>
<li> 그 다음, fc층을 통하여 2500 x n개의 출력으로 변경한다. </li>
<li> 이 2500 x n개의 출력을 다시 50 x 50 x n으로 변경시킨다 → PI가 됨. </li>
</ul>
<li> Image PI-Net </li>
<ul>
<li> 처음 input은 b x h x w x c의 형태이다. </li>
<li> b는 batch size, h는 높이, w는 너비, c는 channel의 개수(RGB)이다. </li>
<li> encoder block에서는 kernel_size=3, out_channels가 128로 고정이다. </li>
<li> 각 블록마다 maxpooling을 통해 특성 맵의 크기를 반으로 줄인다. </li>
<li> GAP와 fc층을 통해 2500 x n의 크기로 만들어준다. </li>
<li> Decoder 과정을 통해 2500 x n의 특성들을 50 x 50 x n으로 변환해준다. (deconvolution이라는게 있나봄.) 여기서는 batch normalization 다음에 Relu가 아닌 Sigmoid를 통해서 각 좌표의 확률값으로 나타냄. 이렇게 얻어낸 것이 PI이다. </li>
</ul>
<li> 여기서 n의 의미는 이미지에서는 채널의 개수(색상), time-series에서는 신호의 개수를 의미하며, 각 채널마다 PI를 하나씩 생성하게 됨. </li>


## 실험 (시계열 데이터의 여러 표현과 MLP모델, 1D CNN 모델의 결합 성능 비교)
### 데이터 표현 방식과 분류
<li> 데이터 표현 방식 (시계열 데이터를 표현하는 3가지 방식) </li>
<ol>
<li> 19차원 통계적 특징 벡터(SF) </li>
<ul>
<li> 시계열 데이터에서 10초 프레임 단위로 평균, 분산, 제곱평균 등 19개의 통계량을 계산해서 하나의 벡터로 표현한다. </li>
</ul>
<li> MLP 또는 1D CNN을 통해 학습한 특징 </li>
<ul>
<li> 시계열 데이터를 다층 퍼셉트론(multi layer perceptron) 또는 1D CNN을 사용해 직접 학습한 특징으로 표현 </li>
<li> MLP는 8개의 Fully Connected 레이어로 구성되며, ReLU 활성화 함수와 Dropout(0.2) 및 Batch Normalization이 적용됨. </li>
<li> 1D CNN은 10개의 Conv 레이어를 사용하며, MaxPooling 및 Global Average Pooling을 활용하여 최종 출력을 생성함. </li>
</ul>
<li> Persistence Image (PI) </li>
<ul>
<li> 기존 TDA 기법 또는 Signal PI-Net을 통해 계산된 PI를 사용 </li>
<li> PI는 각 좌표가 위상적 속성을 나타내는 2D이미지로 표현되며, Signal PI-Net을 활용해 시계열 데이터를 직접 PI로 변환함. </li>
</ul>
</ol>

### 분류 모델
<li> MLP, 1D CNN 두 가지로 나뉨. </li>
<li> MLP는 8개의 Dense Layer, Dropout과 Normalization 적용, 최종 출력층에서 Softmax 활성화 함수로 클래스 분류를 함. </li>
<li> 1D CNN 모델은 10개의 Conv Layer, Maxpooing, ReLU를 사용하고, 최종적으로 GAP와 Dense Layer를 사용하여 클래스를 분류했다. </li>

### 비교 할 방법들
<li> 우선 여기서의 PI들은 벡터화 되어서 input으로 들어감. </li>
<li> 아레에서 말하는 입력은 센서 데이터를 통해 얻어진 데이터들을 어떻게 표현 방식으로 표현한 뒤 입력에 넣었느냐를 의미함. </li>
<li> 즉 앞에 MLP와 1D CNN은 분류 모델을 의미하고, 뒤에는 데이터 표현 방식을 의미하는 것. </li>
<li> 1. MLP + PI : PI를 벡터화하여 MLP 모델에 입력함 </li>
<li> 2. MLP + PI-Net : Signal PI-Net으로 생성한 PI를 MLP에 입력으로 넣음. </li>
<li> 3. MLP + SF : 통계적 특징벡터(SF)를 MLP에 입력함.  </li>
<li> 4. MLP + SF + PI : SF와 TDA로 얻은 PI를 입력으로 사용 (19차원 + PI를 벡터화한 형태(ex 7500차원이면) = 7519차원이 되는거임.) </li>
<li> 5. MLP + SF + PI-Net </li>
<li> 6. 1D CNN </li>
<li> 7. 1D CNN + PI </li>
<li> 8. 1D CNN + PI-Net </li>

### DenseNet을 이용한 Image PI-Net의 성능 비교
<li> 이 실험에서는 DenseNet을 이용하여 여러 데이터 이미지 데이터 표현 방식에 따른 결과를 비교할 것이다. </li>
<li> DenseNet은 논문에 쓰인 형태로 사용됨. </li>
<li> 1. DenseNet </li>
<li> 2. DenseNet + TDA_PI </li>
<li> 3. DenseNet + Image PI-Net PI </li>
<li> 4. DenseNet + Image PI-Net FA(Fine Tuning All) : 전이 모델을 사용하는데, fine-tuning을 할 때 target dataset의 모든 데이터로 가중치를 업데이트 한 경우 </li>
<li> 5. DenseNet + Image PI-Net FS(Fine Tuning Subset) : 전이 모델을 사용하는데, fine-tuning을 할 때 SubSet으로 조금만 가지고 사용한 경우. (제한된 데이터로도 잘 작동하는지 확인.) </li>
<li> 위의 다섯 가지 방식에 대해 성능을 비교한다. </li>

### Gaussian Noise에 대한 강건성(Robustness) 실험
<li> Gaussian Noise가 추가된 테스트 이미지에 대해 DenseNet 기반 모델들의 분류 성능 저하를 평가한다. </li>
<li> 특히 TDA기반 PI와 Image PI-Net PI를 사용한 DenseNet 모델이 노이즈에 더 강건한지를 파악하는게 목적이다. </li>
<li> 가우시안 노이즈 필터를 얼마나 심하게 먹이느냐에 따라 Level 1~4로 나눔. </li>
<li> Data Augmentation 없이 학습된 모델이 있을 때, Gaussian Noise를 먹인 테스트 데이터에 대해 얼마나 강건한지 실험하는 것이다. (즉, 학습할 때에는 Data Augmentation이 없지만, 테스트 데이터에 Data Agumentation을 적용 후 성능 저하 비교.) </li>
<li> 즉, 학습할 때 Data Augmentation은 없지만, 어떤 데이터 형태를 사용하느냐에 따른 성능 비교임 (그냥 원본 데이터, TDA-PI, PI-Net PI, PI-Net FA/FS) </li>
<li> FA, FS는 전이 학습을 통해 가중치를 조금 바꾼 모델을 사용한다? (PI-Net PI를 사용하고, DenseNet 모델의 가중치가 fine tuning된거임.) </li>

### TDA를 이용한 PI 생성과 PI-Net을 이용한 PI 생성 시간의 비교
<li> 거즘 1700배는 빨라졌다. GPU로 학습한 결과로는 </li>

## 두 번째 논문
### Abstract
<li> Mixup은 linear-interpolation을 기반으로 새로운 데이터 샘플을 생성하는 인기 있는 데이터 증강 기법이다. </li>
<li> Mixup은 모델의 일반화 능력과 robustness(노이즈에 강하게 하는)를 향상시키기 위해 사용된다. </li>
<li> KD는 모델 압축 및 전이 학습에서 널리 사용되는 기법으로, 데 큰 네트워크의 암묵적(?) 지식을 작은 네트워크에 전달하는 방식이다. </li>
<li> Mixup과 KD는 매우 관련이 없어 보이지만, 'Smoothness'(부드러움)이 Mixup과 KD간의 연결 고리이고, Mixup과 KD의 상호작용을 이해하는 데 중요한 속성이라는 것을 발견했다. </li>
<li> 많은 Mixup과 KD의 결합들이 제안되기는 했지만, Mixup이 정확히 KD에서 어떤 역할을 수행하는지에 대한 명확한 이해가 부족하다. </li>
<li> 이 논문에서 하는 일 </li>
<ul>
<li> Mixup과 KD의 호환성을 다양한 중요한 차원에서 실험 </li>
<li> mixup을 적용하여 훈련된 네트워크가 KD 관점에서 어떻게 작동하는지 조사. </li>
<li> 이미지 분류 문제를 중심으로 포괄적인 실험, 분석, 시각화 수행 </li>
</ul>

### Introduction
<li> Mixup은 샘플과 레이블을 선형 조합(convex combination)하여 새로운 데이터 포인트를 생성하는 데이터 증강 기법이다. </li>
<li> mixup을 통해서 모델의 일반화 성능을 향상시킬 수 있고, 적대적 샘플(adversarial examples)과 같은 노이즈에 대한 강건성(robustness)이 증가한다. </li>
<li> KD는 큰 네트워크의 지식을 작은 네트워크에 전이시키는 방법으로, 모델의 크기는 줄이면서 성능은 유지하는 것이 목표이다. </li>
<li> KD는 교사 모델의 출력 분포를 학생 모델의 출력 분포와 비교하여 출력 분포를 비슷하게 하는 것이 목표이다. </li>
<li> Mixup과 KD는 매우 다른 기법처럼 보이지만, smoothness라는 속성이라는 연결 고리가 있다. </li>
<li> MixUp을 적용하면 모델의 Accuracy는 증가하지만, 유사 클래스 간의 특징 분산(feature dispersion)은 증가하여 V-Score 점수가 낮아지는 경향이 있었다. </li>
<li> KD에서 온도 매개변수 t를 높이면, 출력의 엔트로피가 증가하여 학생 모델이 학습할 수 있는 정보량이 많아져서 Accuracy는 증가했지만, 특징 분산 또한 증가하여 V-Score 점수가 낮아지는 경향이 있었다. </li>
<li> 즉, KD와 Mixup 모두 Smoothness를 모델 학습에 도입하며, 학생 모델이 교사 모델의 분포를 더 잘 모방하고, 차별화된 표현을 학습하도록 돕는다는 의미이다. </li>
<li> 주요 정리 </li>
<ul>
<li> Mixup을 활용한 KD에서 features, logits, 네트워크 행동에 대한 깊은 이해를 통해 학생 모델 학습을 위한 향상된 전략을 제시함. </li>
<li> 적절한 온도 parameter 설정의 중요성을 줄임. 간단한 rescaling 기법을 개발하여 교사와 학생 간 logits의 통계적 특성을 유사한 범위로 조정하며, 클래스 간의 상대적인 정보를 유지함. (온도 매개변수를 사용하지 않아도 됨?) </li>
<li> PMU(Partial Mixup)방식(batch내에서 소수의 Mixup 쌍만 생성함)을 사용해도 KD에서 기존 방법과 동등하거나 더 나은 성능을 달성할 수 있음을 발견함. </li>
</ul>

### Background
<li> Mixup Augmentation </li>
<ul>
<li> 두 샘플의 선형 보간(linear interpolation)을 통해 혼합하는 방식이다. </li>
<li> λ를 통해 두 데이터 포인트와, 레이블이 얼마나/어떻게 섞이는지를 결정한다. 이 λ는 Beta(α, α)분포에서 샘플링 된다. α값이 클수록, 샘플 간 보간(interpolation)이 강해지고, 생성된 데이터가 원본 데이터와 더 멀어지게 된다. </li>
<li> Mixup Loss : Mixup을 적용한 샘플과 레이블은 새로운 크로스 엔트로피 손실로 계산된다. </li>
<li> 즉, 원래의 cross entropy loss에 mix up loss를 더해서 mixup된 데이터에 대해서도 확실히 학습할 수 있게 된다는 의미인듯 하다. </li>
<li> 모델은 이러한 혼합 데이터를 학습하면서 더 일반화된 패턴을 학습할 수 있게 된다. </li>
</ul>
<li> Knowledge Distillation </li>
<ul>
<li> KD는 교사 모델의 soft출력을 학생 모델이 학습하는 것이 목표임. </li>
<li> 교사 모델의 logit은 softmax함수를 거쳐 확률 분포로 변환된다. 온도 매개변수를 통해 이 확률 분포를 더 부드럽게 만들고, 클래스 간의 상대적인 정보가 명확하게 드러난다. </li>
<li> KL발산의 평균으로 loss를 추가한다는 의미인듯. </li>
</ul>

<li> Data Augmentation in KD </li>
<ul>
<li> 데이터 증강 기법이 KD에 활용되어 좋은 결과를 도출해낸 결과가 많다.(증강된 샘플은 KD에서 네트워크가 완화된(relaxed)지식을 학습할 수 있게 도와주기 때문.) </li>
<li> 하지만, 기존 데이터 증강 기법은 모델 내부 작동 방식(inner working)에 대한 깊은 이해(통찰)을 제공해주지 않는다.(경험적 분석을 통해 결과를 내는 경우가 많다.) </li>
<li> 이 논문에서는 데이터 증강이 KD 과정에서 작동하는 내제적 메커니즘에 관심을 두고 있다. </li>
<li> 또한 Mixup과 KD가 어떻게 상호작용하는지에 대한 호환성과 비호환성을 포괄적인 경험적 분석을 통해 조사한다. 또한, 관찰된 결과를 바탕으로, 네트워크 성능을 향상시키기 위한 개선된 학습 전략을 제안한다. </li>
</ul>

### Key Findings from Mixup and KD interplay
<li> Mixup과 KD의 상호작용에서 도출된 주요 발견들을 설명함. </li>
<li> Figure2를 참조하여 Mixup 증강이 KD에 포함될 수 있는 네 가지 가능한 시나리오를 설명한다. </li>
<ul>
<li> 표쥰 교사와 표준 학생 (T & S) </li>
<li> Mixup으로 학습된 교사와 표준 학생 (Mix-T & S) </li>
<li> Mixup으로 학습된 교사와 Mixup으로 학습된 학생 (Mix-T & Mix-S) </li>
<li> 표쥰 교사와 Mixup으로 학습된 학생 (T & Mix-S) </li>
<li> 온도 T = 4는 소프트 출력을 생성하는 데 사용되는 매개변수로 고정됨. </li>
</ul>
<li> 학생 모델은 ResNet56으로 고정하고, 교사 모델은 ResNet20~ResNet(110)까지 다양하게 설정하여 평가했다. </li>
<li> Figure2를 분석하여 Mixup과 KD의 상호작용에서 중요한 통찰 </li>
<ol>
<li> Mix-T를 사용하는 경우는 표준 교사(T)를 사용하는 경우에 비해 항상 효과(Accuracy)가 떨어졌다. 이는 Mixup으로 학습된 교사가 자체적으로는 Test Accuracy가 높을지언정, KD 과정에서 학생에게 정보를 전달하는 데에는 덜 효과적임을 보여준다. </li>
<li> 일반적으로 더 높은 용량의 교사 모델이 더 나은 학생 모델을 만들어내지만, 용량이 낮은 교사 모델(학생보다)을 사용하는 경우에도 학생 모델의 성능은 KD를 사용하지 않은 그냥 S(Resnet56)보다 향상되었다. </li>
</ol>
<li> 이 논문에서는 위의 두 결과를 바탕으로 아레의 두 가지 질문을 조사한다. </li>
<ul>
<li> Mixup으로 학습된 교사 모델이 왜 KD에서 학생 모델의 성능을 저하시킬까? (Section 3) </li>
<li> Mixup 증강이 적용될 때, KD의 효과를 개선하기 위해 무엇을 할 수 있을까? (Section 4) </li>
</ul>

![Figure2](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.11.20/Figure2.png)

### Figure3에 대한 분석
<li> Mixup 증강으로 학습된 네트워크의 영향을 분석하기 위해, CIFAR-100에서 몇몇 클래스를 선택해 두 그룹으로 나눴습니다. </li>
<li> 비슷한 클래스와 완전 다른 클래스로 나누어서 클러스터링 지표를 보고 V-socre까지도 매겨봄. </li>
<li> Observation 1 </li>
<ul>
<li> 1번 동그라미를 보면, KD를 적용하지 않은 그냥 학생 모델(Resnet56)은 Mixup을 적용하고 나서, similar class에 대해서 더 분산이 되었지만, different class에 대해서는 여전히 잘 구분하고 있다. </li>
<li> 이러한 정보 손실이 오른쪽의 V-Score 히스토그램에서 잘 보여준다. </li>
</ul>

<li> Observation 2 </li>
<ul>
<li> 더 낮은 정확도의 교사 모델(ex): Resnet 20)을 사용할 경우 학생 모델에 전달되는 지식의 품질이 떨어진다. </li>
<li> 낮은 용량의 교사 모델은 Mixup을 사용하더라도 KD 효과를 충분히 발휘하지 못하며, 이는 학습된 특징의 정확한 구분 능력을 제한함. </li>
</ul>

![Figure3](https://github.com/wjdwocks/ML-DNN/raw/main/markdown/24.11.20/Figure3.png)

## 주요 내용 요약

## Trainning Environment
<li> Dataset = Cifar10, CINIC10, Tiny ImageNet </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1 + CUDA ??? </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results