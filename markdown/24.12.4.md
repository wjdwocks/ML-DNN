## 24.11.26 공부할 내용
<li> 첫 번째 논문 : Leveraging Topological Guidance for Improved Knowledge Distillation </li>
<li> 두 번째 논문 : Topological Persistence Guided Knowledge Distillation for Wearable Sensor Data </li>

## 첫 번째 논문
### Abstract 정리
<li> 이전의 KD들은 Embedding Feature간의 점대점 or 쌍 관계를 지식으로 정의하고 이를 전달함. </li>
<li> 하지만, 복잡한 잠재 공간(Latent Space)의 관계를 효율적으로 전달하지 못했다. </li>
<li> 그래서 TopKD라는 방법을 제안한다. </li>
<li> Topological Feature를 PD를 이용해 PI로 근사하여 학습시킴. </li>

### Introduction 정리
<li> Vanilla KD는 Teacher의 soft logit을 Student에게 학습시킴. </li>
<li> Feature-Based KD는 Teacher와 Student의 중간 Layer의 Feature를 일치시키는 방식으로 학습시킴. </li>
<li> Relation-Based KD는 Embedding Features간의 복잡한 관계를 탐지하는데 중점을 둠 (거리와 각도 기반으로 특징 간의 구조를 정의, 학습시킴) </li>
<li> 이 논문에서는 Global Topological Characteristics(전역 위상 특성)을 활용하여 Teacher 모델의 전반적인 구조적 관계를 Student에게 전달한다. </li>

### Method 정리
<li> 이 논문에서 제시하는 Topology-Informed Knowledge Distillation (TopKD) </li>
<li> Teacher 모델의 잠재 공간(Latent Space)에서 지속성 다이어그램(PD)를 활용하여 Global Topology Knowledge를 정의함. </li>
<li> PD는 아레의 세 가지 정보를 요약하고 있다. </li>
<ul>
<li> 데이터 간 유사성 </li>
<li> 데이터 분포 </li>
<li> 데이터 간 상호작용 및 거리 </li>
</ul>
<li> TopKD Loss의 요약 </li>
<ul>
<li> PD를 계산하는 것은 매우 높은 계산 비용이 들기에 RipsNet을 사용하여 PD를 근사한 뒤 PI를 생성한다. </li>
<li> Teacher는 실제 PI(TDA)와 RipsNet으로 근사한 PI가 비슷해지도록 RipsNet을 학습시킴. </li>
<li> RipsNet으로 근사된 Teacher의 PI와 RipsNet으로 근사된 Student의 PI가 비슷해지도록 Student의 loss항을 추가함 </li>
<li> 기존의 KD loss와 이 Topological loss를 더하여 학습함. </li>
</ul>

## 두 번째 논문
### Abstract
<li> TDA는 데이터의 기하학적/위상적 구조를 파악하는데 유용하지만, 높은 계산 비용과 메모리 소모로 인해 제한이 있다. </li>
<li> KD에서 PI를 통해 TDA의 Topological Feature를 time-series Data와 결합해 Student의 성능을 개선시킬 수 있다. </li>
<li> 이 논문에서는 KD에서 원시 시계열 데이터와 TDA기반의 Topological Feature를 함께 사용했을 때 성능을 비교한다. </li>
<li> 여러 Teacher를 활용해서 Time-series Data와 Topological Feature를 융합함으로써 Student 학습 성능을 최적화함. </li>

### Introduction

### Method


## Trainning Environment
<li> Dataset = Cifar10, CINIC10, Tiny ImageNet </li>
<li> python = 3.8.18 </li>
<li> pytorch = 2.4.1 + CUDA ??? </li>
<li> GPU = NVIDIA GeForce RTX 3080 </li>
<li> CPU = 12th Gen Intel(R) Core(TM) i5-12400F, 2500Mhz, 6 코어, 12 논리 프로세서 </li>
<li> epoch = 20 </li>
<li> batch size = 64 </li>
<li> learning rate = 0.0005 </li>
<li> optimizer = Adam </li>



## Evaluation


## Results